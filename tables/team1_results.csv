Link/DOI,Publication Date,Title,Authors,Abstract
https://doi.org/10.1101/2022.03.20.485041,2023-12-15,slendr: a framework for spatio-temporal population genomic simulations on geographic landscapes,"['Petr, M.; Haller, B. C.; Ralph, P. L.; Racimo, F.']","one of the goals of population genetics is to understand how evolutionary forces shape patterns of genetic variation over time. however, because populations evolve across both time and space, most evolutionary processes also have an important spatial component, acting through phenomena such as isolation by distance, local mate choice, or uneven distribution of resources. this spatial dimension is often neglected, partly due to the lack of tools specifically designed for building and evaluating complex spatio-temporal population genetic models. to address this methodological gap, we present a new framework for simulating spatially-explicit genomic data, implemented in a new r package called slendr (www.slendr.net), which leverages a slim simulation back-end script bundled with the package. with this framework, the users can programmatically and visually encode spatial population ranges and their temporal dynamics (i.e., population displacements, expansions, and contractions) either on real earth landscapes or on abstract custom maps, and schedule splits and gene-flow events between populations using a straightforward declarative language. additionally, slendr can simulate data from traditional, non-spatial models, either with slim or using an alternative built-in coalescent msprime back end. together with its r-idiomatic interface to the tskit library for tree-sequence processing and analysis, slendr opens up the possibility of performing efficient, reproducible simulations of spatio-temporal genomic data entirely within the r environment, leveraging its wealth of libraries for geospatial data analysis, statistics, and visualization. here, we present the design of the slendr r package and demonstrate its features on several practical example workflows."
https://doi.org/10.1101/2023.10.03.560595,2023-12-15,a method for estimating the cholesterol affinity of integral membrane proteins from experimental data,"['Steck, T. L.; Tabei, A.; Lange, Y.']","the cholesterol affinities of many integral plasma membrane proteins have been estimated by molecular computation. however, these values lack experimental confirmation. we therefore developed a simple mathematical model to extract sterol affinity constants and stoichiometries from published isotherms for the dependence of the activity of such proteins on membrane cholesterol concentration. the binding curves for these proteins are sigmoidal with strongly-lagged thresholds attributable to competition for the cholesterol by bilayer phospholipids. the model provided isotherms that matched the experimental data using published values for the sterol association constants and stoichiometries of the phospholipids. three oligomeric transporters were found to bind cholesterol without cooperativity with dimensionless association constants of 35 for kir3.4* and 100 for both kir2 and a gat transporter. (the corresponding {rho}g{degrees} values were -8.8, -11.4 and -11.4 kj/mol, respectively.) these association constants are significantly lower than those for the phospholipids which range from [~]100 to 6,000. the bk channel, the nicotinic acetylcholine receptor and the m192i mutant of kir3.4* appear to bind multiple cholesterol molecules cooperatively (n = 2 or 4) with subunit affinities of 563, 950 and 700, respectively. the model predicts that the three less avid transporters are approximately half-saturated in their native plasma membranes; hence, sensitive to variations in cholesterol in vivo. the more avid proteins would be nearly saturated in vivo. the method can be applied to any integral protein or other ligand in any bilayer for which there are reasonable estimates of the sterol affinities and stoichiometries of the phospholipids."
https://doi.org/10.1101/2022.09.19.508413,2023-12-19,elisl: early-late integrated synthetic lethality prediction in cancer,"['Tepeli, Y.; Seale, C.; Goncalves, J. P.']","anti-cancer therapies based on synthetic lethality (sl) exploit tumor vulnerabilities for treatment with reduced side effects. since simultaneous loss-of-function of sl genes causes cell death, tumors with known gene disruptions can be treated by targeting sl partners. computational selection of promising sl candidates amongst all gene combinations is key to expedite experimental screening. however, current sl prediction models: (i) only use tissue type-specific molecular data, which can be scarce/noisy, limiting performance for some cancers; and (ii) often rely on shared sl patterns across genes, showing sensitivity to prevalent gene selection bias. we propose elisl, early-late integrated models for sl prediction using forest ensembles. elisl models ignore shared sl patterns, and integrate context-specific data from cancer cell lines or tumor tissue with context-free functional associations derived from protein sequence. elisl outperformed existing methods and was more robust to selection bias in 8 cancer types, with prominent contribution from sequence. we found better survival for patients whose tumors carried simultaneous mutations in a brca gene together with an elisl-predicted sl gene from the hh, fgf, or wnt families. elisl thus arises as a promising strategy to discover sl interactions with therapeutic potential."
https://doi.org/10.1101/2023.07.14.549022,2023-12-20,decrypting the languages of allostery in membrane-bound k-ras4b using four complementary in silico approaches,"['Castelli, M.; Marchetti, F.; Serapian, S. A.; Colombo, G.']","proteins are evolved molecular machines whose diverse biochemical functions can be dynamically regulated by allostery, through which even faraway protein residues can conformationally communicate. allostery can express itself in different ways, akin to different ""languages"": allosteric control pathways predominating in an unperturbed protein are superseded by others as soon as a perturbation arises--e.g., a mutation--that alters its function (pathologically or not). accurately modeling these often-unintuitive phenomena could therefore help explain functional changes in a specific protein whenever they are unclear. unbiased molecular dynamics (md) simulations are a possibility; however, since allostery can operate at longer timescales than those accessible by md, simulations require integration with a reliable method able to, e.g., detect regions of incipient allosteric change or likely perturbation pathways. several methods exist but are typically applied singularly: we argue their joint application, in a ""multilingual"" approach, could significantly enrich this picture. to prove this, we perform unbiased md simulations ([~]100 {micro}s) of the widely studied, allosterically active oncotarget k-ras4b, solvated and embedded in a phospholipid membrane, proceeding to decrypt its allostery using four showcase ""languages"": distance fluctuation analysis and the shortest path map capture allosteric communication hotspots at equilibrium; anisotropic thermal diffusion and dynamical non-equilibrium md simulations assess them once the gtp that oncogenically activates k-ras4b is, respectively, either superheated or hydrolyzed. ""languages"" provide a uniquely articulate, mutually coherent, experimentally consistent picture of allostery in k-ras4b. at equilibrium, pathways stretch from the membrane-embedded hypervariable region all the way to the active site, touching known flexible allosteric ""switches"" and proposed pockets. upon gtp cleavage/perturbation, known to inactivate k-ras4b, allosteric signals most reverberate on switches and interfaces that recruit effector proteins. our work highlights the benefits of integrating different allostery detection methods with unbiased md."
https://doi.org/10.1101/2023.03.29.534747,2023-12-22,novel mrna-based vp8* vaccines against rotavirus are highly immunogenic in rodents,"['Roier, S.; Prasad, V. M.; McNeal, M. M.; Lee, K. K.; Petsch, B.; Rauch, S.']","despite the availability of live-attenuated oral vaccines, rotavirus remains a major cause of severe childhood diarrhea worldwide. due to the growing demand for parenteral rotavirus vaccines, we developed novel mrna-based vaccine candidates targeting the viral spike protein vp8*. our monomeric p2 (universal t cell epitope)-vp8* mrna design is equivalent to a protein vaccine currently in clinical development, while ls (lumazine synthase)-p2-vp8* was designed to form nanoparticles. cyro-electron microscopy and western blotting-based data presented here suggest that proteins derived from ls-p2-vp8* mrna are secreted in vitro and self-assemble into 60-mer nanoparticles displaying vp8*. mrna encoded vp8* was immunogenic in rodents and introduced both humoral and cellular responses. ls-p2-vp8* induced superior humoral responses to p2-vp8* in guinea pigs, both as monovalent and trivalent vaccines, with encouraging responses detected against the most prevalent p genotypes. overall, our data provide evidence that trivalent ls-p2-vp8* represents a promising mrna-based next-generation rotavirus vaccine candidate."
https://doi.org/10.1101/2023.08.10.552766,2023-12-20,design of a biohybrid materials circuit with binary decoder functionality,"['Mohsenin, H.; Wagner, H.; Rosenblatt, M.; Kemmer, S.; Timmer, J.; Weber, W.']","synthetic biology applies concepts from electrical engineering and information processing to endow cells with computational functionality. transferring the underlying molecular components into materials and wiring them according to topologies inspired by electronic circuit boards has yielded materials systems that perform selected computational operations. however, the limited functionality of available building blocks is restricting the implementation of advanced information-processing circuits into materials. here, we engineer a set of protease-based biohybrid modules the bioactivity of which can either be induced or inhibited. guided by a quantitative mathematical model and following a design-build-test-learn cycle, we wire the modules according to circuit topologies inspired by electronic signal decoders, a fundamental motif in information processing. we design a 2-input/4-output binary decoder for the detection of two small molecules in a material framework that could perform regulated outputs in form of distinct protease activities. the here demonstrated smart material system is strongly modular and could be used for biomolecular information processing for example in advanced biosensing or drug delivery applications."
https://doi.org/10.1101/2023.10.03.560786,2023-12-20,systematic investigation of machine learning on limited data: a study on predicting protein-protein binding strength,"['Zheng, F.; Jiang, X.; Wen, Y.; Yang, Y.; Li, M.']","the application of machine learning techniques in biological research, especially when dealing with limited data availability, poses significant challenges. in this study, we leveraged advancements in method development for predicting protein-protein binding strength to conduct a systematic investigation into the application of machine learning on limited data. the binding strength, quantitatively measured as binding affinity, is vital for understanding the processes of recognition, association, and dysfunction that occur within protein complexes. by incorporating transfer learning, integrating domain knowledge, and employing both deep learning and traditional machine learning algorithms, we mitigate the impact of data limitations and make significant advancements in predicting protein-protein binding affinity. in particular, we developed over 20 models, ultimately selecting three representative best-performing ones that belong to distinct categories. the first model is structure-based, consisting of a random forest regression and thirteen handcrafted features. the second model is sequence-based, employing an architecture that combines transferred embedding features with a multilayer perceptron. finally, we created an ensemble model by averaging the predictions of the two aforementioned models. the comparison with other predictors on three independent datasets confirmed the significant improvements achieved by our models in predicting protein-protein binding affinity. the source codes for these three models are available at https://github.com/minghuilab/bindppi."
https://doi.org/10.1101/2023.08.14.553221,2023-12-20,tissue transglutaminase 2 has higher affinity for relaxed than for stretched fibronectin fibers,"['Selcuk, K.; Leitner, A.; Braun, L.; Le Blanc, F.; Pacak, P.; Pot, S.; Vogel, V.']","tissue transglutaminase 2 (tg2) plays a vital role in stabilizing extracellular matrix (ecm) proteins through enzymatic crosslinking during tissue growth, repair, and inflammation. tg2 also binds non-covalently to fibronectin (fn), an essential component of the ecm, facilitating cell adhesion, migration, proliferation, and survival. however, the interaction between tg2 and fibrillar fn remains poorly understood, as most studies have focused on soluble or surface-adsorbed fn or fn fragments, which differ in their conformations from insoluble fn fibers. using a well-established in vitro fn-fiber stretch assay, we discovered that the binding of a crosslinking enzyme to ecm fibers is mechano-regulated. tg2 binding to fn is tuned by the mechanical tension of fn fibers, whereby tg2 predominantly co-localizes to low-tension fn fibers, while fiber stretching reduces their affinity for tg2. this mechano-regulated binding relies on the proximity between the n-terminal {beta}-sandwich and c-terminal {beta}-barrels of tg2. crosslinking mass spectrometry (xl-ms) revealed a novel tg2-fn synergy site within tg2s c-terminal {beta}-barrels that interacts with fn regions outside of the canonical gelatin binding domain, specifically fni2 and fniii14-15. combining xl-ms distance restraints with molecular docking reveals the mechano-regulated binding mechanism between tg2 and modules fni7-9 by which mechanical forces regulate tg2-fn interactions. this highlights a previously unrecognized role of tg2 as a tension sensor for fn fibers. this novel interaction mechanism has significant implications in physiology and mechanobiology, including how force regulate ecm deposition and maturation, and how tg2 mediates cell-ecm adhesion in health and in various pathophysiological processes. data are available via proteomexchange with identifier pxd043976."
https://doi.org/10.1101/2022.05.15.490691,2023-12-20,open access repository-scale propagated nearest neighbor suspect spectral library for untargeted metabolomics,"[""Bittremieux, W.; Avalon, N. E.; Thomas, S. P.; Kakhkhorov, S. A.; Aksenov, A. A.; Portal Gomes, P. W.; Aceves, C. M.; Caraballo Rodriguez, A. M.; Gauglitz, J. M.; Gerwick, W. H.; Jarmusch, A. K.; Kaddurah-Daouk, R. F.; Kang, K. B.; Kim, H. W.; Kondic, T.; Mannochio-Russo, H.; Meehan, M. J.; Melnik, A.; Nothias, L.-F.; O'Donovan, C.; Panitchpakdi, M.; Petras, D.; Schmid, R.; Schymanski, E. L.; van der Hooft, J. J. J.; Weldon, K. C.; Yang, H.; Zemlin, J.; Wang, M.; Dorrestein, P. C.""]","despite the increasing availability of tandem mass spectrometry (ms/ms) community spectral libraries for untargeted metabolomics over the past decade, the majority of acquired ms/ms spectra remain uninterpreted. to further aid in interpreting unannotated spectra, we created a nearest neighbor suspect spectral library, consisting of 87,916 annotated ms/ms spectra derived from hundreds of millions of public ms/ms spectra. annotations were propagated based on structural relationships to reference molecules using ms/ms-based spectrum alignment. we demonstrate the broad relevance of the nearest neighbor suspect spectral library through representative examples of propagation-based annotation of acylcarnitines, bacterial and plant natural products, and drug metabolism. our results also highlight how the library can help to better understand an alzheimers brain phenotype. the nearest neighbor suspect spectral library is openly available through the gnps platform to help investigators hypothesize candidate structures for unknown ms/ms spectra in untargeted metabolomics data."
https://doi.org/10.1101/2023.09.08.556619,2023-12-19,simplicity: web-based visualization and analysis of high-throughput cancer cell line screens,"['Ling, A. L.; Zhang, W.; Lee, A. M.; Xia, Y.; Su, M.-C.; Gruener, R. F.; Jena, S.; Huang, Y.; Pareek, S.; Shan, Y.; Huang, R. S.']","high-throughput drug screens are a powerful tool for cancer drug development. however, the results of such screens are often made available only as raw data, which is intractable for researchers without informatic skills, or as highly processed summary statistics, which can lack essential information for translating screening results into clinically meaningful discoveries. to improve the usability of these datasets, we developed simplicity, a robust and user-friendly web interface for visualizing, exploring, and summarizing raw and processed data from high-throughput drug screens. importantly, simplicity allows for easy recalculation of summary statistics at user-defined drug concentrations. this allows simplicitys outputs to be used with methods that rely on statistics being calculated at clinically relevant doses. simplicity can be freely accessed at https://oncotherapyinformatics.org/simplicity/."
https://doi.org/10.1101/2023.07.20.549955,2023-12-19,predicting gene expression changes upon epigenomic drug treatment,"['Agrawal, P.; Gopalan, V.; Hannenhalli, S.']","backgroundtumors are characterized by global changes in epigenetic changes such as dna methylation and histone modifications that are functionally linked to tumor progression. accordingly, several drugs targeting the epigenome have been proposed for cancer therapy, notably, histone deacetylase inhibitors (hdaci) such as vorinostatis and dna methyltransferase inhibitors (dnmti) such as zebularine. however, a fundamental challenge with such approaches is the lack of genomic specificity, i.e., the transcriptional changes at different genomic loci can be highly variable thus making it difficult to predict the consequences on the global transcriptome and drug response. for instance, treatment with dnmti may upregulate the expression of not only a tumor suppressor but also an oncogene leading to unintended adverse effect.  methodsgiven the pre-treatment transcriptome and epigenomic profile of a sample, we assessed the extent of predictability of locus-specific changes in gene expression upon treatment with hdaci using machine learning.  resultswe found that in two cell lines (hct116 treated with largazole at 8 doses and rh4 treated with entinostat at 1{micro}m) where the appropriate data (pre-treatment transcriptome and epigenome as well as post-treatment transcriptome) is available, our model distinguished the post-treatment up versus downregulated genes with high accuracy (up to roc of 0.89). furthermore, a model trained on one cell line is applicable to another cell line suggesting generalizability of the model.  conclusionshere we present a first assessment of the predictability of genome-wide transcriptomic changes upon treatment with hdaci. lack of appropriate omics data from clinical trials of epigenetic drugs currently hampers the assessment of applicability of our approach in clinical setting."
https://doi.org/10.1101/2023.09.08.555904,2023-12-19,deep learning-assisted single-molecule detection of protein post-translational modifications with a biological nanopore,"['Cao, C.; Magalhaes, P.; Krapp, L.; Bada Juarez, J.; Mayer, S.; Rukes, V.; Chiki, A.; Lashuel, H.; Dal Peraro, M.']","protein post-translational modifications (ptms) play a crucial role in countless biological processes, profoundly modulating protein properties on both the spatial and temporal scales. protein ptms have also emerged as reliable biomarkers for several diseases. however, only a handful of techniques are available to accurately measure their levels, capture their complexity at a single molecule level and characterize their multifaceted roles in health and disease. nanopore sensing provides high sensitivity for the detection of low-abundance proteins, holding the potential to impact single-molecule proteomics and ptm detection in particular. here, we demonstrate the ability of a biological nanopore, the pore-forming toxin aerolysin, to detect and distinguish -synuclein-derived peptides bearing single or multiple ptms, namely phosphorylation, nitration and oxidation occurring at different positions and in various combinations. the characteristic current signatures of the -synuclein peptide and its ptm variants could be confidently identified using a deep learning model for signal processing. we further demonstrate that this framework can quantify -synuclein peptides at picomolar concentration and detect the c-terminal peptides generated by digestion of full-length -synuclein. collectively, our work highlights the unique advantage of using nanopore as a tool for simultaneous detection of multiple ptms and paves the way for their use in biomarker discovery and diagnostics."
https://doi.org/10.1101/2023.06.21.546022,2023-12-18,pathway centric analysis for single-cell rna-seq and spatial transcriptomics data with gsdensity,"['Liang, Q.; Huang, Y.; He, S.; Chen, K.']","advances in single-cell technology have enabled molecular cellular dissection of heterogeneous biospecimens at unprecedented scales and resolutions. although cluster-centric approaches followed by gene-set analysis can reveal distinct cell types and states, they have limited power in dissecting and interpretating highly heterogenous, dynamically evolving data. here, we present gsdensity, a graph-modeling approach that allows users to obtain pathway-centric interpretation and dissection of single-cell and spatial transcriptomics (st) data without performing clustering. we show that gsdensity can not only accurately detect biologically distinct gene sets but also reveal novel cell-pathway associations that are ignored by existing methods. this is particularly evident in characterizing cancer cell states that are transcriptomically distinct but are driven by shared tumor-immune interaction mechanisms. moreover, we show that gsdensity, combined with trajectory analysis can identify pathways that are active at various stages of mouse brain development. finally, we show that gsdensity can identify spatially relevant pathways in mouse brains including those following a high-order organizational patterns in the st data. we also created a pan-cancer pathway activity st map, which revealed pathways spatially relevant and recurrently active across six different tumor types. gsdensity is available as an open-source r package and can be widely applied to single-cell and st data generated by various technologies."
https://doi.org/10.1101/2022.08.31.505992,2023-12-18,diffusion controls local versus dispersed inheritance of histones during replication and shapes epigenomic architecture,"['Chakrabarti, S.; Singh, A.']","the dynamics of inheritance of histones and their associated modifications across cell divisions can have major consequences on maintenance of the cellular epigenomic state. recent experiments contradict the long-held notion that histone inheritance during replication is always local, suggesting that active and repressed regions of the genome exhibit fundamentally different histone dynamics independent of transcription-coupled turnover. here we develop a stochastic model of histone dynamics at the replication fork and demonstrate that differential diffusivity of histones in active versus repressed chromatin is sufficient to quantitatively explain these recent experiments. further, we use the model to predict patterns in histone mark similarity between pairs of genomic loci that should be developed as a result of diffusion, but not from prc2 mediated mark spreading or transcriptional processes. interestingly, using a combination of chip-seq, replication timing and hi-c datasets we demonstrate that all the computationally predicted patterns are consistently observed for both active and repressive histone marks in two different cell lines. our results suggest that in contrast to current models that posit histone transfer exclusively between parental and daughter dna strands, dislodged histones in euchromatin and facultative heterochromatin diffuse within larger ""diffusion-accessible-domains"" (dads), leading to redistribution of epigenetic marks both within and across chromosomes. preservation of the epigenomic state across cell divisions therefore might be achieved not by passing on strict positional information of histone marks, but by maintaining the marks in somewhat larger dads of the genome."
https://doi.org/10.1101/2023.02.16.528799,2023-12-18,an attention network for predicting t cell receptor-peptide binding can associate attention with interpretable protein structural properties,"['Koyama, K.; Hashimoto, K.; Nagao, C.; Mizuguchi, K.']","understanding how a t cell receptor (tcr) recognizes its specific ligand peptide is crucial for gaining insight into biological functions and disease mechanisms. despite its importance, experimentally determining tcr-peptide interactions is expensive and time-consuming. to address this challenge, computational methods have been proposed, but they are typically evaluated by internal retrospective validation only, and few have incorporated and tested an attention layer from language models into structural information.  therefore, in this study, we developed a machine learning model based on a modified version of the transformer, a source-target-attention neural network, to predict tcr-peptide binding solely from the amino acid sequences of the tcrs complementarity-determining region (cdr) 3 and the peptide. this model achieved competitive performance on a benchmark dataset of tcr-peptide binding, as well as on a truly new external dataset. additionally, by analyzing the results of binding predictions, we associated the neural network weights with protein structural properties. by classifying the residues into large and small attention groups, we identified statistically significant properties associated with the largely attended residues, such as hydrogen bonds within the cdr3. the dataset that we have created and our models ability to provide an interpretable prediction of tcr-peptide binding should increase our knowledge of molecular recognition and pave the way to designing new therapeutics."
https://doi.org/10.1101/2023.08.29.555304,2023-12-17,a short sequence in the tail of sars-cov-2 envelope protein controls accessibility of its pdz binding motif to the cytoplasm.,"['Neitthoffer, B.; Alvarez, F.; Larrous, F.; Caillet-Saguy, C.; Etienne-Manneville, S.; Boeda, B.']","the carboxy terminal tail of the severe acute respiratory syndrome coronavirus 2 (sars-cov-2) envelope protein (e) contains a pdz-binding motif (pbm) which is crucial for coronavirus pathogenicity. during sars-cov-2 infection, the viral e protein is expressed within the golgi apparatus membrane of host cells with its pbm facing the cytoplasm. in this work we study the molecular mechanisms controlling the presentation of the pbm to host pdz (psd-95/dlg/zo-1) domain-containing proteins. we show that at the level of the golgi apparatus, the pdz-binding motif of the e protein is not detected by e c-terminal specific antibodies neither by pdz domain-containing protein binding partner. four alanine substitutions upstream of the pbm in the central region of the e protein tail is sufficient to generate immunodetection by anti-e antibodies and trigger robust recruitment of the pdz domain-containing protein into the golgi organelle. overall, this work suggests that the presentation of the pbm to the cytoplasm is under conformational regulation mediated by the central region of the e protein tail and that pbm presentation probably does not occur at the surface of golgi cisternae but likely at post-golgi stages of the viral cycle."
https://doi.org/10.1101/2023.07.07.548002,2023-12-16,a machine learning approach for quantifying age-related histological changes in the mouse kidney.,"['Sheehan, S.; Mawe, S.; Chen, M.; Klug, J.; Ladiges, W. C.; Korstanje, R.; Mahoney, J. M.']","the ability to quantify aging-related changes in histological samples is important, as it allows for evaluation of interventions intended to effect health span. we used a machine learning architecture that can be trained to detect and quantify these changes in the mouse kidney. using additional held out data, we show validation of our model, correlation with scores given by pathologists using the geropathology research network aging grading scheme, and its application in providing reproducible and quantifiable age scores for histological samples. aging quantification also provides the insights into possible changes in image appearance that are independent of specific geropathology-specified lesions. furthermore, we provide trained classifiers for h&e-stained slides, as well as tutorials on how to use these and how to create additional classifiers for other histological stains and tissues using our architecture.this architecture and combined resources allow for the high throughput quantification of mouse aging studies in general and specifically applicable to kidney tissues."
https://doi.org/10.1101/2021.08.26.457795,2023-12-15,efficient decoding of large-scale neural population responses with gaussian-process multiclass regression,"['Greenidge, C. D.; Scholl, B.; Yates, J.; Pillow, J. W.']","neural decoding methods provide a powerful tool for quantifying the information content of neural population codes and the limits imposed by correlations in neural activity. however, standard decoding methods are prone to overfitting and scale poorly to high-dimensional settings. here, we introduce a novel decoding method to overcome these limitations. our approach, the gaussian process multi-class decoder (gpmd), is well-suited to decoding a continuous low-dimensional variable from high-dimensional population activity, and provides a platform for assessing the importance of correlations in neural population codes. the gpmd is a multinomial logistic regression model with a gaussian process prior over the decoding weights. the prior includes hyperparameters that govern the smoothness of each neurons decoding weights, allowing automatic pruning of uninformative neurons during inference. we provide a variational inference method for fitting the gpmd to data, which scales to hundreds or thousands of neurons and performs well even in datasets with more neurons than trials. we apply the gpmd to recordings from primary visual cortex in three different species: monkey, ferret, and mouse. our decoder achieves state-of-the-art accuracy on all three datasets, and substantially outperforms independent bayesian decoding, showing that knowledge of the correlation structure is essential for optimal decoding in all three species."
https://doi.org/10.1101/2023.05.18.541381,2023-12-15,sctie: data integration and inference of gene regulation using single-cell temporal multimodal data,"['Lin, Y.; Wu, T.-Y.; Chen, X.; Wan, S.; Chao, B.; Xin, J.; Yang, J. Y. H.; Wong, W. H.; Wang, Y. X. R.']","single-cell technologies offer unprecedented opportunities to dissect gene regulatory mecha-nisms in context-specific ways. although there are computational methods for extracting gene regulatory relationships from scrna-seq and scatac-seq data, the data integration problem, essential for accurate cell type identification, has been mostly treated as a standalone challenge. here we present sctie, a unified method that integrates temporal multimodal data and infers regulatory relationships predictive of cellular state changes. sctie uses an autoencoder to embed cells from all time points into a common space using iterative optimal transport, followed by extracting interpretable information to predict cell trajectories. using a variety of synthetic and real temporal multimodal datasets, we demonstrate sctie achieves effective data integration while preserving more biological signals than existing methods, particularly in the presence of batch effects and noise. furthermore, on the exemplar multiome dataset we generated from differentiating mouse embryonic stem cells over time, we demonstrate sctie captures regulatory elements highly predictive of cell transition probabilities, providing new potentials to understand the regulatory landscape driving developmental processes."
https://doi.org/10.1101/2023.03.21.533676,2023-12-15,modeling transcriptional regulation of the cell cycle using a novel cybernetic-inspired approach,"['Raja, R.; Khanum, S.; Aboulmouna, L.; Maurya, M. R.; Gupta, S.; Subramaniam, S.; Ramkrishna, D.']","quantitative understanding of cellular processes, such as cell cycle and differentiation, is impeded by various forms of complexity ranging from myriad molecular players and their multilevel regulatory interactions, cellular evolution with multiple intermediate stages, lack of elucidation of cause-effect relationships among the many system players, and the computational complexity associated with the profusion of variables and parameters. in this paper, we present an elegant modeling framework based on the cybernetic concept that biological regulation is inspired by objectives embedding entirely novel strategies for dimension reduction, process stage specification through the system dynamics, and innovative causal association of regulatory events with the ability to predict the evolution of the dynamical system. the elementary step of the modeling strategy involves stage-specific objective functions that are computationally-determined from experiments, augmented with dynamical network computations involving end point objective functions, mutual information, change point detection, and maximal clique centrality. we demonstrate the power of the method through application to the mammalian cell cycle, which involves thousands of biomolecules engaged in signaling, transcription, and regulation. starting with a fine-grained transcriptional description obtained from rna sequencing measurements, we develop an initial model, which is then dynamically modeled using the cybernetic-inspired method (cim), utilizing the strategies described above. the cim is able to distill the most significant interactions from a multitude of possibilities. in addition to capturing the complexity of regulatory processes in a mechanistically causal and stage-specific manner, we identify the functional network modules, including novel cell cycle stages. our model is able to predict future cell cycles consistent with experimental measurements. we posit that this state-of-the-art framework has the promise to extend to the dynamics of other biological processes, with a potential to provide novel mechanistic insights.  statement of significancecellular processes like cell cycle are overly complex, involving multiple players interacting at multiple levels, and explicit modeling of such systems is challenging. the availability of longitudinal rna measurements provides an opportunity to ""reverse-engineer"" for novel regulatory models. we develop a novel framework, inspired using goal-oriented cybernetic model, to implicitly model transcriptional regulation by constraining the system using inferred temporal goals. a preliminary causal network based on information-theory is used as a starting point, and our framework is used to distill the network to temporally-based networks containing essential molecular players. the strength of this approach is its ability to dynamically model the rna temporal measurements. the approach developed paves the way for inferring regulatory processes in many complex cellular processes."
https://doi.org/10.1101/2023.08.13.553125,2023-12-15,orphan quality control by an scf ubiquitin ligase directed to pervasive c-degrons,"['Kong, K.-Y. E.; Shankar, S.; Ruehle, F.; Khmelinskii, A.']","selective protein degradation typically involves substrate recognition via short linear motifs known as degrons. various degrons can be found at protein termini from bacteria to mammals. while n-degrons have been extensively studied, our understanding of c-degrons is still limited. towards a comprehensive understanding of eukaryotic c-degron pathways, we performed an unbiased survey of c-degrons in budding yeast. we identified over 5000 potential c-degrons by stability profiling of random peptide libraries and of the yeast c-terminome. combining machine learning, high-throughput mutagenesis and genetic screens revealed that the scf ubiquitin ligase targets [~]40% of degrons using a single f-box substrate receptor das1. although sequence-specific, das1 is highly promiscuous, recognizing a variety of c-degron motifs. by screening for endogenous substrates, we implicate scfdas1 in degradation of orphan protein complex subunits. altogether, this work provides a comprehensive view of a eukaryotic c-degronome and uncovers how an scf/c-degron pathway of broad specificity contributes to proteostasis."
https://doi.org/10.1101/2023.10.30.563998,2023-12-15,efficient semi-supervised semantic segmentation of electron microscopy cancer images with sparse annotations,"['Pagano, L.; Thibault, G.; Bousselham, W.; Riesterer, J. L.; Song, X.; Gray, J. W.']","electron microscopy (em) enables imaging at nanometer resolution and can shed light on how cancer evolves to develop resistance to therapy. acquiring these images has become a routine task; however, analyzing them is now the bottleneck, as manual structure identification is very time-consuming and can take up to several months for a single sample. deep learning approaches offer a suitable solution to speed up the analysis. in this work, we present a study of several state-of-the-art deep learning models for the task of segmenting nuclei and nucleoli in volumes from tumor biopsies. we compared previous results obtained with the resunet architecture to the more recent unet++, fractalresnet, senformer, and ceecnet models. in addition, we explored the utilization of unlabeled images through semi-supervised learning with cross pseudo supervision. we have trained and evaluated all of the models on sparse manual labels from three fully annotated in-house datasets that we have made available on demand, demonstrating improvements in terms of 3d dice score. from the analysis of these results, we drew conclusions on the relative gains of using more complex models, semi-supervised learning as well as next steps for the mitigation of the manual segmentation bottleneck."
https://doi.org/10.1101/2023.05.18.541276,2024-01-09,improved detection and quantitation of rna-interactomes using dia silac,"['Tan, T. C.-J.; Spanos, C.; Tollervey, D.']","the rna-interacting proteome is commonly characterized by uv-crosslinking followed by rna purification, with protein recovery quantified using silac labeling followed by data-dependent acquisition (dda) of proteomic data. however, the low efficiency of uv-crosslinking, combined with limited sensitivity of the dda approach often restricts detection to relatively abundant proteins, necessitating multiple mass spec injections of fractionated peptides for each biological sample. here we report an application of data-independent acquisition (dia) with silac in a total rna-associated protein purification (trapp) uv-crosslinking experiment. this gave 15% greater protein detection and lower inter-replicate variation relative to the same biological materials analyzed using dda, while allowing single-shot analysis of the sample. as proof of concept, we determined the effects of arsenite treatment on the rna-bound proteome of hek293t cells. the dia dataset yielded similar go term enrichment for rna-binding proteins involved in cellular stress responses to the dda dataset while detecting extra proteins unseen by dda. overall, the dia silac approach improved detection of proteins over conventional dda silac for generating rna-interactome datasets, at a lower cost due to reduced machine time."
https://doi.org/10.1101/2022.01.25.477595,2023-12-22,a computational method for predicting the most likely evolutionary trajectories in the stepwise accumulation of resistance mutations,"['Eccleston, R. C.; Manko, E.; Campino, S.; Clark, T. G.; Furnham, N.']","pathogen evolution of drug resistance often occurs in a stepwise manner via the accumulation of multiple mutations that in combination have a non-additive impact on fitness, a phenomenon known as epistasis. the evolution of resistance via the accumulation of point mutations in the dhfr genes of plasmodium falciparum (pf) and plasmodium vivax (pv) has been studied extensively and multiple studies have shown epistatic interactions between these mutations determine the accessible evolutionary trajectories to highly resistant multiple mutations. here, we simulated these evolutionary trajectories using a model of molecular evolution, parameterized using rosetta flex ddg predictions, where selection acts to reduce the target-drug binding affinity. we observe strong agreement with pathways determined using experimentally measured ic50 values of pyrimethamine binding, which suggests binding affinity is strongly predictive of resistance and epistasis in binding affinity strongly influences the order of fixation of resistance mutations. we also infer pathways directly from the frequency of mutations found in isolate data, and observe remarkable agreement with the most likely pathways predicted by our mechanistic model, as well as those determined experimentally. this suggests mutation frequency data can be used to intuitively infer evolutionary pathways, provided sufficient sampling of the population."
https://doi.org/10.1101/2023.02.27.530360,2023-12-21,elucidating molecular mechanisms of protoxin-2 state-specific binding to the human nav1.7 channel,"['Ngo, K.; Mateos, D. L.; Rouen, K.; Han, Y.; Wulff, H.; Clancy, C. E.; Yarov-Yarovoy, V.; Vorobyov, I.']","human voltage-gated sodium (hnav) channels are responsible for initiating and propagating action potentials in excitable cells and mutations have been associated with numerous cardiac and neurological disorders. hnav1.7 channels are expressed in peripheral neurons and are promising targets for pain therapy. the tarantula venom peptide protoxin-2 (ptx2) has high selectivity for hnav1.7 and serves as a valuable scaffold to design novel therapeutics to treat pain. here, we used computational modeling to study the molecular mechanisms of the state-dependent binding of ptx2 to hnav1.7 voltage-sensing domains (vsds). using rosetta structural modeling methods, we constructed atomistic models of the hnav1.7 vsd ii and iv in the activated and deactivated states with docked ptx2. we then performed microsecond-long all-atom molecular dynamics (md) simulations of the systems in hydrated lipid bilayers. our simulations revealed that ptx2 binds most favorably to the deactivated vsd ii and activated vsd iv. these state-specific interactions are mediated primarily by ptx2s residues r22, k26, k27, k28, and w30 with vsd as well as the surrounding membrane lipids. our work revealed important protein-protein and protein-lipid contacts that contribute to high-affinity state-dependent toxin interaction with the channel. the workflow presented will prove useful for designing novel peptides with improved selectivity and potency for more effective and safe treatment of pain.  summarynav1.7, a voltage-gated sodium channel, plays a crucial role in pain perception and is specifically targeted by ptx2, which serves as a template for designing pain therapeutics. in this study, ngo et al. employed computational modeling to evaluate the state-dependent binding of ptx2 to nav1.7."
https://doi.org/10.1101/2023.04.29.538834,2024-01-15,detection of ghost introgression from phylogenomic data requires a full-likelihood approach,"['Pang, X.-X.; Zhang, D.-Y.']","ao_scplowbstractc_scplowin recent years, the study of hybridization and introgression has made significant progress, with ghost introgression - the transfer of genetic material from extinct or unsampled lineages to extant species - emerging as a key area for research. accurately identifying ghost introgression, however, presents a challenge. to address this issue, we focused on simple cases involving three species with a known phylogenetic tree. using mathematical analyses and simulations, we evaluated the performance of popular phylogenetic methods, including hyde and phylonet/mpl, and the full-likelihood method, bayesian phylogenetics and phylogeography (bpp), in detecting ghost introgression. our findings suggest that heuristic approaches relying on site patterns or gene tree topologies struggle to differentiate ghost introgression from introgression between sampled non-sister species, frequently leading to incorrect identification of donor and recipient species. the full-likelihood method bpp using multilocus sequence alignments, by contrast, is capable of detecting ghost introgression in phylogenomic datasets. we analyzed a real-world phylogenomic dataset of 14 species of jaltomata (solanaceae) to showcase the potential of full-likelihood methods for accurate inference of introgression."
https://doi.org/10.1101/2023.04.21.537581,2024-01-18,translating deep learning to neuroprosthetic control,"['Deo, D. R.; Willett, F. R.; Avansino, D. T.; Hochberg, L. R.; Henderson, J. M.; Shenoy, K. V.']","advances in deep learning have given rise to neural network models of the relationship between movement and brain activity that appear to far outperform prior approaches. brain-computer interfaces (bcis) that enable people with paralysis to control external devices, such as robotic arms or computer cursors, might stand to benefit greatly from these advances. we tested recurrent neural networks (rnns) on a challenging nonlinear bci problem: decoding continuous bimanual movement of two computer cursors. surprisingly, we found that although rnns appeared to perform well in offline settings, they did so by overfitting to the temporal structure of the training data and failed to generalize to real-time neuroprosthetic control. in response, we developed a method that alters the temporal structure of the training data by dilating/compressing it in time and re-ordering it, which we show helps rnns successfully generalize to the online setting. with this method, we demonstrate that a person with paralysis can control two computer cursors simultaneously, far outperforming standard linear methods. our results provide evidence that preventing models from overfitting to temporal structure in training data may, in principle, aid in translating deep learning advances to the bci setting, unlocking improved performance for challenging applications."
https://doi.org/10.1101/2023.05.03.539297,2024-01-22,small-molecule binding to an intrinsically disordered protein revealed by experimental nmr 19f transverse spin-relaxation,"['Heller, G. T.; Shukla, V. K.; Figueiredo, A. M.; Hansen, D. F.']","intrinsically disordered proteins are highly dynamic biomolecules that rapidly interconvert between many structural conformations. traditionally, these proteins have been considered un-druggable because of their lack of classical long-lived binding pockets. recent evidence suggests that intrinsically disordered proteins can bind small, drug-like molecules, however, there are limited approaches to characterize these interactions experimentally. here we demonstrate that ligand-detected 19f transverse relaxation rates (r2) obtained from nuclear magnetic resonance spectroscopy are highly sensitive to the interaction between a small-molecule and an intrinsically disordered protein, in contrast to chemical shift perturbations which are minimally sensitive for this interaction. with this method, we show that the small molecule, 5-fluoroindole, interacts with the disordered domains of non-structural protein 5a from hepatitis c virus with a kd of 260 {+/-} 110 m. we also demonstrate that 5-fluoroindole remains highly dynamic in the bound form. our findings suggest that ligand-detected 19f transverse relaxation measurements could represent a highly effective screening strategy to identify molecules capable of interacting with these traditionally elusive, dynamic biomolecules."
https://doi.org/10.1101/2023.06.08.544243,2024-01-29,autoencoder model for translating omics signatures,"['Meimetis, N.; Pullen, K. M.; Zhu, D. Y.; Hoang, T. N.; Magliacane, S.; Lauffenburger, D. A.']","the development of effective therapeutics and vaccines for human diseases requires a systematic understanding of human biology. while animal and in vitro culture models have successfully elucidated the molecular mechanisms of diseases in many studies, they yet fail to adequately recapitulate human biology as evidenced by the predominant likelihood of failure in clinical trials. to address this broadly important problem, we developed autotransop, a neural network autoencoder framework to map omics profiles from designated species or cellular contexts into a global latent space, from which germane information can be mapped between different contexts. this approach performs as well or better than extant machine learning methods and can identify animal/culture-specific molecular features predictive of other contexts, without requiring homology matching. for an especially challenging test case, we successfully apply our framework to a set of inter-species vaccine serology studies, where no 1-1 mapping between human and non-human primate features exists."
https://doi.org/10.1101/2022.04.15.487700,2024-01-26,estimation of cochlear frequency selectivity using a convolution model of forward-masked compound action potentials,"['Deloche, F.; Parida, S.; Sivaprakasam, A.; Heinz, M. G.']","frequency selectivity is a fundamental property of the peripheral auditory system; however, the invasiveness of auditory nerve (an) experiments limits its study in the human ear. compound action potentials (caps) associated with forward-masking have been suggested as an alternative means to assess cochlear frequency selectivity. previous methods relied on an empirical comparison of an and cap tuning curves in animal models, arguably not taking full advantage of the information contained in forward-masked caps. in this work, we seek to provide a direct estimate of the quality factor characterizing an frequency tuning using many forward-masked cap responses. the method is based on a convolution model of the cap that takes into account the masking of an populations induced by notched-noise maskers with various notch widths and attenuations. the model produces masking patterns that, once convolved by a unitary response, predict forward-masked cap waveforms. model parameters, including those characterizing frequency selectivity, are fine-tuned by minimizing waveform prediction errors across the different masking conditions, yielding robust estimates. the method was applied to click-evoked caps at the round window of anesthetized chinchillas. the estimated quality factor q10 as a function of center frequency is shown to closely match the average quality factor obtained from an-fiber tuning curves, without the need for an empirical correction factor. beyond the estimation of frequency selectivity, the proposed model proves to be accurate in predicting forward-masked cap responses, and therefore could be extended to study more complex aspects of cochlear signal processing using a similar experimental approach."
https://doi.org/10.1101/2023.06.12.544410,2024-01-25,"tksm: highly modular, user-customizable, and scalable transcriptomic sequencing long-read simulator","['Karaoglanoglu, F.; Orabi, B.; Flannigan, R.; Chauve, C.; Hach, F.']","motivationtranscriptomic long-read (lr) sequencing is an increasingly cost-effective technology for probing various rna features. numerous tools have been developed to tackle various transcriptomic sequencing tasks (e.g. isoform and gene fusion detection). however, the lack of abundant gold standard datasets hinders the benchmarking of such tools. therefore, simulation of lr sequencing is an important and practical alternative to enable the assessment of these tools. while the existing lr simulators aim to imitate the sequencing machine noise and to target specific library protocols, they lack some important library preparation steps (e.g. pcr) and are difficult to modify to new and changing library preparation techniques (e.g. single-cell lrs).  resultswe present tksm, a modular and scalable lr simulator. tksm is designed so that each rna modification step is targeted explicitly by a software module. this allows the user to assemble a simulation pipeline of any combination of tksm modules to emulate the sequencing design the user is targeting. additionally, the input/output of all the core modules of tksm follow the same simple format (molecule description format) allowing the user to easily extend tksm with new modules targeting new library preparation steps.  availabilitytksm is available as an open source software at https://github.com/vpc-ccg/tksm."
https://doi.org/10.1101/2023.06.14.544900,2024-01-16,structural assembly of the bacterial essential interactome,"['Torrent, M.; Gomez Borrego, J.']","the study of protein interactions in living organisms is fundamental to understanding biological processes and central metabolic pathways. however, our understanding of the bacterial interactome remains limited, hindering the identification of new drug targets and the development of new therapeutic strategies. here, we predict the assembly of the essential proteins in bacteria using the deep learning protein folding algorithm alphafold2. we modeled 1089 interactions between essential proteins in bacteria and generated 115 high-accuracy models. our analysis reveals previously unknown details about the assembly mechanisms of these complexes, highlighting the importance of specific structural features in their stability and function. furthermore, we identify several novel protein-protein interactions that provide new targets for drug development. our work provides a framework for predicting the interactome of other bacteria and highlights the potential of deep learning algorithms in advancing our understanding of the complex biology of living organisms."
https://doi.org/10.1101/2023.10.22.563383,2024-02-09,deep dnashape: predicting dna shape considering extended flanking regions using a deep learning method,"['Li, J.; Chiu, T.-P.; Rohs, R.']","understanding the mechanisms of protein-dna binding is critical in comprehending gene regulation. three-dimensional dna shape plays a key role in these mechanisms. in this study, we present a deep learning-based method, deep dnashape, that fundamentally changes the current k-mer based high-throughput prediction of dna shape features by accurately accounting for the influence of extended flanking regions, without the need for extensive molecular simulations or structural biology experiments. by using the deep dnashape method, refined dna shape features can be predicted for any length and number of dna sequences in a high-throughput manner, providing a deeper understanding of the effects of flanking regions on dna shape in a target region of a sequence. deep dnashape method provides access to the influence of distant flanking regions on a region of interest. our findings reveal that dna shape readout mechanisms of a core target are quantitatively affected by flanking regions, including extended flanking regions, providing valuable insights into the detailed structural readout mechanisms of protein-dna binding. furthermore, when incorporated in machine learning models, the features generated by deep dnashape improve the model prediction accuracy. collectively, deep dnashape can serve as a versatile and powerful tool for diverse dna structure-related studies."
https://doi.org/10.1101/2023.07.27.550798,2024-01-25,"the mediterranean mussel, mytilus galloprovincialis, a novel model for developmental studies of mollusks","['Miglioli, A.; Tredez, M.; Boosten, M.; Sant, C.; Carvalho, J. E.; Dru, P.; Canesi, L.; Schubert, M.; Dumollard, R.']","a model organism in developmental biology is defined by its experimental amenability as well as by resources created for the model system by the scientific community. for the most powerful models, the combination of both has already yielded a thorough understanding of development. however, the number of developmental model systems is still very limited, and their phylogenetic distribution is heavily biased. members of one of the largest animal phyla, the mollusks, for example, have long been neglected as developmental model organisms. to remedy this shortcoming, we produced a detailed developmental transcriptome for the mediterranean mussel mytilus galloprovincialis, a bivalve mollusk, and expanded the list of experimental protocols available for this species. our high-quality transcriptome allowed us to identify transcriptomic signatures of developmental transitions and to perform a first comparison with the pacific oyster crassostrea gigas that can be used in future multi-species analyses. to allow co-labelling studies, we optimized protocols for immunohistochemistry and hybridization chain reaction and combined both techniques to create high-resolution co-expression maps of developmental genes. the resources and protocols we describe here thus represent an enormous boost for the establishment of the mediterranean mussel as a laboratory model in developmental biology.  summary statementresources and techniques are described for the mediterranean mussel mytilus galloprovincialis, which, together, establish a novel model system for studying mollusk development and animal evolution."
https://doi.org/10.1101/2023.02.03.527053,2024-01-25,gene panel selection for targeted spatial transcriptomics,"['Zhang, Y.; Petukhov, V.; Biederstedt, E.; Kharchenko, P. V.']","targeted spatial transcriptomics hold particular promise in analysis of complex tissues. most such methods, however, measure only a limited panel of transcripts, which need to be selected in advance to inform on the cell types or processes being studied. a limitation of existing gene selection methods is that they rely on scrna-seq data, ignoring platform effects between technologies. here we describe gpsfish, a computational method to perform gene selection through optimizing detection of known cell types. by modeling and adjusting for platform effects, gpsfish outperforms other methods. furthermore, gpsfish can incorporate cell type hierarchies and custom gene preferences to accommodate diverse design requirements."
https://doi.org/10.1101/2023.09.28.558338,2024-01-25,seizure event detection using intravital two-photon calcium imaging data,"['Stern, M. A.; Cole, E. R.; Gross, R. E.; Berglund, K.']","significancegenetic cellular calcium imaging has emerged as a powerful tool to investigate how different types of neurons interact at the microcircuit level to produce seizure activity, with newfound potential to understand epilepsy. although many methods exist to measure seizure-related activity in traditional electrophysiology, few yet exist for calcium imaging.  aimto demonstrate an automated algorithmic framework to detect seizure-related events using calcium imaging - including the detection of pre-ictal spike events, propagation of the seizure wavefront, and terminal spreading waves for both population-level activity and that of individual cells.  approachwe developed an algorithm for precise recruitment detection of population and individual cells during seizure-associated events, which broadly leverages averaged population activity and high-magnitude slope features to detect single-cell pre-ictal spike and seizure recruitment. we applied this method to data recorded using awake in vivo two-photon calcium imaging during pentylenetetrazol induced seizures in mice.  resultswe demonstrate that our detected recruitment times are concordant with visually identified labels provided by an expert reviewer and are sufficiently accurate to model the spatiotemporal progression of seizure-associated traveling waves.  conclusionsour algorithm enables accurate cell recruitment detection and will serve as a useful tool for researchers investigating seizure dynamics using calcium imaging."
https://doi.org/10.1101/2023.10.08.561377,2024-01-25,diffdec: structure-aware scaffold decoration with an end-to-end diffusion model,"['Xie, J.; Chen, S.; lei, j.; Yang, Y.']","in molecular optimization, one popular way is r-groups decoration on molecular scaffolds, and many efforts have been put to generate r-groups based on deep generative models. however, these methods mostly use information of known binding ligands, without fully utilizing target structure information. in this study, we proposed a new method, diffdec, to involve three-dimensional pocket constraints by a modified diffusion technique for optimizing molecules through molecular scaffold decoration. for an end-to-end generation of r-groups with different sizes, we designed a novel fake atom mechanism. diffdec was shown able to generate structure-aware r-groups, and simultaneously generate multiple r-groups for one scaffold on different growth anchors. the growth anchors could be provided by users or automatically determined by our model. diffdec achieved r-group recovery rates of 69.67% and 45.34% in the single and multiple r-group decoration tasks, respectively, and these values were significantly higher than competing methods (37.33% and 26.85%). according to the molecular docking study, our decorated molecules obtained better average binding affinity than baseline methods. the docking pose analysis revealed that diffdec could decorate scaffolds with r-groups that exhibited improved binding affinities and more favourable interactions with the pocket. these results demonstrated the potential and applicability of diffdec in real-world scaffold decoration for molecular optimization."
https://doi.org/10.1101/2023.10.10.561763,2024-01-24,chemi-northern: a versatile chemiluminescent northern blot method for analysis and quantitation of rna molecules.,"['McKenney, K. M.; Connacher, R. P.; Dunshee, E. B.; Goldstrohm, A. C.']","this report describes a chemiluminescence-based detection method for rnas on northern blots, designated chemi-northern. this approach builds on the simplicity and versatility of northern blotting, while dispensing of the need for expensive and cumbersome radioactivity. rnas are first separated on denaturing gel electrophoresis, transferred to a nylon membrane, and then hybridized to a biotinylated rna or dna antisense probe. streptavidin conjugated with horseradish peroxidase and enhanced chemiluminescence substrate are then used to detect the probe bound to the target rna. our results demonstrate the versatility of this method in detecting natural and engineered rnas expressed in cells, including messenger and noncoding rnas. we show that chemi-northern detection is sensitive and fast, detecting attomole amounts of rna in as little as 1 second, with high signal intensity and low background. the dynamic response displays excellent linearity. using chemi-northern, we measure the significant, reproducible reduction of mrna levels by human sequence-specific rna-binding proteins, pum1 and pum2. additionally, we measure the interaction of endogenous poly(a) binding protein, pabpc1, with poly-adenylated mrna. thus, the chemi-northern method provides a versatile, simple, cost-effective method to enable researchers to detect and measure changes in rna expression, processing, binding, and decay of rnas."
https://doi.org/10.1101/2022.11.30.518528,2024-01-24,a deep learning approach for improving two-photon vascular imaging speeds,"['Zhou, A.; Mihelic, S.; Engelmann, S.; Tomar, A.; Dunn, A. K.; Narasimhan, V. M.']","a potential method for tracking neurovascular disease progression over time in preclinical models is multiphoton fluorescence microscopy (mpm), which can image cerebral vasculature with capillary-level resolution. however, obtaining high-quality, three-dimensional images with a traditional point scanning mpm is time-consuming and limits sample sizes for chronic studies. here, we present a convolutional neural network-based algorithm for fast upscaling of low-resolution or sparsely sampled images and combine it with a segmentation-less vectorization process for 3d reconstruction and statistical analysis of vascular network structure. in doing so, we also demonstrate that the use of semi-synthetic training data can replace the expensive and arduous process of acquiring low- and high-resolution training pairs without compromising vectorization outcomes, and thus open the possibility of utilizing such approaches for other mpm tasks where collecting training data is challenging. we applied our approach to large field of view images and show that our method generalizes across imaging depths, disease states and other differences in neurovasculature. our pre-trained models and lightweight architecture can be used to reduce mpm imaging time by up to fourfold without any changes in underlying hardware, thereby enabling deployability across a range of settings."
https://doi.org/10.1101/2022.10.31.514489,2024-01-24,long-term bird abundance in a threatened grassland ecosystem: an assessment using daily bird lists,"['Bharadwaj, A.; Mhamane, S.; Bangal, P.; Menon, T.; Isvaran, K.; Quader, S.']","open natural ecosystems (ones), such as tropical grasslands, are among the most threatened habitats on earth today. the long-term monitoring of ones is an important research domain that is essential for understanding anthropogenic impacts and facilitating conservation action. using a simple day-listing method over a 13-year period, we studied species trends in a central indian grassland-agriculture mosaic experiencing several land-use changes. our results indicate that some grassland species (such as the great indian bustard ardeotis nigriceps) show steep declines during the study period, while other generalist species (such as the indian peafowl pavo cristatus) show an increasing trend. daily listing also reveals distinct seasonal patterns, and we discuss the great indian bustard and western marsh harrier circus aeruginosus as examples. our study highlights the utility of consistent checklist surveys to monitor population trends of bird communities within a changing landscape."
https://doi.org/10.1101/2023.11.09.566399,2024-01-24,spred: a simulation-supervised neural network tool for gene regulatory network reconstruction,"['Wu, Z.; Sinha, S.']","reconstruction of gene regulatory networks (grns) from expression data is a significant open problem. common approaches train a machine learning (ml) model to predict a genes expression using transcription factors (tfs) expression as features and designate important features/tfs as regulators of the gene. here, we present an entirely different paradigm, where grn edges are directly predicted by the ml model. the new approach, named ""spred"" is a simulation-supervised neural network for grn inference. its inputs comprise expression relationships (e.g., correlation, mutual information) between the target gene and each tf and between pairs of tfs. the output includes binary labels indicating whether each tf regulates the target gene. we train the neural network model using synthetic expression data generated by a biophysics-inspired simulation model that incorporates linear as well as non-linear tf-gene relationships and diverse grn configurations. we show spred to outperform state-of-the-art grn reconstruction tools genie3, ennet, portia and tigress on synthetic datasets with high co-expression among tfs, similar to that seen in real data. a key advantage of the new approach is its robustness to relatively small numbers of conditions (columns) in the expression matrix, which is a common problem faced by existing methods. finally, we evaluate spred on real data sets in yeast that represent gold standard benchmarks of grn reconstruction and show it to perform significantly better than or comparably to existing methods. in addition to its high accuracy and speed, spred marks a first step towards incorporating biophysics principles of gene regulation into ml-based approaches to grn reconstruction."
https://doi.org/10.1101/2022.12.03.518995,2024-01-23,cooperativity between cas9 and aid establishes broad and diversifying mutational footprints in base editors,"['Berrios, K. N.; Barka, A.; Gill, J.; Evitt, N. H.; Gajula, K. S.; Shi, J.; Kohli, R. M.']","the partnership of dna deaminase enzymes with crispr-cas nucleases is now a well-established method to enable targeted genomic base editing. however, an understanding of how cas9 and dna deaminases collaborate to shape base editor (be) outcomes has been lacking. here, we support a novel mechanistic model of base editing by deriving a range of hyperactive activation-induced deaminase (aid) base editors (hbes) and exploiting their characteristic diversifying activity. our model involves multiple layers of previously underappreciated cooperativity in be steps including: (1) cas9 binding can potentially expose both dna strands for  capture by the deaminase, a feature that is enhanced by guide rna mismatches; (2) after strand capture, the intrinsic activity of the dna deaminase can tune window size and base editing efficiency; (3) cas9 defines the boundaries of editing on each strand, with deamination blocked by cas9 binding to either the pam or the protospacer; and (4) non-canonical edits on the guide rna bound strand can be further elicited by changing which strand is nicked by cas9. leveraging insights from our mechanistic model, we create novel hbes that can remarkably generate simultaneous c>t and g>a transitions over >65 bp with significant potential for targeted gene diversification."
https://doi.org/10.1101/2023.08.15.553430,2024-01-23,"predicting host-based, synthetic lethal antiviral targets from omics data","['Staheli, J. P.; Neal, M. L.; Navare, A.; Mast, F. D.; Aitchison, J. D.']","traditional antiviral therapies often have limited effectiveness due to toxicity and development of drug resistance. host-based antivirals, while an alternative, may lead to non-specific effects. recent evidence shows that virus-infected cells can be selectively eliminated by targeting synthetic lethal (sl) partners of proteins disrupted by viral infection. thus, we hypothesized that genes depleted in crispr ko screens of virus-infected cells may be enriched in sl partners of proteins altered by infection. to investigate this, we established a computational pipeline predicting sl drug targets of viral infections. first, we identified sars-cov-2-induced changes in gene products via a large compendium of omics data. second, we identified sl partners for each altered gene product. last, we screened crispr ko data for sl partners required for cell viability in infected cells. despite differences in virus-induced alterations detected by various omics data, they share many predicted sl targets, with significant enrichment in crispr ko-depleted datasets. comparing data from sars-cov-2 and influenza infections, we found possible broad-spectrum, host-based antiviral sl targets. this suggests that crispr ko data are replete with common antiviral targets due to their sl relationship with virus-altered states and that such targets can be revealed from analysis of omics datasets and sl predictions."
https://doi.org/10.1101/2023.10.08.561441,2024-01-23,pindel-td: a tandem duplication detector based on a pattern growth approach,"['Yang, X.; Zheng, G.; Jia, P.; Wang, S.; Ye, K.']","tandem duplication (td) is a major type of structural variation (sv), and plays an important role in novel gene formation and human diseases. however, tds are often missed or incorrectly classified as insertions by most of modern sv detection methods due to the lacking of specialized operation on td related mutational signals. herein, we developed a td detection module of pindel referred as pindel-td based on a td specific pattern growth approach. pindel-td detects tds with a wide size range at single nucleotide resolution. using simulation and real read data of hg002, we demonstrate that pindel-td outperformed other leading methods in terms of precision, recall, f1-score and robustness. further applying pindel-td on data generated from k562 cancer cell line, we identified a td located at the seventh exon of sage1, explaining its high expression. pindel-td is available at https://github.com/xjtu-omics/pindel and free for non-commercial use."
https://doi.org/10.1101/852210,2024-01-23,mity: a highly sensitive mitochondrial variant analysis pipeline for whole genome sequencing data,"['Puttick, C.; Kumar, K. R.; Davis, R. L.; Pinese, M.; Thomas, D. M.; Dinger, M. E.; Sue, C. M.; Cowley, M. J.']","motivationmitochondrial diseases (mds) are the most common group of inherited metabolic disorders and are often challenging to diagnose due to extensive genotype-phenotype heterogeneity. mds are caused by mutations in the nuclear or mitochondrial genome, where pathogenic mitochondrial variants are usually heteroplasmic and typically at much lower allelic fraction in the blood than affected tissues. both genomes can now be readily analysed using unbiased whole genome sequencing (wgs), but most nuclear variant detection methods fail to detect low heteroplasmy variants in the mitochondrial genome.  resultswe present mity, a bioinformatics pipeline for detecting and interpreting heteroplasmic snvs and indels in the mitochondrial genome using wgs data. in 2,980 healthy controls, we observed on average 3,166x coverage in the mitochondrial genome using wgs from blood. mity utilises this high depth to detect pathogenic mitochondrial variants, even at low heteroplasmy. mity enables easy interpretation of mitochondrial variants and can be incorporated into existing diagnostic wgs pipelines. this could simplify the diagnostic pathway, avoid invasive tissue biopsies and increase the diagnostic rate for mds and other conditions caused by impaired mitochondrial function.  availabilitymity is available from https://github.com/kccg/mityunder an mit license.  contactclare.puttick@crick.ac.uk, carolyn.sue@sydney.edu.au, mcowley@ccia.org.au"
https://doi.org/10.1101/2023.07.21.550033,2023-12-20,source-free random forest model calibration for myoelectric control,"['Jiang, X.; Ma, C.; Nazarpour, K.']","objectivemost existing machine learning models for myoelectric control require a large amount of data to learn user-specific characteristics of the electromyographic (emg) signals, which is burdensome. our objective is to develop an approach to enable the calibration of a pre-trained model with minimal data from a new myoelectric user.  approachwe trained a random forest model with emg data from 20 people collected during the performance of multiple hand grips. to adapt the decision rules for a new user, first, the branches of the pre-trained decision trees were pruned using the validation data from the new user. then new decision trees trained merely with data from the new user were appended to the pruned pre-trained model.  resultsreal-time myoelectric experiments with 18 participants over two days demonstrated the improved accuracy of the proposed approach when compared to benchmark user-specific random forest and the linear discriminant analysis models. furthermore, the random forest model that was calibrated on day one for a new participant yielded significantly higher accuracy on day two, when compared to the benchmark approaches, which reflects the robustness of the proposed approach.  significancethe proposed model calibration procedure is completely source-free, that is, once the base model is pre-trained, no access to the source data from the original 20 people is required. our work promotes the use of efficient, explainable, and simple models for myoelectric control."
https://doi.org/10.1101/2023.08.20.553608,2024-01-16,improved modeling of rna-binding protein motifs in an interpretable neural model of rna splicing,"['Gupta, K.; Yang, C.; McCue, K.; Bastani, O.; Sharp, P.; Burge, C.; Lezama, A. S.']","sequence-specific rna-binding proteins (rbps) play central roles in splicing decisions, but their exact binding locations and activities are difficult to predict. here, we describe a modular splicing architecture that leverages in vitro-derived rna affinity models for 79 human rbps and the annotated human genome to produce improved models of rbp binding and activity. binding and activity are modeled by separate motif and aggregator components that can be mixed and matched, enforcing sparsity to improve interpretability. standard affinity models yielded reasonable predictions, but substantial improvements resulted from using a new adjusted motif (am) architecture. while maintaining accurate modeling of in vitro binding, training these ams on the splicing task yielded improved predictions of binding sites in vivo and of splicing activity, using independent crosslinking and massively parallel splicing reporter assay data. the modular structure of our model enables improved generalizability to other species (insects, plants) and to exons of different evolutionary ages."
https://doi.org/10.1101/2023.04.13.536830,2024-01-17,high-resolution tracking of unconfined zebrafish behavior reveals stimulatory and anxiolytic effects of psilocybin,"['Braun, D.; Rosenberg, A.; Haruvi, R.; Malamud, D.; Barbara, R.; Kawashima, T.']","serotonergic psychedelics are emerging therapeutics for psychiatric disorders, yet their underlying mechanisms of action in the brain remain largely elusive. zebrafish have evolutionarily conserved serotonergic circuits and subcortical targets such as the brainstem regions and the cerebellum, providing a promising model for studying the subcortical effects of serotonergic drugs. here, we developed a wide-field behavioral tracking system for larval zebrafish and investigated the effects of psilocybin, a psychedelic serotonin receptor agonist. machine learning analyses of precise body kinematics identified latent behavioral states reflecting spontaneous exploration, visually-driven rapid swimming, and irregular swim patterns following stress exposure. using this method, we identified two main behavioral effects of acute psilocybin treatment: [i] increased rapid swimming in the absence of visual stimuli and [ii] prevention of irregular swim patterns following stress exposure. together, these effects indicate that psilocybin induces a brain state that is both stimulatory and anxiolytic. these findings pave the way for using larval zebrafish to elucidate subcortical mechanisms underlying the behavioral effects of serotonergic psychedelics."
https://doi.org/10.1101/2023.02.03.526944,2023-12-26,mechanisms of sars-cov-2 inactivation using uvc laser radiation,"['Devitt, G.; Johnson, P. B.; Hanrahan, N.; Lane, S. I. R.; Vidale, M. C.; Sheth, B.; Allen, J. D.; Humbert, M. V.; Spalluto, C. M.; Herve, R. C.; Staples, K.; West, J.; Forster, R.; Divecha, N.; McCormick, C. J.; Crispin, M.; Hempler, N.; Malcolm, G. P. A.; Mahajan, S.']","severe acute respiratory syndrome coronavirus 2 (sars-cov-2) has had a tremendous impact on humanity. prevention of transmission by disinfection of surfaces and aerosols through a chemical-free method is highly desirable. ultraviolet c (uvc) light is uniquely positioned to achieve inactivation of pathogens. we report the inactivation of sars-cov-2 virus by uvc radiation and explore its mechanisms. a dose of 50mj/cm2 using a uvc laser at 266nm achieved an inactivation efficiency of 99.89%, whilst infectious virions were undetectable at 75mj/cm2 indicating >99.99% inactivation. infection by sars-cov-2 involves viral entry mediated by the spike glycoprotein (s), and viral reproduction, reliant on translation of its genome. we demonstrate that uvc radiation damages ribonucleic acid (rna) and provide in-depth characterisation of uvc-induced damage of the s protein. we find that uvc severely impacts sars-cov-2 spike proteins ability to bind human angiotensin-converting enzyme 2 (hace2) and this correlates with loss of native protein conformation and aromatic amino acid integrity. this report has important implications for the design and development of rapid and effective disinfection systems against the sars-cov-2 virus and other pathogens."
https://doi.org/10.1101/2023.09.27.559854,2024-01-24,automoldesigner for antibiotic discovery: an ai-based open-source software for automated design of small-molecule antibiotics,"['Shen, T.; Guo, J.; Han, Z.; Zhang, G.; Liu, Q.; Si, X.; Wang, D.; Wu, S.; Xia, J.']","discovery of small-molecule antibiotics with novel chemotypes serves as one of the essential strategies to address antibiotic resistance. although a considerable number of computational tools committed to molecular design have been reported, there is a deficit in the holistic and efficient tool specifically developed for small-molecule antibiotic discovery. to address this issue, we report automoldesigner, a computational modeling software dedicated to small-molecule antibiotic design. it is a generalized framework comprising two functional modules, i.e., generative deep learning-enabled molecular generation and automated machine learning based-antibacterial activity/property prediction, wherein individually trained models and curated datasets are out-of-the-box for whole cell-based antibiotic screening and design. it is open-source thus allows for the incorporation of new features for flexible use. unlike most software programs based on linux and command lines, this application equipped with qt-based graphical user interface can be run on personal computers with multiple operating systems, making it much easier to use for experimental scientists. the software and related materials are freely available at github (https://github.com/taoshen99/automoldesigner) and zenodo (https://zenodo.org/record/8366085)."
https://doi.org/10.1101/2023.11.15.567294,2024-01-17,atom filtering algorithm and gpu-accelerated calculation of simulation atomic force microscopy images,"['Amyot, R.; Kodera, N.; Flechsig, H.']","simulation atomic force microscopy computationally emulates experimental scanning of a biomolecular structure to produce topographic images that can be correlated with measured images. its application to the enormous amount of available high-resolution structures, as well as to molecular dynamics modelling data, facilitates the quantitative interpretation of experimental observations by inferring atomistic information from resolution-limited measured topographies. the computation required to generate a simulated afm image generally includes the calculation of contacts between the scanning tip and all atoms from the biomolecular structure. however, since only contacts with surface atoms are relevant, a filtering method shall highly improve the efficiency of simulation afm computations. in this report we address this issue and present an elegant solution based on graphics processing unit (gpu) computations that significantly accelerates the computation of simulation afm images. the method not only allows for visualization of biomolecular structures combined with ultra-fast synchronized calculation and graphical representation of corresponding simulated afm images (live simulation afm), but, as we demonstrate, can also reduce the computational effort during automatized fitting of atomistic structures into measured afm topographies by orders of magnitude. hence, the developed method will play an important role in post-experimental computational analysis involving simulation afm. implementation is realized in our bioafmviewer software package for simulation afm of biomolecular structures and dynamics."
https://doi.org/10.1101/2023.04.25.538301,2024-01-17,dorsal peduncular cortex activity modulates affective behaviors in mice,"['Botterill, J. J.; Khlaifia, A.; Appings, R.; Premachandran, H.; Cruz-Sanchez, A.; Arruda-Carvalho, M.']","the medial prefrontal cortex (mpfc) is critical to cognitive and emotional function and underlies many neuropsychiatric disorders, including mood, fear and anxiety disorders. in rodents, disruption of mpfc activity affects anxiety- and depression-like behavior, with specialized contributions from its subdivisions. the rodent mpfc is divided into the dorsomedial prefrontal cortex (dmpfc), spanning the anterior cingulate cortex (acc) and dorsal prelimbic cortex (pl), and the ventromedial prefrontal cortex (vmpfc), which includes the ventral pl, infralimbic cortex (il), and in some studies the dorsal peduncular cortex (dp) and dorsal tenia tecta (dtt). the dp/dtt have recently been implicated in the regulation of stress- induced sympathetic responses via projections to the hypothalamus. while many studies implicate the pl and il in anxiety-, depression-like and fear behavior, the contribution of the dp/dtt to affective and emotional behavior remains unknown. here, we used chemogenetics and optogenetics to bidirectionally modulate dp/dtt activity and examine its effects on affective behaviors, fear and stress responses in c57bl/6j mice. acute chemogenetic activation of dp/dtt significantly increased anxiety-like behavior in the open field and elevated plus maze tests, as well as passive coping in the tail suspension test. dp/dtt activation also led to an increase in serum corticosterone levels and facilitated auditory fear extinction learning and retrieval. activation of dp/dtt projections to the dorsomedial hypothalamus (dmh) acutely decreased freezing at baseline and during extinction learning, but did not alter affective behavior. these findings point to the dp/dtt as a new regulator of affective behavior and fear extinction in mice."
https://doi.org/10.1101/2023.10.04.560913,2024-02-11,mtx-cobra: subcellular localization prediction for bacterial proteins,"['Arora, I.; Kummer, A.; Zhou, H.; Gadjeva, M.; Ma, E.; Chuang, G.-Y.; Ong, E.']","backgroundbacteria can have beneficial effects on our health and environment; however, many are responsible for serious infectious diseases, warranting the need for vaccines against such pathogens. bioinformatic and experimental technologies are crucial for the development of vaccines. the vaccine design pipeline requires identification of bacteria-specific antigens that can be recognized and induce a response by the immune system upon infection. immune system recognition is influenced by the location of a protein. methods have been developed to determine the subcellular localization (scl) of proteins in prokaryotes and eukaryotes. bioinformatic tools such as psortb can be employed to determine scl of proteins, which would be tedious to perform experimentally. unfortunately, psortb often predicts many proteins as having an ""unknown"" scl, reducing the number of antigens to evaluate as potential vaccine targets.  methodwe present a new pipeline called subcellular localization prediction for bacterial proteins (mtx-cobra). mtx-cobra uses metas protein language model, evolutionary scale modeling, combined with an extreme gradient boosting machine learning model to identify scl of bacterial proteins based on amino acid sequence. this pipeline is trained on a curated dataset that combines data from uniprot and the publicly available epsortdb dataset.  resultsusing benchmarking analyses, nested 5-fold cross-validation, and leave-one-pathogen-out methods, followed by testing on the held-out dataset, we show that our pipeline predicts the scl of bacterial proteins more accurately than psortb.  conclusionsmtx-cobra provides an accessible pipeline with greater efficiency to classify bacterial proteins with currently ""unknown"" scls than existing bioinformatic and experimental methods."
https://doi.org/10.1101/2021.10.20.464614,2024-01-17,markerless mouse tracking for social experiments,"['Le, V. A.; Sterley, T.-L.; Cheng, N.; Bains, J.; Murari, K.']","automated behavior quantification requires accurate tracking of animals. simultaneous tracking of multiple animals, particularly those lacking visual identifiers, is particularly challenging. problems of mistaken identities and lost information on key anatomical features are common in existing methods. here we propose a markerless video-based tool to simultaneously track two socially interacting mice of the same appearance. it incorporates conventional handcrafted tracking and deep learning based techniques, which are trained on a small number of labeled images from a very basic, uncluttered experimental setup. the output consists of body masks and coordinates of the snout and tail-base for each mouse. the method was tested on a series of cross-setup videos recorded under commonly used experimental conditions including bedding in the cage and fiberoptic or headstage implants on the mice. results obtained without any human intervention showed the effectiveness of the proposed approach, evidenced by a near elimination of identities switches and a 10% improvement in tracking accuracy over a pure deep-learning-based keypoint tracking approach trained on the same data. finally, we demonstrated an application of this approach in studies of social behaviour of mice, by using it to quantify and compare interactions between pairs of mice in which some are anosmic, i.e. unable to smell. our results indicated loss of olfaction impaired typical snout-directed social recognition behaviors of mice, while non-snout-directed social behaviours were enhanced. together, these results suggest that the hybrid approach could be valuable for studying group behaviors in rodents, such as social interactions."
https://doi.org/10.1101/2023.05.31.543104,2024-01-17,accurate identification of structural variations from cancer samples,"['Li, L.; Hong, C.; Xu, J.; Chung, C. Y.-L.; Leung, A. K.-Y.; Boncan, D. A. T.; Cheng, L.; Lo, K.-W.; Lai, P. B. S.; Wong, J.; Zhou, J.; Cheng, A. S.-L.; Chan, T.-F.; Yue, F.; Yip, K. Y.']","structural variations (svs) are commonly found in cancer genomes. they can cause gene amplification, deletion, and fusion, among other functional consequences. with an average read length of hundreds of kilobases, nano-channel-based optical dna mapping is powerful in detecting large svs. however, existing sv calling methods are not tailored for cancer samples, which have special properties such as mixed cell types and sub-clones. here we propose the comsv method that is specifically designed for cancer samples. it shows high sensitivity and specificity in benchmark comparisons. applying to cancer cell lines and patient samples, comsv identifies hundreds of novel svs per sample."
https://doi.org/10.1101/2023.09.09.556969,2024-02-10,rapid and cost-effective epitope mapping using pure ribosome display coupled with next-generation sequencing and bioinformatics,"['Jia, B. X.; Ojima-Kato, T.; Kojima, T.; Nakano, H.']","a novel, efficient and cost-effective approach for epitope identification of an antibody has been developed using a ribosome display platform. this platform, known as pure ribosome display, utilizes an escherichia coli-based reconstituted cell-free protein synthesis system (pure system). it stabilizes the mrna-ribosome-peptide complex via a ribosome-arrest peptide sequence. this system was complemented by next-generation sequencing (ngs) and an algorithm for analyzing binding epitopes. to showcase the effectiveness of this method, selection conditions were refined using the anti-pa tag monoclonal antibody with the pa tag peptide as a model. subsequently, a random peptide library was constructed using 10 nnk triplet oligonucleotides via the pure ribosome display. the resulting random peptide library-ribosome-mrna complex was selected using a commercially available anti-ha (ypydvpdya) tag monoclonal antibody, followed by ngs and bioinformatic analysis. our approach successfully identified the ""dvpdy"" sequence as an epitope within the hemagglutinin amino acid sequence, which was then experimentally validated. this platform provided a valuable tool for investigating continuous epitopes in antibodies."
https://doi.org/10.1101/2023.08.24.554582,2023-12-21,beyond micrornas: analysis of chimeric reads characterises the diverse targetome of ago2-mediated regulation.,"['Hejret, V.; Varadarajan, N. M.; Klimentova, E.; Gresova, K.; Giassa, I.-C.; Vanacova, S.; Alexiou, P.']","argonaute proteins are instrumental in regulating rna stability and translation. ago2, the major mammalian argonaute protein, is known to primarily associate with micrornas, a family of small rna  driver sequences, and identifies its targets primarily via a  seed mediated partial complementarity process despite numerous studies, a definitive experimental dataset of ago2  driver-target interactions remains elusive. our study employs two experimental methods - ago2 clash and ago2 eclip, to generate thousands of ago2 target sites verified by chimeric reads. these chimeric reads contain both the ago2 loaded small rna  driver and the target sequence, providing a robust resource for modeling ago2 binding preferences. our novel analysis pipeline reveals thousands of ago2 target sites driven by micrornas and a significant number of ago2  drivers derived from fragments of other small rnas such as trnas, yrnas, snornas, rrnas, and more. we utilize convolutional neural networks to train machine learning models that accurately predict the binding potential for each  driver class and experimentally validate several interactions. in conclusion, our comprehensive analysis of the ago2 targetome broadens our understanding of its  driver repertoire and potential function in development and disease. moreover, we offer practical bioinformatic tools for future experiments and the prediction of ago2 targets. all data and code from this study are freely available at https://github.com/ml-bioinfo-ceitec/hybridetector/  contactpanagiotis.alexiou@um.edu.mt, stepanka.vanacova@ceitec.muni.cz"
https://doi.org/10.1101/2023.03.22.533800,2024-02-09,"eggnet, a generalizable geometric deep learning framework for protein complex pose scoring","['Wang, Z.; Brand, R.; Adolf-Bryfogle, J.; Grewal, J.; Qi, Y.; Combs, S. A.; Golovach, N.; Alford, R.; Rangwala, H.; Clark, P. M.']","computational prediction of molecule-protein interactions has been key for developing new molecules to interact with a target protein for therapeutics development. past work includes two independent streams of approaches: (1) predicting protein-protein interactions (ppi) between naturally occurring proteins and (2) predicting the binding affinities between proteins and small molecule ligands (aka drug target interaction, or dti). studying the two problems in isolation has limited the ability of these computational models to generalize across the ppi and dti tasks, both of which ultimately involve non-covalent interactions with a protein target. in this work, we developed an equivariant graph of graphs neural network (eggnet), a geometric deep learning framework for molecule-protein binding predictions that can handle three types of molecules for interacting with a target protein: (1) small molecules, (2) synthetic peptides and (3) natural proteins. eggnet leverages a graph of graphs (gogs) representation constructed from the molecule structures at atomic-resolution and utilizes a multi-resolution equivariant graph neural network (gnn) to learn from such representations. in addition, eggnet leverages the underlying biophysics and makes use of both atom- and residue-level interactions, which improve eggnets ability to rank candidate poses from blind docking. eggnet achieves competitive performance on both a public proteinsmall molecule binding affinity prediction task (80.2% top-1 success rate on casf-2016) and an synthetic protein interface prediction task (88.4% aupr). we envision that the proposed geometric deep learning framework can generalize to many other protein interaction prediction problems, such as binding site prediction and molecular docking, helping accelerate protein engineering and structure-based drug development."
https://doi.org/10.1101/2023.02.19.529100,2024-01-18,machine learning inference of continuous single-cell state transitions during myoblast differentiation and fusion,"['Shakarchy, A.; Zarfati, G.; Hazak, A.; Mealem, R.; Huk, K.; Avinoam, O.; Zaritsky, A.']","cells modifying their internal organization during continuous state-transitions, supporting functions from cell division to differentiation. however, tools to measure dynamic physiological states of individual transitioning cells are lacking. we combined live-cell imaging and machine learning to monitor erk1/2-inhibited primary murine skeletal muscle precursor cells, that transition rapidly and robustly from proliferating myoblasts to post-mitotic myocytes and then fuse, forming multinucleated myotubes. our model, trained using motility and actin intensity features from single cell tracking data, effectively tracked real-time continuous differentiation, revealing that differentiation occurs 7.5-14.5 hours post-induction, followed by fusion [~]3 hours later. co-inhibition of erk1/2 and p38 led to differentiation without fusion. our model inferred co-inhibition leads to terminal differentiation, indicating that p38 is specifically required for transitioning from terminal differentiation to fusion. our model also predicted that co-inhibition leads to changes in actin dynamics. mass spectrometry supported these in silico predictions and suggested novel fusion and maturation regulators downstream of differentiation. collectively, this approach can be adapted to various biological processes to uncover novel links between dynamic single-cell states and their functional outcomes."
https://doi.org/10.1101/2023.03.24.534096,2024-02-09,super-resolved fret and co-tracking in pminflux,"['Cole, F.; Zaehringer, J.; Bohlen, J.; Schroeder, T.; Steiner, F.; Stefani, F. D.; Tinnefeld, P.']","single-molecule fret (smfret) is widely used to investigate dynamic (bio) molecular interactions taking place over distances of up to 10 nm. with the advent of recent super-resolution methods such as minflux, minsted or rastmin, the spatiotemporal resolution of these techniques advanced towards the smfret regime. while these methods do not suffer from the spatial restriction of fret, they only visualize one emitter at a time, thus rendering fast dynamics of interactions out of reach. here, we describe two approaches to overcome this limitation in pminflux using its intrinsic fluorescence lifetime information. first, we combined pminflux with smfret. this enabled us to track a fret donor fluorophore and simultaneously colocalize its fret acceptor with nanometer precision. to extend co-localized tracking beyond the fret range, we developed pminflux lifetime multiplexing, a method to simultaneously track two fluorophores with similar spectral properties but distinct fluorescence lifetimes. we demonstrated its application on both static and dynamic dna origami systems with a precision better than 2 nm. within the fret range, pminflux lifetime multiplexing additionally uses a novel combined phasor-microtimegating approach. this paves the way for nanometer precise co-localized tracking for inter-dye distances between 4 nm and 100 nm, closing the resolution gap between smfret and co-tracking.  graphical abstract  o_fig o_linksmallfig width=200 height=137 src=""figdir/small/534096v1_ufig1.gif"" alt=""figure 1""> view larger version (36k): org.highwire.dtl.dtlvardef@11459forg.highwire.dtl.dtlvardef@1f1cee2org.highwire.dtl.dtlvardef@4592e6org.highwire.dtl.dtlvardef@1af3e48_hps_format_figexp  m_fig c_fig"
https://doi.org/10.1101/2022.10.13.511593,2024-01-16,kinase inhibitor pulldown assay (kip) for clinical proteomics,"['Saltzman, A. B.; Chan, D.; Holt, M. V.; Wang, J.; Jaehnig, E. J.; Anurag, M.; Singh, P.; Malovannaya, A.; Kim, B.-J.; Ellis, M. J.']","protein kinases are frequently dysregulated and/or mutated in cancer and represent essential targets for therapy. accurate quantification is essential, but current approaches are inadequate. for breast cancer treatment for example, the identification and quantification of the protein kinase erbb2 is critical for therapeutic decisions. while immunohistochemistry (ihc) is the current clinical diagnostic approach, it is only semiquantitative. mass spectrometry-based proteomics offers more quantitative assays that, unlike ihc, can be used to accurately evaluate hundreds of kinases simultaneously. the enrichment of less abundant kinase targets for quantification, along with depletion of interfering proteins, improves sensitivity and thus promotes more effective downstream analyses. multiple kinase inhibitors were therefore deployed as a capture matrix for kinase inhibitor pulldown (kip) assays designed to profile the human protein kinome as broadly as possible. optimized assays were initially evaluated in 16 patient derived xenograft models (pdx) where kip identified multiple differentially expressed and biologically relevant kinases. from these analyses, an optimized single-shot parallel reaction monitoring (prm) method was developed to improve quantitative fidelity. the prm kip approach was then reapplied to low quantities of proteins typical of protein yields from core needle biopsies of human cancers. the initial prototype targeting 100 kinases recapitulated intrinsic subtyping of pdx models obtained from comprehensive proteomic and transcriptomic profiling. luminal and her2 enriched oct-frozen patient biopsies subsequently analyzed through kip-prm also clustered by subtype. finally, stable isotope labeled peptide standards were developed to define a prototype clinical method."
https://doi.org/10.1101/2022.09.01.504602,2024-01-15,a principal odor map unifies diverse tasks in human olfactory perception,"['Lee, B. K.; Mayhew, E. E.; Sanchez-Lengeling, B.; Wei, J. N.; Qian, W. W.; Little, K.; Andres, M.; Nguyen, B. B.; Moloy, T.; Parker, J. K.; Gerkin, R. C.; Mainland, J. D.; Wiltschko, A. B.']","mapping molecular structure to odor perception is a key challenge in olfaction. here, we use graph neural networks (gnn) to generate a principal odor map (pom) that preserves perceptual relationships and enables odor quality prediction for novel odorants. the model is as reliable as a human in describing odor quality: on a prospective validation set of 400 novel odorants, the model-generated odor profile more closely matched the trained panel mean (n=15) than did the median panelist. applying simple, interpretable, theoretically-rooted transformations, the pom outperformed chemoinformatic models on several other odor prediction tasks, indicating that the pom successfully encoded a generalized map of structure-odor relationships. this approach broadly enables odor prediction and paves the way toward digitizing odors.  one-sentence summaryan odor map achieves human-level odor description performance and generalizes to diverse odor-prediction tasks."
https://doi.org/10.1101/2021.04.27.441658,2024-01-12,deepregfinder: deep learning-based regulatory elements finder,"['Ramakrishnan, A.; Wangensteen, G.; Kim, S.; Nestler, E. J.; Shen, L.']","motivationenhancers and promoters are important classes of dna regulatory elements that control gene expression. identifying them at the genomic scale is a critical and challenging task in bioinformatics. the most successful method so far is to train machine learning models on known enhancer and promoter sites and predict them at other genomic regions using chip-seq and related data.  resultswe have developed a highly customizable program called deepregfinder which automates data processing, model training and genome-wide prediction of enhancers and promoters using convolutional and recurrent neural networks. our program further classifies the enhancers and promoters into active and poised states to facilitate downstream analysis. based on mean average precision scores of different classes across multiple cell types, our method significantly outperforms the existing algorithms.  availabilityhttps://github.com/shenlab-sinai/deepregfinder"
https://doi.org/10.1101/2022.03.06.483195,2024-02-08,visse: a versatile tool to identify and visualise higher-order molecular phenotypes from functional enrichment analysis,"['Bhuva, D. D.; Tan, C. W.; Liu, N.; Whitfield, H. J.; Papachristos, N.; Lee, S.; Kharbanda, M.; Mohamed, A.; Davis, M. J.']","functional analysis of high throughput experiments using pathway analysis is now ubiquitous. though powerful, these methods often produce thousands of redundant results owing to knowledgebase redundancies upstream. this scale of results hinders extensive exploration by biologists and often leads to investigator biases due to previous knowledge and expectations. to address this issue, we present visse, a flexible network-based analysis method that summarises redundancies into biological themes and provides various analytical modules to characterise and visualise them with respect to the underlying data, thus providing a comprehensive view of the biological system. we demonstrate visses versatility by applying it to three different technologies: bulk, single-cell and spatial transcriptomics. applying visse to a factor analysis of a breast cancer spatial transcriptomic data, we identified stromal phenotypes that support tumour dissemination. its adaptability allows visse to enhance all existing gene-set enrichment and pathway analysis workflows, removing investigator bias from molecular discovery.    o_fig o_linksmallfig width=200 height=175 src=""figdir/small/483195v1_ufig1.gif"" alt=""figure 1""> view larger version (64k): org.highwire.dtl.dtlvardef@1c384b0org.highwire.dtl.dtlvardef@13c6bd3org.highwire.dtl.dtlvardef@1cb1d91org.highwire.dtl.dtlvardef@24cbf_hps_format_figexp  m_fig c_fig"
https://doi.org/10.1101/2023.11.16.567487,2024-02-08,the physical logic of protein machines,"['McBride, J. M.; Tlusty, T.']","proteins are intricate molecular machines whose complexity arises from the heterogeneity of the amino acid building blocks and their dynamic network of many-body interactions. these nanomachines gain function when put in the context of a whole organism through interaction with other inhabitants of the biological realm. and this functionality shapes their evolutionary histories through intertwined paths of selection and adaptation. recent advances in machine learning have solved the decades-old problem of how protein sequence determines their structure. however, the ultimate question regarding the basic logic of protein machines remains open: how does the collective physics of proteins lead to their functionality? and how does a sequence encode the full range of dynamics and chemical interactions that facilitate function? here, we explore these questions within a physical approach that treats proteins as mechano-chemical machines, which are adapted to function via concerted evolution of structure, motion, and chemical interactions."
https://doi.org/10.1101/2023.09.20.558674,2024-02-08,the rna-binding domain of hnrnp u extends beyond the rgg/rg motifs,"['Kletzein, O. A.; Wuttke, D. S.; Batey, R. T.']","heterogeneous nuclear ribonucleoprotein u (hnrnp u) is a ubiquitously expressed protein that regulates chromatin architecture through its interactions with numerous dna, protein, and rna partners. the rna-binding domain (rbd) of hnrnp u was previously mapped to an rgg/rg element within its disordered c-terminal region, but little is understood about its binding mode and potential for selective rna recognition. analysis of publicly available hnrnp u enhanced uv crosslinking and immunoprecipitation (eclip) data identified high-confidence binding sites within human rnas. we synthesized a set of diverse rnas encompassing eleven of these identified crosslink sites for biochemical characterization using a combination of fluorescence anisotropy and electrophoretic mobility shift assays. these in vitro binding experiments with a rationally designed set of rnas and hnrnp u domains revealed that the rgg/rg element is a small part of a more expansive rbd that encompasses most of the disordered c-terminal region. this rbd contains a second, previously experimentally uncharacterized rgg/rg element with rna-binding properties comparable to the canonical rgg/rg element. these rgg/rg elements serve redundant functions, with neither serving as the primary rbd. while in isolation each rgg/rg element has modest affinity for rna, together they significantly enhance the association of hnrnp u with rna, enabling binding of most of the designed rna set with low to mid-nanomolar binding affinities. identification and characterization of the complete hnrnp u rbd highlights the perils of a reductionist approach to defining biochemical activities in this system and paves the way for a detailed investigation of its rna-binding specificity."
https://doi.org/10.1101/2023.07.25.550443,2024-02-08,antibp3: a hybrid method for predicting antibacterial peptides against gram-positive/negative/variable bacteria,"['Bajiya, N.; Choudhury, S.; Dhall, A.; Raghava, G. P. S.']","this study focuses on the development of in silico models for predicting antibacterial peptides as a potential solution for combating antibiotic-resistant strains of bacteria. existing methods for predicting antibacterial peptides are mostly designed to target either gram-positive or gram-negative bacteria. in this study, we introduce a novel approach that enables the prediction of antibacterial peptides against several bacterial groups, including gram-positive, gram-negative, and gram-variable bacteria. firstly, we developed an alignment-based approach using blast to identify antibacterial peptides and achieved poor sensitivity. secondly, we employed a motif-based approach to predict antibacterial peptides and obtained high precision with low sensitivity. to address the similarity issue, we developed machine learning-based models using a variety of compositional and binary features. our machine learning-based model developed using the amino acid binary profile of terminal residues achieved maximum auc 0.93, 0.98 and 0.94 for gram-positive, gram-negative, and gram-variable bacteria, respectively, when evaluated on a validation/independent dataset. our attempts to develop hybrid or ensemble methods by merging machine learning models with similarity and motif-based techniques did not yield any improvements. to ensure robust evaluation, we employed standard techniques such as five-fold cross-validation, internal validation, and external validation. our method performs better than existing methods when we compare our method with existing approaches on an independent dataset. in summary, this study makes significant contributions to the field of antibacterial peptide prediction by providing a comprehensive set of methods tailored to different bacterial groups. as part of our contribution, we have developed the antibp3 web server and standalone package, which will assist researchers in the discovery of novel antibacterial peptides for combating bacterial infections (https://webs.iiitd.edu.in/raghava/antibp3/).  key points blast-based similarity for annotating antibacterial peptides.  machine learning-based models developed using composition and binary profiles.  identification and mapping of motifs exclusively found in antibacterial peptides  improved version of antibp and antibp2 for predicting antibacterial peptides.  web server for predicting/designing/scanning antibacterial peptides for all groups of bacteria   authors biographyo_linisha bajiya is currently working as ph.d. in computational biology from department of computational biology, indraprastha institute of information technology, new delhi, india. c_lio_lishubham choudhury is currently working as ph.d. in computational biology from department of computational biology, indraprastha institute of information technology, new delhi, india. c_lio_lianjali dhall is currently working as ph.d. in computational biology from department of computational biology, indraprastha institute of information technology, new delhi, india. c_lio_ligajendra p. s. raghava is currently working as professor and head of department of computational biology, indraprastha institute of information technology, new delhi, india. c_li"
https://doi.org/10.1101/2023.12.21.572760,2024-02-08,an improved workflow for the quantification of orthohantavirus infection using automated imaging and flow cytometry,"['Menke, L.; Sieben, C.']","determination of the infectious titer is a central requirement when working with pathogenic viruses. the plaque or focus assay is commonly used but a labor- and time-consuming approach to determine the infectious titer of orthohantavirus samples. we have developed an optimized virus quantification approach that relies on the fluorescence-based detection of the orthohantavirus nucleocapsid protein (n) in infected cells with high sensitivity. we present the use of flow cytometry but highlight fluorescence microscopy in combination with automated data analysis as an attractive alternative to increase the information retrieved from an infection experiment. additionally, we offer an open-source software equipped with a user-friendly graphical interface, eliminating the necessity for advanced programming skills."
https://doi.org/10.1101/2023.11.21.567874,2024-02-08,fluorotensor: identification and tracking of colocalised molecules and their stoichiometries in multi-colour single molecule imaging via deep learning.,"['Wills, M. F. K.; Bueno Alejo, C.; Hundt, N.; Hudson, A. J.; Eperon, I. C.']","the identification of photobleaching steps in single molecule fluorescence imaging is a well-established procedure for analysing the stoichiometries of molecular complexes. nonetheless, the method is challenging with protein fluorophores because of the high levels of noise, rapid bleaching and very variable signal intensities, all of which complicate methods based on statistical analyses of intensities to identify bleaching steps. it has recently been shown that deep learning by convolutional neural networks can yield an accurate analysis with a relatively short computational time. we describe here an improved use of such an approach that detects bleaching events even in the first time point of observation, and we have included this within an integrated software package incorporating fluorescence spot detection, colocalisation, tracking, fret and photobleaching step analyses of single molecules or complexes. this package, known as fluorotensor, is written in python with a self-explanatory user interface."
https://doi.org/10.1101/2023.10.09.561613,2024-01-10,msisensor-rna: microsatellite instability detection for bulk and single-cell gene expression data,"['Jia, P.; Yang, X.; Yang, X.; Wang, T.; Xu, Y.; Ye, K.']","microsatellite instability (msi) is an indispensable biomarker in cancer immunotherapy. currently, msi scoring methods by high-throughput omics methods have gained popularity and demonstrated better performance than the gold standard method for msi detection. however, the msi detection method on expression data, especially single-cell expression data, is still lacking, limiting the scope of clinical application and prohibiting the investigation of msi at a single-cell level. herein, we developed msisensor-rna, an accurate, robust, adaptable, and standalone software to detect msi status based on expression values of msi-associated genes. we demonstrated the favorable performance and promise of msisensor-rna in both bulk and single-cell gene expression data in multiplatform technologies including rna-seq, microarray, and single-cell rna-seq. msisensor-rna is a versatile, efficient, and robust method for msi status detection from both bulk and single-cell gene expression data in clinical studies and applications. msisensor-rna is available at https://github.com/xjtu-omics/msisensor-rna."
https://doi.org/10.1101/2023.08.22.554137,2024-01-20,sccancer2: data-driven in-depth annotations of the tumor microenvironment at single-level resolution,"['Chen, Z.; Miao, Y.; Tan, Z.; Hu, Q.; Wu, Y.; Guo, W.; Gu, J.']","single-cell rna-seq (scrna-seq) is a powerful technique for decoding the complex cellular compositions in the tumor microenvironment (tme). as previous studies have defined many meaningful cell subtypes in several tumor types, there is a great need to computationally transfer these labels to new datasets. also, different studies used different approaches or criteria to define the cell subtypes for the same major cell lineages. the relationships between the cell subtypes defined in different studies should be carefully evaluated. in this updated package sccancer2, designed for integrative tumor scrna-seq data analysis, we developed a supervised machine learning framework to annotate tme cells with annotated cell subtypes from 15 scrna datasets with 594 samples in total. based on the trained classifiers, we quantitatively constructed the similarity maps between the cell subtypes defined in different references by testing on all the 15 datasets. secondly, to improve the identification of malignant cells, we designed a classifier by integrating large-scale pan-cancer tcga bulk gene expression datasets and scrna-seq datasets (10 cancer types, 159 samples, 652,160 cells). this classifier shows robust performances when no internal confidential reference cells are available. thirdly, this package also integrated a module to process the seq-based spatial transcriptomic data and analyze the spatial features of tme. software availability: http://lifeome.net/software/sccancer2/."
https://doi.org/10.1101/2023.03.09.531928,2024-01-17,concurrent prediction of rna secondary structures with pseudoknots and local 3d motifs in an integer programming framework,"['Loyer, G.; Reinharz, V.']","motivationthe prediction of rna structure canonical base pairs from a single sequence, especially pseudoknotted ones, remains challenging in a thermodynamic models that approximates the energy of the local 3d motifs joining canonical stems. it has become more and more apparent in recent years that the structural motifs in the loops, composed of non-canonical interactions, are essential for the final shape of the molecule enabling its multiple functions. our capacity to predict accurate 3d structures is also limited when it comes to the organization of the large intricate network of interactions that form inside those loops.  resultswe previously developed the integer programming framework rnamoip (rna motifs over integer programming) to reconcile rna secondary structure and local 3d motif information available in databases. we further develop our model to now simultaneously predict the canonical base pairs (with pseudoknots) from base pair probability matrices with or without alignment. we benchmarked our new method over the all non-redundant rnas below 150 nucleotides. we show that the joined prediction of canonical base pairs structure and local conserved motifs (i) improves the ratio of well-predicted interactions in the secondary structure, (ii) predicts well canonical and wobble pairs at the location where motifs are inserted, (iii) is greatly improved with evolutionary information and (iv) non-canonical motifs at kink-turn locations.  availabilitythe source code of the framework is available at https://gitlab.info.uqam.ca/cbe/rnamoip and an interactive web server at https://rnamoip.cbe.uqam.ca/"
https://doi.org/10.1101/2023.07.25.550587,2024-01-23,cloudrnaspades: isoform assembly using bulk barcoded rna sequencing data,"['Meleshko, D.; Prjbelski, A. D.; Rayko, M.; Tomescu, A. I.; Tilgner, H.; Hajirasouliha, I.']","motivationrecent advancements in long-read rna sequencing have enabled the examination of full-length isoforms, previously uncaptured by short-read sequencing methods. an alternative powerful method for studying isoforms is through the use of barcoded short-read rna reads, for which a barcode indicates whether two short-reads arise from the same molecule or not. such techniques included the 10x genomics linked-read based sparse isoform sequencing (spiso-seq), as well as loop-seq, or tell-seq. some applications, such as novel-isoform discovery, require very high coverage. obtaining high coverage using long reads can be difficult, making barcoded rna-seq data a valuable alternative for this task. however, most annotation pipelines are not able to work with a set of short reads instead of a single transcript, also not able to work with coverage gaps within a molecule if any. in order to overcome this challenge, we present an rna-seq assembler allowing the determination of the expressed isoform per barcode.  resultsin this paper, we present cloudrnaspades, a tool for assembling full-length isoforms from barcoded rna-seq linked-read data in a reference-free fashion. evaluating it on simulated and real human data, we found that cloudrnaspades accurately assembles isoforms, even for genes with high isoform diversity.  availabilitycloudrnaspades is a feature release of a spades assembler and available at https://cab.spbu.ru/software/cloudrnaspades/.  contactdmm2017@med.cornell.edu"
https://doi.org/10.1101/2023.08.08.552417,2024-02-07,performing highly parallelized and reproducible gwas analysis on biobank-scale data,"['Schoenherr, S.; Schachtl-Riess, J.; Di Maio, S.; Filosi, M.; Mark, M.; Lamina, C.; Fuchsberger, C.; Kronenberg, F.; Forer, L.']","motivationgenome-wide association studies (gwas) in large biobanks are transforming genetic research and enable the detection of novel genotype-phenotype relationships. in the last two decades, over 60,000 genetic associations across thousands of human diseases and traits have been discovered using a gwas approach. due to denser genotyping and increasing sample sizes, researchers are increasingly faced with computational challenges when executing gwas analysis. a reproducible, modular and extensible pipeline with a focus on parallelization is essential to simplify data analysis and to allow researchers to devote their time to other essential tasks such as result interpretation and downstream analysis.  resultshere we present nf-gwas, a nextflow pipeline to run biobank-scale gwas analysis. the pipeline automatically performs numerous pre- and post-processing steps, integrates regression modeling from the regenie package and currently supports single-variant, gene-based and interaction testing. nf-gwas also includes an extensive reporting functionality that allows to inspect thousands of phenotypes and navigate interactive manhattan plots directly in the web browser. the pipeline is extensively tested using the unit-style testing framework nf-test to ensure code maintainability, a crucial requirement in clinical and pharmaceutical settings. furthermore, we validated the pipeline against published gwas datasets and benchmarked the pipeline on high-performance computing and cloud infrastructures to provide cost estimations to end users.  availabilitynf-gwas is free available at https://github.com/genepi/nf-gwas.  contactlukas.forer@i-med.ac.at"
https://doi.org/10.1101/2023.03.27.534188,2024-01-12,massively parallel profiling of rna-targeting crispr-cas13d,"['Kuo, H.-C.; Prupes, J.; Chou, C.-W.; Finkelstein, I. J.']","type vi crispr enzymes cleave target rnas and are widely used for gene regulation, rna tracking, and diagnostics. however, a systematic understanding of their rna binding specificity and cleavage activation is lacking. here, we describe rna chip-hybridized association-mapping platform (rna-champ), a massively parallel platform that repurposes next-generation dna sequencing chips to measure the binding affinity for over 10,000 rna targets containing structural perturbations, mismatches, insertions, and deletions relative to the crispr rna (crrna). deep profiling of cas13d, a compact and widely used rna nuclease, reveals that it does not require a protospacer flanking sequence (pfs) but is exquisitely sensitive to secondary structure within the target rna. cas13d binding is strongly penalized by mismatches, insertions, and deletions in the distal crrna-target rna regions, while alterations in the proximal region inhibit nuclease activity without affecting binding. a biophysical model built from these data reveals that target recognition begins at the distal end of unstructured target rnas and proceeds to the proximal end. using this model, we designed a series of partially mismatched guide rnas that modulate nuclease activity to detect single nucleotide polymorphisms (snps) in circulating sars-cov-2 variants. this work describes the key determinants of rna targeting by a type vi crispr enzyme to improve crispr diagnostics and in vivo rna editing. more broadly, rna-champ provides a quantitative platform for systematically measuring protein-rna interactions."
https://doi.org/10.1101/2023.01.20.524948,2024-01-12,"incorporating metabolic activity, taxonomy and community structure to improve microbiome-based predictive models for host phenotype prediction","['Monshizadeh, M.; Ye, Y.']","we developed microkpnn, a prior-knowledge guided interpretable neural network for microbiomebased human host phenotype prediction. the prior-knowledge used in microkpnn includes the metabolic activities of different bacterial species, phylogenetic relationships, and bacterial community structure. application of microkpnn to seven gut microbiome datasets (involving five different human diseases including inflammatory bowel disease, type 2 diabetes, liver cirrhosis, colorectal cancer, and obesity) shows that incorporation of the prior knowledge helped improve the microbiome-based host phenotype prediction. microkpnn outperformed fully-connected neural network based approaches in all seven cases, with the most improvement of accuracy in the prediction of type 2 diabetes. microkpnn outperformed a recently developed deep-learning based approach deepmicro, which selects the best combination of autoencoder and machine learning approach to make predictions, in six out of the seven cases. more importantly, we showed that microkpnn provides a way for interpretation of the predictive models. our results suggested that the metabolic potential of the bacterial species contributed more than the two other sources of prior knowledge. microkpnn is publicly available at https://github.com/mgtools/microkpnn."
https://doi.org/10.1101/2021.09.14.460353,2024-01-12,quantitative measurement of antibiotic resistance in mycobacterium tuberculosis reveals genetic determinants of resistance and susceptibility in a target gene approach,"['CRyPTIC Consortium,  ; Carter, J. J.']","the world health organization goal of universal drug susceptibility testing for patients with tuberculosis is most likely to be achieved through molecular diagnostics; however, to date these have focused largely on first-line drugs, and always on predicting binary susceptibilities. here, we used whole genome sequencing and a quantitative microtiter plate assay to relate genomic mutations to minimum inhibitory concentration in 15,211 mycobacterium tuberculosis patient isolates from 27 countries across five continents.  this work identifies 449 unique mic-elevating genetic determinants across thirteen drugs, as well as 91 mutations resulting in hypersensitivity for eleven drugs. our results provide a guide for further implementation of personalized medicine for the treatment of tuberculosis using genetics-based diagnostics and can serve as a training set for novel approaches to predict drug resistance."
https://doi.org/10.1101/2022.06.09.495423,2024-02-07,"genomic prediction using machine learning: a comparison of the performance of regularized regression, ensemble, instance-based and deep learning methods on synthetic and empirical data","['Lourenco, V. M.; Ogutu, J. O.; Rodrigues, R. P.; Piepho, H.-P.']","the accurate prediction of genomic breeding values is central to genomic selection in both plant and animal breeding studies. genomic prediction involves the use of thousands of molecular markers spanning the entire genome and therefore requires methods able to efficiently handle high dimensional data. not surprisingly, machine learning methods are becoming widely advocated for and used in genomic prediction studies. these methods encompass different groups of supervised and unsupervised learning methods. although several studies have compared the predictive performances of individual methods, studies comparing the predictive performance of different groups of methods are rare. however, such studies are crucial for identifying (i) groups of methods with superior genomic predictive performance and assessing (ii) the merits and demerits of such groups of methods relative to each other and to the established classical methods. here, we comparatively evaluate the genomic predictive performance and computational cost of several groups of supervised machine learning methods, specifically, regularized regression methods, deep, ensemble and instance-based learning algorithms, using one simulated animal breeding dataset and three empirical maize breeding datasets obtained from a commercial breeding program. our results show that the relative predictive performance and computational expense of the groups of machine learning methods depend upon both the data and target traits and that for classical regularized methods, increasing model complexity can incur huge computational costs but does not necessarily always improve predictive accuracy. thus, despite their greater complexity and computational burden, neither the adaptive nor the group regularized methods clearly improved upon the results of their simple regularized counterparts. this rules out selection of one procedure among machine learning methods for routine use in genomic prediction. the results also show that, because of their competitive predictive performance, computational efficiency, simplicity and therefore relatively few tuning parameters, the classical linear mixed model and regularized regression methods are likely to remain strong contenders for genomic prediction. the dependence of predictive performance and computational burden on target datasets and traits call for increasing investments in enhancing the computational efficiency of machine learning algorithms and computing resources.  author summarymachine learning methods are well suited for efficiently handling high dimensional data. particularly, supervised machine learning methods have been successfully used in genomic prediction or genome-enabled selection. however, their comparative predictive accuracy is still poorly understood, yet this is a critical issue in plant and animal breeding studies given that increasing methodological complexity can substantially increase computational complexity or cost. here, we show that predictive performance is both data and target trait dependent thus ruling out selection of one method for routine use in genomic prediction. we also show that for this reason, relatively low computational complexity and competitive predictive performance, the classical linear mixed model approach and regularized regression methods remain strong contenders for genomic prediction."
https://doi.org/10.1101/2023.11.09.566411,2024-01-12,prokbert family: genomic language models for microbiome applications,"['Ligeti, B.; Szepesi-Nagy, I.; Bodnar, B.; Ligeti-Nagy, N.; Juhasz, J.']","machine learning offers transformative capabilities in microbiology and microbiome analysis, deciphering intricate microbial interactions, predicting functionalities, and unveiling novel patterns in vast datasets. this enriches our comprehension of microbial ecosystems and their influence on health and disease. however, the integration of machine learning in these fields contends with issues like the scarcity of labeled datasets, the immense volume and complexity of microbial data, and the subtle interactions within microbial communities. addressing these challenges, we introduce the prokbert model family. built on transfer learning and self-supervised methodologies, prokbert models capitalize on the abundant available data, demonstrating adaptability across diverse scenarios. the models learned representations align with established biological understanding, shedding light on phylogenetic relationships. with the novel local context-aware (lca) tokenization, the prokbert family overcomes the context size limitations of traditional transformer models without sacrificing performance or the information rich local context. in bioinformatics tasks like promoter prediction and phage identification, prokbert models excel. for promoter predictions, the best performing model achieved an mcc of 0.74 for e. coli and 0.62 in mixed-species contexts. in phage identification, they all consistently outperformed tools like virsorter2 and deepvirfinder, registering an mcc of 0.85. compact yet powerful, the prokbert models are efficient, generalizable, and swift. they cater to both supervised and unsupervised tasks, providing an accessible tool for the community. the models are available on github and huggingface."
https://doi.org/10.1101/2023.06.21.545898,2024-01-12,a model-based hypothesis framework to define and estimate the diel niche via the 'diel.niche' r package,"['Gerber, B. D.; Devarajan, K.; Farris, Z. J.; Fidino, M.']","o_lihow animals use the diel period (24-hour light-dark cycle) is of fundamental importance to understand their niche. while ecological and evolutionary literature abound with discussion of diel phenotypes (e.g., diurnal, nocturnal, crepuscular, cathemeral), they lack clear and explicit quantitative definitions. as such, inference can be confounded when evaluating hypotheses of animal diel niche switching or plasticity across studies because researchers may be operating under different definitions of diel phenotypes. c_lio_liwe propose quantitative definitions of diel phenotypes using four alternative hypotheses sets (maximizing, traditional, general, and selection) aimed at achieving different objectives. each hypothesis set is composed of mutually exclusive hypotheses defined based on the activity probabilities in the three fundamental periods of light availability (twilight, daytime, and nighttime). c_lio_liwe develop a bayesian modeling framework that compares diel phenotype hypotheses using bayes factors and estimates model parameters using a multinomial model with linear inequality constraints. model comparison, parameter estimation, and visualizing results can be done in the diel.niche r package. a simplified r shiny web application is also available. c_lio_liwe provide extensive simulation results to guide researchers on the power to discriminate among hypotheses for a range of sample sizes (10 to 1280). we also work through several examples of using data to make inferences on diel activity, and include online vignettes on how to use the diel.niche package. we demonstrate how our modeling framework complements analyses that are commonly used to investigate diel activity, such as circular kernel density estimators. c_lio_liour aim is to encourage standardization of the language of diel activity and bridge conceptual frameworks and hypotheses in diel research with data and models. lastly, we hope more research focuses on the ecological and conservation importance of understanding how animals use diel time. c_li"
https://doi.org/10.1101/2022.11.04.515241,2024-01-11,a graph clustering algorithm for detection and genotyping of structural variants from long reads,"['Gaitan, N.; Duitama, J.']","structural variants (sv) are polymorphisms defined by their length (>50 bp). the usual types of svs are deletions, insertions, translocations, inversions, and copy number variants. sv detection and genotyping is fundamental given the role of svs in phenomena such as phenotypic variation and evolutionary events. thus, methods to identify svs using long read sequencing data have been recently developed. we present an accurate and efficient algorithm to predict svs from long-read sequencing data. the algorithm starts collecting evidence (signatures) of svs from read alignments. then, signatures are clustered based on a euclidean graph with coordinates calculated from lengths and genomic positions. clustering is performed by the dbscan algorithm, which provides the advantage of delimiting clusters with high resolution. clusters are transformed into svs and a bayesian model allows to precisely genotype svs based on their supporting evidence. this algorithm is integrated in the single sample variants detector of the next generation sequencing experience platform (ngsep), which facilitates the integration with other functionalities for genomics analysis. for benchmarking, our algorithm is compared against different tools using visor for simulation and the giab sv dataset for real data. for indel calls in a 20x depth nanopore simulated dataset, the dbscan algorithm performed better, achieving an f-score of 98%, compared to 97.8 for dysgu, 97.8 for svim, 97.7 for cutesv, and 96.8 for sniffles. we believe that this work makes a significant contribution to the development of bioinformatic strategies to maximize the use of long read sequencing technologies."
https://doi.org/10.1101/2023.07.04.546825,2024-01-11,machine learning made easy (mlme): a comprehensive toolkit for machine learning-driven data analysis,"['Akshay, A.; Katoch, M.; Shekarchizadeh, N.; Abedi, M.; Sharma, A.; C. Burkhard, F.; M. Adam, R.; Monastyrskaia, K.; Hashemi Gheinani, A.']","backgroundmachine learning (ml) has emerged as a vital asset for researchers to analyze and extract valuable information from complex datasets. however, developing an effective and robust ml pipeline can present a real challenge, demanding considerable time and effort, thereby impeding research progress. existing tools in this landscape require a profound understanding of ml principles and programming skills. furthermore, users are required to engage in the comprehensive configuration of their ml pipeline to obtain optimal performance.  resultsto address these challenges, we have developed a novel tool called machine learning made easy (mlme) that streamlines the use of ml in research, specifically focusing on classification problems at present. by integrating four essential functionalities, namely data exploration, automl, customml, and visualization, mlme fulfills the diverse requirements of researchers while eliminating the need for extensive coding efforts. to demonstrate the applicability of mlme, we conducted rigorous testing on six distinct datasets, each presenting unique characteristics and challenges. our results consistently showed promising performance across different datasets, reaffirming the versatility and effectiveness of the tool. additionally, by utilizing mlmes feature selection functionality, we successfully identified significant markers for cd8+ naive (bach2), cd16+ (cd16), and cd14+ (vcan) cell populations.  conclusionmlme serves as a valuable resource for leveraging machine learning (ml) to facilitate insightful data analysis and enhance research outcomes, while alleviating concerns related to complex coding scripts. the source code and a detailed tutorial for mlme are available at https://github.com/functionalurology/mlme.  key pointso_limlme is a novel tool that simplifies machine learning (ml) for researchers by integrating data exploration, automl, customml, and visualization functionalities. c_lio_limlme improves efficiency and productivity by streamlining the ml workflow and eliminating the need for extensive coding efforts. c_lio_lirigorous testing on diverse datasets demonstrates mlmes promising performance in classification problems. c_lio_limlme provides intuitive interfaces for data exploration, automated ml, customizable ml pipelines, and result visualization. c_lio_lifuture developments aim to expand mlmes capabilities to include support for unsupervised learning, regression, hyperparameter tuning, and integration of user-defined algorithms. c_li"
https://doi.org/10.1101/2023.09.17.557904,2024-01-11,preclinical exploration of the dna damage response pathway using the interactive neuroblastoma cell line explorer clean.,"['Gabre, J.; Merseburger, P.; Claeys, A.; Siaw, J.; Bekaert, S.-L.; Speleman, F.; Hallberg, B.; Palmer, R.; Van den Eynden, J.']","neuroblastoma (nb) is the most common cancer in infancy with an urgent need for more efficient targeted therapies. the development of novel (combinatorial) treatment strategies relies on extensive explorations of signaling perturbations in neuroblastoma cell lines, using rna-seq or other high throughput technologies (e.g., phosphoproteomics). this typically requires dedicated bioinformatics support, which is not always available. additionally, while data from published studies are highly valuable and raw data (e.g., fastq files) are nowadays released in public repositories, data processing is time-consuming and again difficult without bioinformatics support. to facilitate nb research, more user-friendly and immediately accessible platforms are needed to explore newly generated as well as existing high throughput data. to make this possible, we developed an interactive data centralization and visualization web application, called clean (the cell line explorer web application of neuroblastoma data; https://ccgg.ugent.be/shiny/clean/). by focusing on the regulation of the dna damage response, a therapeutic target of major interest in neuroblastoma, we demonstrate how clean can be used to gain novel mechanistic insights and identify putative drug targets in neuroblastoma."
https://doi.org/10.1101/2023.05.30.542829,2024-01-11,miniaturized car knocked onto cd3epsilon extends tcr function with car specificity under control of endogenous tcr signaling cascade,"['Manske, K.; Dressler, L.; Fraessle, S. P.; Effenberger, M.; Tschulik, C.; Cletiu, V.; Benke, E.; Wagner, M.; Schober, K.; Mueller, T. R.; Busch, D. H.; Stemberger, C.; Germeroth, L.; Poltorak, M. P.']","immunotherapy using tcr and especially car transgenic t cells is a rapidly advancing field with the potential to become standard of care for the treatment of multiple diseases. while all current fda approved car t cell products are generated using lentiviral gene transfer, extensive work is put into crispr/cas mediated gene delivery to develop the next generation of safer and more potent cell products. one limitation of all editing systems is the size restriction of the knock-in cargo. targeted integration under control of an endogenous promotor and/or signaling cascades opens the possibility to reduce car gene size to absolute minimum. here we demonstrate that a first-generation car payload can be reduced to its minimum component - the antigen-binding domain - by targeted integration under control of the cd3{varepsilon} promoter generating a car-cd3{varepsilon} fusion protein that exploits the endogenous tcr signaling cascade. miniaturizing car payload in this way results in potent car activity while simultaneously retaining the primary antigen recognition function of the tcr. introducing car-specificity using a car binder only while maintaining endogenous tcr function may be an appealing design for future autologous car t cell therapies."
https://doi.org/10.1101/2023.08.18.553902,2024-01-13,"differences in oligomerization of the sars-cov-2 envelope protein, poliovirus vp4, and hiv vpu","['Townsend, J. A.; Fapohunda, O.; Wang, Z.; Pham, H.; Taylor, M. T.; Kloss, B.; Park, S. H.; Opella, S. J.; Aspinwall, C. A.; Marty, M. T.']","viroporins constitute a class of viral membrane proteins with diverse roles in the viral life cycle. they can self-assemble and form pores within the bilayer that transport substrates, such as ions and genetic material, that are critical to the viral infection cycle. however, there is little known about the oligomeric state of most viroporins. here, we use native mass spectrometry (ms) in detergent micelles to uncover the patterns of oligomerization of the full-length sars-cov-2 envelope (e) protein, poliovirus vp4, and hiv vpu. our data suggest that the e protein is a specific dimer, vp4 is exclusively monomeric, and vpu assembles into a polydisperse mixture of oligomers under these conditions. overall, these results revealed the diversity in the oligomerization of viroporins, which has implications for mechanisms of their biological functions as well as their potential as therapeutic targets."
https://doi.org/10.1101/2023.07.25.550477,2024-02-06,the impact of incidental anxiety on the neural signature of mentalizing,"['Chang, L.-A.; Engelmann, J. B.']","while the effects of anxiety on various cognitive processes, including memory, attention, and learning, have been relatively well documented, the neurobiological effects of anxiety on social cognitive processes remain largely unknown. we address this gap using threat-of-shock to induce incidental anxiety while participants performed two false-belief tasks, a standard and an economic-games version. during belief formation and belief inferences, regions in a canonical social cognition network showed activation reflecting mentalizing, including the temporoparietal junction (tpj), precuneus, and dorsomedial prefrontal cortex (dmpfc). at the same time, we found threat-related suppression of social cognition regions during belief inferences. a conjunction analysis confirmed that a network of regions was simultaneously engaged during mentalizing and suppressed by anxiety: bilateral tpj, bilateral ifg, and putamen. we examined how threat impacted the connectivity between seed regions from the conjunction analyses and its targets. during belief formation, we found that anxiety suppressed the connectivity between the precuneus seed and two key mentalizing nodes, the dmpfc and right tpj. moreover, during belief inferences threat specificallty suppressed belief-based connectivity between putamen and its targets in ips and dlpfc, and dispositional distress significantly modulated threat-related suppression of connectivity between the left tpj seed and left ips. our results highlight important effects of incidental and dispositional anxiety on specific nodes of the social cognition network. taken together, our study uncovers novel interactions between the reward, social cognition, and attentional systems, indicating that social cognitive processes rely on support from other large-scale networks, and that these network interactions are disrupted under incidental anxiety."
https://doi.org/10.1101/2023.05.02.539152,2024-01-10,joint representation of molecular networks from multiple species improves gene classification,"['Mancuso, C. A.; Johnson, K. A.; Liu, R.; Krishnan, A.']","network-based machine learning (ml) has the potential for predicting novel genes associated with nearly any health and disease context. however, this approach often uses network information from only the single species under consideration even though networks for most species are noisy and incomplete. while some recent methods have begun addressing this shortcoming by using networks from more than one species, they lack one or more key desirable properties: handling networks from multiple species, incorporating many-to-many orthology information, or generating a network representation that is reusable across different types of and newly-defined prediction tasks. here, we present geneplexuszoo, a framework that casts molecular networks from multiple species into a single reusable feature space for network-based ml. we demonstrate that this multi-species network representation improves both gene classification within a single species and knowledge-transfer across species, even in cases where the inter-species correspondence is undetectable based on shared orthologous genes. thus, geneplexuszoo enables effectively leveraging the high evolutionary molecular, functional, and phenotypic conservation across species to discover novel genes associated with diverse biological contexts."
https://doi.org/10.1101/2023.06.20.545834,2024-02-05,a 128-ch area-efficient neurochemical-sensing front-end for fscv recordings of dopamine,"['White, K. A.; Darroudi, M.; Park, J.; Kim, B. N.']","neurochemical recordings rely on electrochemical reactions of electroactive neurotransmitters such as dopamine, serotonin, and norepinephrine. this electrochemical technique allows for highly sensitive monitoring of neurotransmitters in the brain. traditionally, single-channel carbon-fiber microelectrode recordings have been considered the gold standard method. however, an alternative approach involves the use of a microelectrode array, which enables high spatiotemporal resolution imaging of electroactive neurotransmitters. to enable neurochemical imaging using a microelectrode array, the development of a high-density current-sensing microchip is necessary. here, a neurochemical microchip is introduced, featuring a 128-channel current sensing front-end capable of supporting 128 parallel neurochemical measurements. the designed amplifier array employs a highly scalable resistive feedback transimpedance amplifier design. this design allows for a large neurochemical dynamic range of {+/-}5{micro}a with a noise performance as low as 0.22narms. with the integration of this microchip, in vivo neurochemical imaging of dopamine can be achieved with high spatiotemporal resolution."
https://doi.org/10.1101/2023.05.25.542289,2024-01-10,htlv-1 reverse transcriptase homology model provides structural basis for sensitivity to existing nucleoside/nucleotide reverse transcriptase inhibitors,"[""O'Donnell, J. S.; Tardiota, N.; Jaberolansar, N.; Lackenby, J. A.; Chappell, K. J.""]","the human t-lymphotropic virus type 1 (htlv-1) infects millions of people globally and is endemic to various resource-limited regions. infections persist for life and are associated with increased susceptibility to opportunistic infections and severe diseases including adult t cell leukemia/lymphoma (atll) and htlv-1-associated myelopathy-tropical spastic paraparesis (ham-tsp). no htlv-1-specific anti-retrovirals have been developed and it is unclear whether existing anti-retrovirals developed for treatment of human immunodeficiency virus (hiv) have efficacy against htlv-1. to understand the structural basis for therapeutic binding, homology modelling and machine learning were used to develop a structural model of the htlv-1 reverse transcriptase. with this, molecular docking experiments using a panel of fda-approved inhibitors of viral reverse transcriptases to assess their capacity for binding, and in turn, inhibition. importantly, nucleoside/nucleotide reverse transcriptase inhibitor (nrti) but not non-nucleoside reverse transcriptase inhibitors (nnrtis) were capable of binding the htlv-1 reverse transcriptase, with similar affinity to hiv-1 reverse transcriptase. by strengthening the rationale for clinical testing of therapies such as tenofovir alafenamide, zidovudine, lamivudine, and azvudine for treatment of htlv-1, this study has demonstrated the power of in silico structural biology approaches in drug design and therapeutic testing."
https://doi.org/10.1101/2023.10.11.561862,2024-01-11,biodistribution analysis of peptide-coated magnetic iron nanoparticles: a simple and quantitative method,"['Natarajan, P.; Horak, K.; Rowe, J.; Joshua, L. J.; Tomich, J.; Fleming, S.']","biodistribution is the tracking of compounds or molecules of interest in the subject which is integral to understanding their anticipated efficacy and safety. nanoparticles are highly desirable delivery systems which have the ability to deliver higher nucleic acid and drug payloads and they have enhanced tumor permeability due to their unique properties such as high surface area to volume ratio. studying the biodistribution of nanoparticles is crucial to understand their effectiveness and safety in vivo, facilitate a more application driven approach for nanoparticle development which will lead to their successful translation into clinical use. in this study, we present a relatively simple method to determine the biodistribution of magnetic iron nanoparticles in mice. branched amphiphilic peptide coated magnetic nanobeads bapc-mnbs like their counterpart i.e., branched amphiphilic peptide capsules (bapcs) with a hollow water-filled core, are readily taken up by cells in vitro and have widespread application as a nanodelivery systems. we evaluated the bapc-mnbs tissue distribution in wildtype mice injected intravenously (i.v.), intraperitoneally (i.p.) or orally gavaged to understand the biological interactions of the peptide nanoparticles and to further the development of branched amphiphilic peptides-based nanoparticles. bapc-mnbs were distributed widely to various organs when injected i.v. and were eliminated from the system via the intestines in feces. the spleen was found to accumulate the highest amount of bapc-mnbs in mice administered the nps i.v. and i.p. while they were not absorbed into the system via oral gavage. this study not only presents a relatively simple quantification method to determine in vivo biodistribution of magnetic iron nanoparticles that can be widely applied but also demonstrates the potential of branched amphiphilic peptides in the form of bapcs or bapc-mnbs as a delivery system."
https://doi.org/10.1101/2023.08.04.551687,2024-01-10,quantification of biases in predictions of protein-protein binding affinity changes upon mutations,"['Tsishyn, M.; Pucci, F.; Rooman, M.']","understanding the impact of mutations on protein-protein binding affinity is a key objective for a wide range of biotechnological applications and for shedding light on disease-causing mutations, which are often located at protein-protein interfaces. over the past decade, many computational methods using physics-based and/or machine learning approaches have been developed to predict how protein binding affinity changes upon mutations. they all claim to achieve astonishing accuracy on both training and test sets, with performances on standard benchmarks such as skempi 2.0 that seem overly optimistic. here we benchmarked eight well-known and well-used predictors and identified their biases and dataset dependencies, using not only skempi 2.0 as a test set but also deep mutagenesis data on the sars-cov-2 spike protein in complex with the human angiotensin-converting enzyme 2. we showed that, even though most of the tested methods reach a significant degree of robustness and accuracy, they suffer from limited generalizability properties and struggle to predict unseen mutations. interestingly, the generalizability problems are more severe for pure machine learning approaches while physics-based methods are less affected by this issue. moreover, undesirable prediction biases towards specific mutation properties, the most marked being towards destabilizing mutations, are also observed and should be carefully considered by method developers. we conclude from our analyses that there is room for improvement in the prediction models and suggest ways to check, assess and improve their generalizability and robustness."
https://doi.org/10.1101/2021.12.01.470748,2024-02-07,investigating the mutational landscape of the sars-cov-2 omicron variant via ab initio quantum mechanical modeling,"['Genovese, L.; Zaccaria, M.; Farzan, M.; Johnson, W. E.; Momeni, B.']","ab initio quantum mechanical models can characterize and predict intermolecular binding, but only recently have models including more than a few hundred atoms gained traction. here, we simulate [~]13,000 atoms to predict and characterize binding of sars-cov-2 spike variants to the human receptor ace2 (hace2). we compare four spike variants in our analysis: wuhan, omicron, and two omicron-based variants. to assess binding, we mechanistically characterize the energetic contribution of each amino acid involved, and predict the effect of select single point mutations. we validate our computational predictions experimentally by comparing binding efficacy of spike variants to cells expressing hace2. we argue that this computational model, qm-cr, can identify mutations critical for intermolecular interactions and inform the engineering of high-specificity interactors.  one-sentence summaryab initio modeling can predict the strength of sars-cov-2 variants binding to human cell receptor."
https://doi.org/10.1101/2023.09.11.557174,2024-01-10,minimal circuit motifs for second-order conditioning in the insect mushroom body,"['Jürgensen, A.-M.; Schmitt, F. J.; Nawrot, M. P.']","in well-established first-order conditioning experiments, the concurrence of a sensory cue with reinforcement forms an association, allowing the cue to predict future reinforcement. once a sensory cue is established as a predictor, it can also serve as indirect reinforcement, a phenomenon referred to as second-order conditioning. in the insect mushroom body, such associations are encoded in the plasticity of the synapses between the intrinsic and output neurons of the mushroom body, a process mediated by the activity of dopaminergic neurons that encode reinforcement signals. in second-order conditioning, a new sensory cue is paired with an already established one that presumably activates dopaminergic neurons due to its predictive power of the reinforcement. we explore minimal circuit motifs in the mushroom body for their ability to support second-order conditioning. we found that dopaminergic neurons can either be activated directly by the mushroom bodys intrinsic neurons or via feedback from the output neurons via several pathways. we demonstrate that the circuit motifs differ in their computational efficiency and robustness and suggest a particular motif that relies on feedforward input of the mushroom body intrinsic neurons to dopaminergic neurons as a promising additional candidate for experimental evaluation. it differentiates well between trained and novel stimuli, demonstrating robust performance across a range of model parameters."
https://doi.org/10.1101/2023.09.07.556756,2024-02-02,multi-population dissolution in confined active fluids,"['Fylling, C.; Tamayo, J.; Gopinath, A.; Theillard, M.']","autonomous out-of-equilibrium agents or cells in suspension are ubiquitous in biology and engineering. turning chemical energy into mechanical stress, they generate activity in their environment, which may trigger spontaneous large-scale dynamics. often, these systems are composed of multiple populations that may reflect the coexistence of multiple species, differing phenotypes, or chemically varying agents in engineered settings. here, we present a new method for modeling such multi-population active fluids subject to confinement. we use a continuum multi-scale mean-field approach to represent each phase by its first three orientational moments and couple their evolution with those of the suspending fluid. the resulting coupled system is solved using a parallel adaptive level-set-based solver for high computational efficiency and maximal flexibility in the confinement geometry. motivated by recent experimental work, we employ our method to study the spatiotemporal dynamics of confined bacterial suspensions and swarms dominated by fluid hydrodynamic effects. our computational explorations reproduce observed emergent collective patterns, including features of active dissolution in two-population active-passive swarms, with results clearly suggesting that hydrodynamic effects dominate dissolution dynamics. our work lays the foundation for a systematic characterization of natural and synthetic multi-population systems such as bacterial colonies, bird flocks, fish schools, colloidal swimmers, or programmable active matter."
https://doi.org/10.1101/2023.09.11.557224,2024-02-02,hitaxon: a hierarchical ensemble framework for taxonomic classification of short reads,"['Verma, B.; Parkinson, J.']","whole microbiome dna and rna sequencing (metagenomics and metatranscriptomics) are pivotal to determining functional roles within microbial communities. a key challenge in analysing these complex datasets, typically composed of tens of millions of short reads, is accurately classifying reads to their taxon of origin. traditional reference-based short-read classification tools are compromised by reference database biases, leading to interest in classifiers leveraging machine learning (ml) algorithms. while ml classifiers have shown promise, they still lag reference-based tools in species-level classification. to address this performance gap, attention has turned to approaches that incorporate the hierarchical structure of taxonomic classifications, albeit with limited results. here we introduce hitaxon, a hierarchical framework for creating ensembles of reference-dependent and ml classifiers. hitaxon facilitates data collection and processing, reference database construction and model training to streamline ensemble creation. we show that databases created by hitaxon improve the species-level performance of reference-dependent classifiers, while reducing their computational overhead. additionally, through exploring hierarchical methods for hitaxon, we highlight that our custom hierarchical ml approach improves species-level classification relative to traditional strategies. finally, we demonstrate the improved performance of our hierarchical ensemble over current state-of-the-art classifiers in species classification using datasets comprised of either simulated or experimentally-derived reads."
https://doi.org/10.1101/2021.06.18.449084,2024-02-02,"the foster method: rapid and non-invasive detection of clinically relevant american foulbrood disease levels using edna sampling and a dual-target qpcr assay, with its potential for other hive pathogens.","['Mackay, J.; Hewett, R. E.; Smith, N. T.; Waters, T. L.; Scandrett, J. S.']","clinical signs of american foulbrood (afb) can be difficult to diagnose and thus disease is often missed and leads to further spread. diagnosis is centred on the beekeepers skill in recognising clinical symptoms - a highly subjective and time-consuming activity. previous testing methods have relied on sampling that necessitates dismantling the hive and/or requires multiple visits to retrieve passive samples. the foster method is a novel environmental dna sampling method using colony entrance swabs together with a dual-target qpcr for paenibacillus larvae: the causative bacteria of afb disease. the quantification data generated can be used to detect hives with clinically significant infections, even before visual symptoms are apparent. such a sampling method will be applicable to other bee pathogens and incursion pests.  importancediscovery of american foulbrood disease in a honeybee colony typically means the destruction of the bees and hive by burning, in new zealand and many other countries. this discovery is typically by visual examination of capped brood by the beekeeper - a subjective skill that means the disease is being missed or not recognised. it is a time-consuming and exacting process to inspect hives for afb. here we present a novel rapid sampling method that does not require opening/ dismantling the hive, in conjunction with a dual-target quantitative pcr assay for the bacteria responsible, paenibacillus larvae. using the resulting quantitative data, hives presenting visual clinical symptoms or likely to soon become clinical, can be determined and the hives dealt with appropriately before further spread occurs. this study provides the basis for a novel way of sampling for honeybee pathogens and pests."
https://doi.org/10.1101/2023.07.25.549927,2024-02-02,sources of prey availability data alter interpretation of outputs from prey choice null networks,"['Cuff, J. P.; Tercel, M. P.; Windsor, F. M.; Hawthorne, B. S.; Hamback, P. A.; Bell, J. R.; Symondson, W. O.; Vaughan, I. P.']","o_linull models provide an invaluable baseline against which to test fundamental ecological hypotheses and highlight patterns in foraging choices that cannot be explained by neutral processes or sampling artefacts. in this way, null models can advance our understanding beyond simplistic dietary descriptions to identify drivers of interactions. this method, however, requires estimates of resource availability, which are generally imperfect representations of highly dynamic systems. optimising method selection is crucial for study design, but the precise effects of different resource availability data on the efficacy of null models are poorly understood. c_lio_liusing spider-prey networks as a model, we used prey abundance (suction sample) and activity density (sticky trap) data, and combinations of the two, to simulate null networks. we compared null diet composition, network properties (e.g., connectance and nestedness) and deviations of simulations from metabarcoding-based spider dietary data (to ascertain how different prey availability data alter ecological interpretation. c_lio_lidifferent sampling methods produced different null networks and inferred distinct prey selectivity. null networks based on prey abundance and combined frequency-of-occurrence data more closely resembled the observed diet composition, and those based on prey abundance, activity density and proportionally combined data generated network properties most like dietary metabarcoding networks. c_lio_liwe show that survey method choice impacts all aspects of null network analyses, the precise effects varying between methods but ultimately altering ecological interpretation by increasing disparity in network properties or trophic niches between null and directly constructed networks. merging datasets can generate more complete prey availability data but is not a panacea because it introduces different biases. the choice of method should reflect the research hypotheses and study system being investigated. ultimately, survey methods should emulate the foraging mode of the focal predator as closely as possible, informed by the known ecology, natural history and behaviour of the predator. c_li"
https://doi.org/10.1101/2023.05.25.542247,2024-02-01,contrasting functions of atp hydrolysis by mda5 and lgp2 in viral rna sensing,"['Singh, R.; Wu, Y.; Herrero del Valle, A.; Leigh, K. E.; Cheng, M. T. K.; Ferguson, B. J.; Modis, Y.']","cytosolic long double-stranded rna (dsrna), among the most potent proinflammatory signals, is recognized by mda5. mda5 binds dsrna cooperatively, forming helical filaments. atp hydrolysis by mda5 fulfills a proofreading function by promoting dissociation of shorter endogenous dsrnas from mda5 while allowing longer viral dsrnas to remain bound leading to activation of interferon-{beta} responses. here, we show that adjacent mda5 subunits in mda5-dsrna filaments hydrolyze atp cooperatively, inducing cooperative filament disassembly. this amplifies the rna footprint expansion that accompanies each round of atp hydrolysis and allows mda5 to displace tightly bound proteins from dsrna. our electron microscopy and biochemical assays show that lgp2 binds to dsrna at internal binding sites through noncooperative atp hydrolysis. unlike mda5, lgp2 has low nucleic acid selectivity and can hydrolyze gtp and ctp as well as atp. binding of lgp2 to dsrna promotes nucleation of mda5 filament assembly resulting in shorter filaments. molecular modeling of the mda5-lgp2 interface suggests that mda5 interacts with dsrna stem-bound rather than end-bound lgp2. we conclude that ntpase-dependent binding of lgp2 to internal sites on dsrna increases the number and signaling output of mda5-dsrna complexes. our work identifies novel molecular mechanisms contributing the selectivity and sensitivity of cytosolic dsrna sensing.  key pointso_licooperative atp hydrolysis in mda5 filaments confers selectivity for dsrna and displaces other proteins from rna c_lio_linoncooperative ntp hydrolysis by lgp2 induces binding to internal rna sites with low selectivity c_lio_lirna stem-bound lgp2 nucleates assembly of mda5 signaling complexes on a broader set of rna ligands c_li"
https://doi.org/10.1101/2023.12.01.568667,2024-02-01,aberration correction for deformable mirror based remote focusing enables high-accuracy whole-cell super-resolution imaging,"['Shi, W.; He, Y.; Wang, J.; Zhou, L.; Chen, J.; Zhou, L.; Xi, Z.; Wang, Z.; Fang, K.; Li, Y.']","single-molecule localization microscopy (smlm) enables three-dimensional (3d) investigation of nanoscale structures in biological samples, offering unique insights into their organization. however, traditional 3d super-resolution microscopy using high numerical aperture (na) objectives is limited by imaging depth of field (dof), restricting their practical application to relatively thin biological samples. here, we developed a unified solution for thick sample super-resolution imaging using a deformable mirror (dm) which was served for fast remote focusing, optimized point spread function (psf) engineering and accurate aberration correction. by effectively correcting the system aberrations introduced during remote focusing and sample aberrations at different imaging depths, we achieved high-accuracy, large dof imaging of the whole-cell organelles [i.e. nuclear pore complex (npc), microtubules, and mitochondria] with a nearly uniform resolution of approximately 30 nm across the entire cellular volume."
https://doi.org/10.1101/2023.07.20.549960,2024-02-01,ran-gtp assembles a specialized spindle structure for accurate chromosome segregation in medaka early embryos,"['Kiyomitsu, A.; Nishimura, T.; Hwang, S. J.; Ansai, S.; Kanemaki, M.; Tanaka, M.; Kiyomitsu, T.']","despite drastic cellular changes during cleavage divisions, a mitotic spindle is assembled in each blastomere to accurately segregate duplicated chromosomes. recent studies indicate that early embryonic divisions are highly error-prone in bovines and humans. however, processes and mechanisms of embryonic spindle assembly remain little understood in vertebrates. here, we established live functional assay systems in medaka fish (oryzias latipes) embryos by combining crispr knock-in with an auxin-inducible degron technology. in contrast to mammals, mitoses during cleavage divisions are very rapid (<12 min), but segregation errors are rarely observed. importantly, we found that the ran-gtp gradient assembles a specialized, dense microtubule network at the spindle midplane during metaphase, which is essential for faithful chromosome segregation in early embryos. in contrast, ran-gtp becomes dispensable for chromosome segregation in later stages, where spindles are morphologically remodeled into short, somatic-like spindles lacking the dense microtubule network. we propose that the specialized ran-based spindle structure ensures high fidelity of chromosome segregation in large, vertebrate early embryos."
https://doi.org/10.1101/2022.06.08.495397,2024-01-09,application of metallo-supramolecular branched polymer improves particle distribution and orientation in single-particle cryo-em,"['Xu, Y.; Wang, L.; Qin, Y.; Zhang, Y.; Wang, Y.; Dang, S.']","recent technological breakthroughs in single-particle cryo-electron microscopy (cryo-em) enabled rapid atomic structure determination of biological macromolecules. a major bottleneck in the current single particle cryo-em pipeline is the preparation of good quality frozen cryo-em grids, which is mostly a trial-and-error process. among many issues, preferred particle orientation and sample damage by air-water interface (awi) are common practical problems. here we reported a method of applying metallo-supramolecular branched polymer (msbp) in the cryo-sample preparation for high-resolution single-particle cryo-em. our data shows that msbp keeps a majority of particles away from air-water interface and mitigates preferred orientation as verified by the analyses of apoferritin, hemagglutinin (ha) trimer and various sample proteins. the use of msbp is a simple method to improve particle distribution for high-resolution structure determination in single-particle cryo-em."
https://doi.org/10.1101/2023.10.18.562948,2024-01-30,adaptive capacity of a dna polymerase clamp-loader atpase complex,"[""Subramanian, S.; Zhang, W.; Nimkar, S.; Kamel, M.; O'Donnell, M. E.; Kuriyan, J.""]","the ability of mutations to facilitate adaptation is central to evolution. to understand how mutations can lead to functional adaptation in a complex molecular machine, we created a defective version of the t4 clamp-loader complex, which is essential for dna replication. this variant, which is [~]5000-fold less active than the wildtype, was made by replacing the catalytic domains with those from another phage. a directed-evolution experiment revealed that multiple substitutions to a single negatively-charged residue in the chimeric clamp loader - asp 86 - restore fitness to within [~]20-fold of wildtype. these mutations remove an adventitious electrostatic repulsive interaction between asp 86 and the sliding clamp. deep mutagenesis shows that the reduced fitness of the chimeric clamp loader is compensated for by lysine and arginine substitutions of several dna-proximal residues in the clamp loader or the sliding clamp. thus, the fitness decrease of the chimeric clamp loader is caused by a reduction in affinity between the clamp loader and the clamp. our results demonstrate that there is a latent capacity for increasing affinity of the clamp loader for dna and the sliding clamp, such that even single point mutations can readily compensate for the loss of function due to suboptimal interactions elsewhere."
https://doi.org/10.1101/2022.02.04.479107,2024-02-01,model-free prediction of microbiome compositions,"['Asher, E. E.; Bashan, A.']","the recent recognition of the importance of the microbiome to the hosts health and well-being, has yielded efforts to develop therapies that aim to shift the microbiome from a disease-associated steady-state to a healthier one. direct manipulation techniques of the species assemblage are currently available, e.g., using probiotics or narrow-spectrum antibiotics to introduce or eliminate specific taxa. however, predicting the species abundances at the new steady-state remains a challenge, mainly due to the difficulties of deciphering the delicate underlying network of ecological interactions or constructing a predictive model for such complex ecosystems. here, we propose a model-free method to predict the species abundances at the new steady state based on their presence/absence configuration by utilizing a multi-dimensional k-nearest-neighbors (knn) regression algorithm. by analyzing data from numeric simulations of ecological dynamics, we show that our predictions, which consider the presence/absence of all species holistically, outperform both the null model that uses the statistics of each species independently and a predictive neural network model. we analyze real metagenomic data of human-associated microbial communities and found that by relying on a small number of  neighboring samples, i.e., samples with similar species assemblage, the knn predicts the species abundance better than the whole-cohort average. by studying both real metagenomic and simulated data, we show that the predictability of our method is tightly related to the dissimilarity-overlap relationship of the training data. our results demonstrate how model-free methods can prove useful in predicting microbial communities and may facilitate the development of microbial-based therapies."
https://doi.org/10.1101/379701,2024-02-01,a theoretical approach for discriminating accurately intrinsic pattern of biological systems and recognizing three kind soybean proteomes,"['Zou, H.']","proteomics is able to reveal plentiful information related to different physiological and pathological states of biology. further, the determination of accurately proteomic pattern is the essential platform for deeply proteomic research. while this has been somewhat ignored so far. in this article the quantitative standard pg=61%, a biological similarity constant for discriminating accurately intrinsic proteomic patterns was established depending on biological common heredity and variation information equation in symmetric variation state. on the other hand, a novel theoretical method was proposed for linearly dividing nonlinear data sequence into linear segments. the proteomes of three kind soybeans were precisely distinguished from one another by analyzing their infrared fingerprint spectra relying on this theoretically systemic approach. additionally, methods employed in this paper enable us to quickly, accurately and quantitatively determine the proteomic patterns without using any prior knowledge and learning samples, and without using electrophoresis, high performance liquid chromatography-mass spectrometry techniques, which are high cost, time-consuming. this approach provide us with an excellent one for quickly accurate determining biological species, physiological states and diagnosing pathological states based on proteomes."
https://doi.org/10.1101/2023.06.28.546839,2024-01-08,contextualising the developability risk of antibodies with lambda light chains using enhanced therapeutic antibody profiling,"['Raybould, M. I. J.; Turnbull, O. M.; Suter, A.; Guloglu, B.; Deane, C. M.']","antibodies with lambda light chains ({lambda}-antibodies) are generally considered to be less developable than those with kappa light chains ({kappa}-antibodies), leading to substantial systematic biases in drug discovery pipelines. this has contributed to kappa dominance amongst clinical-stage therapeutics. however, the identification of increasing numbers of epitopes preferentially engaged by{lambda} -antibodies shows there is a functional cost to neglecting them as potential lead candidates during discovery campaigns. here, we update our therapeutic antibody profiler (tap) tool to use the latest data and machine learning-based structure prediction methods, and apply this new protocol to evaluate developability risk profiles for{kappa} -antibodies and{lambda} -antibodies based on their surface physicochemical properties. we find that{lambda} -antibodies are on average at a higher risk of poor developability -- as an indication, over 40% of single-cell sequenced human{lambda} -antibodies are flagged by tap for risk-prone patches of surface hydrophobicity (psh), compared to around 11% of human{kappa} -antibodies. nonetheless, a substantial proportion of natural{lambda} -antibodies are assigned more moderate risk profiles by tap and should therefore represent more tractable candidates for therapeutic development. we also analyse the populations of high and low risk antibodies, highlighting opportunities for strategic design that tap suggests would enrich for more developable{lambda} -based candidates. overall, we provide context to the differing developability of{kappa} - and{lambda} -antibodies, enabling a rational approach to incorporate more diversity into the initial pool of immunotherapeutic candidates."
https://doi.org/10.1101/2023.06.12.544667,2024-01-08,improving modelling for epidemic responses: reflections from members of the uk infectious disease modelling community on their experiences during the covid-19 pandemic,"['Sherratt, K.; Carnegie, A. C.; Kucharski, A.; Cori, A.; Pearson, C. A.; Jarvis, C. I.; Overton, C.; Weston, D.; Hill, E. M.; Knock, E.; Fearon, E.; Nightingale, E.; Hellewell, J.; Edmunds, W. J.; Arenas, J. V.; Prem, K.; Pi, L.; Baguelin, M.; Kendall, M.; Ferguson, N.; Davies, N.; Eggo, R. M.; van Elsland, S.; Russell, T.; Funk, S.; Liu, Y.; Abbott, S.']","the covid-19 pandemic both relied and placed significant burdens on the experts involved from research and public health sectors. the sustained high pressure of a pandemic on responders, such as healthcare workers, can lead to lasting psychological impacts including acute stress disorder, post-traumatic stress disorder, burnout, and moral injury, which can impact individual wellbeing and productivity. as members of the infectious disease modelling community, we convened a reflective workshop to understand the professional and personal impacts of response work on our community and to propose recommendations for future epidemic responses. the attendees represented a range of career stages, institutions, and disciplines. this piece was collectively produced by those present at the session based on our collective experiences. key issues we identified at the workshop were lack of institutional support, insecure contracts, unequal credit and recognition, and mental health impacts. our recommendations include rewarding impactful work, fostering academia-public health collaboration, decreasing dependence on key individuals by developing teams, increasing transparency in decision-making, and implementing sustainable work practices. despite limitations in representation, this workshop provided valuable insights into the uk covid-19 modelling experience and guidance for future public health crises. recognising and addressing the issues highlighted is crucial, in our view, for ensuring the effectiveness of epidemic response work in the future."
https://doi.org/10.1101/2023.03.22.533484,2024-01-31,rexprt: a machine learning tool to predict pathogenicity of tandem repeat loci,"['Fazal, S.; Danzi, M.; Xu, I.; Kobren, S. N.; Sunyaev, S.; Reuter, C.; Marwaha, S.; Wheeler, M. T.; Dolzhenko, E.; Lucas, F.; Wuchty, S.; Tekin, M.; Zuchner, S.; Aguiar-Pulido, V.']","tandem repeats (trs) are polymorphic sequences of dna that are composed of repeating units of motifs, whose lengths can vary depending on the type of tr. expansions of trs are responsible for approximately 50 monogenic diseases, compared to over 4,300 disease causing genes disrupted by single nucleotide variants and small indels. it appears thus reasonable to expect the discovery of additional pathogenic repeat expansions, which has the potential of significantly narrowing the current diagnostic gap in many diseases. recently, short and long-read whole genome sequencing with the use of advanced bioinformatics tools, have identified a growing number of tr expansions in the human population. the majority of these loci are expanded in <1% of genomes. categorizing and prioritizing such tr loci is a growing challenge to human genomic studies. we present a first-in-class machine learning tool, rexprt (repeat expansion pathogenicity prediction tool), which is designed to distinguish pathogenic from benign tr expansions. leave-one-out cross validation results demonstrated that an ensemble approach comprised of support vector machines (svm) and extreme gradient boosted decision tree (xgb) classify trs with a precision of 92% and a recall of 90%. further validation of rexprt on unseen test data demonstrate a similar precision of 86%, and a recall of 60%. rexprts high precision in particular, will be of significant value to large-scale discovery studies, which require the prioritization of promising candidate loci for time-consuming and costly functional follow-up studies. application of rexprt to ~800,000 trs in the reference genome identified ~30,000 trs that would be likely pathogenic upon expansion. thus, rexprt establishes a foundation for the application of machine learning approaches to categorize the pathogenicity of tandem repeat loci."
https://doi.org/10.1101/2022.03.23.485485,2024-01-31,ppml-omics: a privacy-preserving federated machine learning system protects patients' privacy from omic data,"['Zhou, J.; Chen, S.; Wu, Y.; Li, H.; Zhang, B.; Zhou, L.; Hu, Y.; Xiang, Z.; Li, Z.; Chen, N.; Han, W.; Wang, D.; Gao, X.']","modern machine learning models towards various tasks with omic data analysis give rise to threats of privacy leakage of patients involved in those datasets. despite the advances in different privacy technologies, existing methods tend to introduce too much computational cost (e.g. cryptographic methods) or noise (e.g. differential privacy), which hampers either model usefulness or accuracy in protecting privacy in biological data. here, we proposed a secure and privacy-preserving machine learning method (ppml-omics) by designing a decentralized version of the differential private federated learning algorithm. we applied ppml-omics to analyze data from three sequencing technologies, and addressed the privacy concern in three major tasks of omic data, namely cancer classification with bulk rna-seq, clustering with single-cell rna-seq, and the integration of spatial gene expression and tumour morphology with spatial transcriptomics, under three representative deep learning models. we also examined privacy breaches in depth through privacy attack experiments and demonstrated that ppml-omics could protect patients privacy. in each of these applications, ppml-omics was able to outperform methods of comparison under the same level of privacy guarantee, demonstrating the versatility of the method in simultaneously balancing the privacy-preserving capability and utility in practical omic data analysis. furthermore, we gave the theoretical proof of the privacy-preserving capability of ppml-omics, suggesting the first mathematically guaranteed method with robust and generalizable empirical performance in protecting patients privacy in omic data."
https://doi.org/10.1101/2023.07.21.550009,2023-12-25,multilayer label free non-faradic electrochemical impedance immunosensor for cortisol detection,"['Gupta, C.; Pattanayek, S. K.; Mukherjee, B.; Kumar, S.']","cortisol, a well-known psychological stress biomarker, produced by the hypothalamic-pituitary-adrenal system, tends to intensify with stressors. prolonged overexpression of cortisol leads to chronic stress that causes disparities in the proper functioning of the human body. thus, there is a huge demand for developing a rapid cortisol detection system. several point-of-care diagnostic techniques are available for rapid cortisol detection, such as electrochemical sensing, which works on changes in the electrical properties due to the binding of an analyte with a biorecognition element. researchers have used different electrochemical methodologies such as cyclic voltammetry (cv), chronoamperometry, and faradic electrochemical impedance spectroscopy (eis) for the detection of cortisol, but usage of external redox active reagents, low sensitivity, limited dynamic range, and electrode fouling nature limits their use. hence, we reported a label-free and non-invasive cortisol detection using non-faradic eis. a novel multilayer immunosensor was fabricated on pedot: pss coated ito glass by functionalizing with cortisol antibodies. specific and rapid detection of cortisol was measured by monitoring the change in impedance in a dynamic range from 50-200 ng/ml. we envision the developed immunosensor has the potential for new developments in stress monitoring, disease prognosis, and enable personalized care.  highlightso_linovel pedot: pss based multilayer immunosensor for cortisol detection c_lio_liimpedance based label free detection of cortisol using non-faradic eis c_lio_lipresentation of detailed multilayer immunosensor fabrication, experimental detection, and equivalent circuit model with working mechanism c_lio_licortisol detection in a dynamic range of 50-200 ng/ml c_li  graphical abstract  o_fig o_linksmallfig width=200 height=126 src=""figdir/small/550009v1_ufig1.gif"" alt=""figure 1""> view larger version (23k): org.highwire.dtl.dtlvardef@16f486forg.highwire.dtl.dtlvardef@fc9b3aorg.highwire.dtl.dtlvardef@333cf1org.highwire.dtl.dtlvardef@1e608e2_hps_format_figexp  m_fig c_fig"
https://doi.org/10.1101/2023.05.10.540124,2024-01-19,pandogen: generating complete instances of future sars-cov2 sequences using deep learning,"['Ramachandran, A.; Lumetta, S. S.; Chen, D.']","one of the challenges in a viral pandemic is the emergence of novel variants with different phenotypical characteristics. an ability to forecast future viral individuals at the sequence level enables advance preparation by characterizing the sequences and closing vulnerabilities in current preventative and therapeutic methods. in this article, we explore, in the context of a viral pandemic, the problem of generating complete instances of undiscovered viral protein sequences, which have a high likelihood of being discovered in the future using protein language models. current approaches to training these models fit model parameters to a known sequence set, which does not suit pandemic forecasting as future sequences differ from known sequences in some respects. to address this, we develop a novel method, called pandogen, to train protein language models towards the pandemic protein forecasting task. pandogen combines techniques such as synthetic data generation, conditional sequence generation, and reward-based learning, enabling the model to forecast future sequences, with a high propensity to spread. applying our method to modeling the sars-cov-2 spike protein sequence, we find empirically that our model forecasts twice as many novel sequences with five times the case counts compared to a model that is thirty times larger. our method forecasts unseen lineages months in advance, whereas models 4x and 30x larger forecast almost no new lineages. when trained on data available up to a month before the onset of important variants of concern, our method consistently forecasts sequences belonging to those variants within tight sequence budgets.  pandogen is available at: https://github.com/uiuc-chenlab/pandogen"
https://doi.org/10.1101/2023.07.12.548729,2024-01-31,estimating person-specific neural correlates of mental rotation: a machine learning approach,"['Uslu, S.; Tangermann, M.; Vögele, C.']","using neurophysiological measures to model how the brain performs complex cognitive tasks such as mental rotation is a promising way towards precise predictions of behavioural responses. the mental rotation task has frequently been used to monitor progressive neurological disorders. up until now, research on neural correlates of mental rotation have largely focused on group analyses yielding models with features common across individuals. here, we propose an individually tailored machine learning approach to identify person-specific patterns of neural activity during mental rotation. we trained ridge regressions to predict the reaction time of correct responses in a mental rotation task using task-related, non-invasive electrocortical activity of the same person. when tested on independent data of the same person, the regression model predicted the reaction times significantly more accurately than when only the average reaction time was used for prediction (bootstrap mean difference of 0.02, 95% ci: 0.01-0.03, p < .001). when tested on another persons data, the predictions were significantly less accurate compared to within-person predictions. further analyses revealed that considering person-specific reaction times and topographical activity patterns substantially improved a models generalizability. our results indicate that a more individualized approach towards neural correlates can improve their predictive performance of behavioural responses, particularly when combined with machine learning."
https://doi.org/10.1101/2023.05.08.539824,2024-01-31,direct prediction of intrinsically disordered protein conformational properties from sequence,"['Lotthammer, J. M.; Ginell, G. M.; Griffith, D.; Emenecker, R. J.; Holehouse, A. S.']","intrinsically disordered regions (idrs) are ubiquitous across all domains of life and play a range of functional roles. while folded domains are generally well-described by a single 3d structure, idrs exist in a collection of interconverting states known as an ensemble. this structural heterogeneity means idrs are largely absent from the pdb, contributing to a lack of computational approaches to predict ensemble conformational properties from sequence. here we combine rational sequence design, large-scale molecular simulations, and deep learning to develop albatross, a deep learning model for predicting idr ensemble dimensions from sequence. albatross enables the instantaneous prediction of ensemble average properties at proteome-wide scale. albatross is lightweight, easy-to-use, and accessible as both a locally installable software package and a point-and-click style interface in the cloud. we first demonstrate the applicability of our predictors by examining the generalizability of sequence-ensemble relationships in idrs. then, we leverage the high-throughput nature of albatross to characterize emergent biophysical behavior of idrs within and between proteomes.  update from previous versiono_lithis preprint reports an updated version of the albatross network weights trained on simulations of over 42,000 sequences. c_lio_liin addition, we provide new colab notebooks that enable proteome-wide idr prediction and annotation in minutes. c_lio_liall conclusions and observations made in versions 1 and 2 of this manuscript remain true and robust. c_li"
https://doi.org/10.1101/2021.03.15.435343,2023-12-23,"spiro-the automated petri plate imaging platform designed by biologists, for biologists","['Ohlsson, J. A.; Leong, J. X.; Elander, P. H.; Dauphinee, A. N.; Ballhaus, F.; Johansson, J.; Lommel, M.; Hofmann, G.; Betner, S.; Sandgren, M.; Schumacher, K.; Bozhkov, P. V.; Minina, A. E. A.']","phenotyping of model organisms grown on petri plates is often carried out manually, despite the procedures being time-consuming and laborious. the main reason for this is the limited availability of automated phenotyping facilities, whereas constructing a custom automated solution can be a daunting task for biologists.  here, we describe spiro, the smart plate imaging robot, an automated platform that acquires time-lapse photos of up to four vertically oriented petri plates in a single experiment, corresponding to 192 seedlings for a typical root growth assay and up to 2500 seeds for a germination assay. spiro is catered specifically to biologists needs, requiring no engineering or programming expertise for assembly and operation. its small footprint is optimized for standard incubators, the inbuilt green led enables imaging under dark conditions, and remote control provides access to the data without interfering with sample growth. spiros excellent image quality is suitable for automated image processing, which we demonstrate on the example of seed germination and root growth assays. furthermore, the robot can be easily customized for specific uses, as all information about spiro is released under open-source licenses.  importantly, uninterrupted imaging allows considerably more precise assessment of seed germination parameters and root growth rates compared to manual assays. moreover, spiro enables previously technically challenging assays such as phenotyping in the dark. we illustrate the benefits of spiro in proof-of-concept experiments which yielded a novel insight on the interplay between autophagy, nitrogen sensing and photoblastic response."
https://doi.org/10.1101/2023.05.26.542489,2024-01-13,sars-cov-2 lineage assignments using phylogenetic placement/usher are superior to pangolearn machine learning method,"[""de Bernardi Schneider, A.; Su, M.; Hinrichs, A. S.; Wang, J.; Amin, H.; Bell, J.; Wadford, D. A.; O'Toole, A.; Scher, E.; Perry, M. D.; Turakhia, Y.; De Maio, N.; Hughes, S.; Corbett-Detig, R.""]","with the rapid spread and evolution of sars-cov-2, the ability to monitor its transmission and distinguish among viral lineages is critical for pandemic response efforts. the most commonly used software for the lineage assignment of newly isolated sars-cov-2 genomes is pangolin, which offers two methods of assignment, pangolearn and pusher. pangolearn rapidly assigns lineages using a machine learning algorithm, while pusher performs a phylogenetic placement to identify the lineage corresponding to a newly sequenced genome. in a preliminary study, we observed that pangolearn (decision tree model), while substantially faster than pusher, offered less consistency across different versions of pangolin v3. here, we expand upon this analysis to include v3 and v4 of pangolin, which moved the default algorithm for lineage assignment from pangolearn in v3 to pusher in v4, and perform a thorough analysis confirming that pusher is not only more stable across versions but also more accurate. our findings suggest that future lineage assignment algorithms for various pathogens should consider the value of phylogenetic placement."
https://doi.org/10.1101/2023.08.03.551883,2024-01-31,compound activity prediction with dose-dependent transcriptomic profiles and deep learning,"['Godinez, W. J.; Trifonov, V.; Fang, B.; Kuzu, G.; Pei, L.; Guiguemde, W. A.; Martin, E. J.; King, F. J.; Jenkins, J. L.; Skewes-Cox, P.']","predicting compound activity in assays is a long-standing challenge in drug discovery. computational models based on compound-induced gene-expression signatures from a single profiling assay have shown promise towards predicting compound activity in other, seemingly unrelated, assays. applications of such models include predicting mechanisms-of-action (moa) for phenotypic hits, identifying off-target activities, and identifying polypharmacologies. here, we introduce transcriptomics-to-activity transformer (tat) models that leverage gene-expression profiles observed over compound treatment at multiple concentrations to predict compound activity in other biochemical or cellular assays. we built tat models based on gene-expression data from a rasl-seq assay to predict the activity of 2,692 compounds in 262 dose response assays. we obtained useful models for 51% of the assays as determined through a realistic held-out set. prospectively, we experimentally validated the activity predictions of a tat model in a malaria inhibition assay. with a 63% hit rate, tat successfully identified several sub-micromolar malaria inhibitors. our results thus demonstrate the potential of transcriptomic responses over compound concentration and the tat modeling framework as a cost-efficient way to identify the bioactivities of promising compounds across many assays."
https://doi.org/10.1101/2023.08.29.555381,2024-01-17,modulation of allostery with multiple mechanisms by hotspot mutations in tetr,"['Deng, J.; Yuan, Y.; Cui, Q.']","modulating allosteric coupling offers unique opportunities for biomedical applications. such efforts can benefit from efficient prediction and evaluation of allostery hotspot residues that dictate the degree of co-operativity between distant sites. we demonstrate that effects of allostery hotspot mutations can be evaluated qualitatively and semi-quantitatively by molecular dynamics simulations in a bacterial tetracycline repressor (tetr). the simulations recapitulate the effects of these mutations on abolishing the induction function of tetr and provide a rationale for the different degrees of rescuability observed to restore allosteric coupling of the hotspot mutations. we demonstrate that the same non-inducible phenotype could be the result of perturbations in distinct structural and energetic properties of tetr. our work underscore the value of explicitly computing the functional free energy landscapes to effectively evaluate and rank hotspot mutations despite the prevalence of compensatory interactions, and therefore provide quantitative guidance to allostery modulation for therapeutic and engineering applications.    o_fig o_linksmallfig width=200 height=111 src=""figdir/small/555381v3_ufig1.gif"" alt=""figure 1""> view larger version (19k): org.highwire.dtl.dtlvardef@16d5ddeorg.highwire.dtl.dtlvardef@12da7e3org.highwire.dtl.dtlvardef@693d33org.highwire.dtl.dtlvardef@1311afa_hps_format_figexp  m_fig o_floatnotoc graphicc_floatno c_fig"
https://doi.org/10.1101/2023.08.15.553454,2024-01-06,marsgt: multi-omics analysis for rare population inference using single-cell graph transformer,"['Wang, X.; Duan, M.; Li, J.; Ma, A.; Xu, D.; Li, Z.; Liu, B.; Ma, Q.']","rare cell populations are key in neoplastic progression and therapeutic response, offering potential intervention targets. however, their computational identification and analysis often lag behind major cell types. to fill this gap, we introduced marsgt: multi-omics analysis for rare population inference using single-cell graph transformer. it identifies rare cell populations using a probability-based heterogeneous graph transformer on single-cell multi-omics data. marsgt outperformed existing tools in identifying rare cells across 400 simulated and four real human datasets. in mouse retina data, it revealed unique subpopulations of rare bipolar cells and a muller glia cell subpopulation. in human lymph node data, marsgt detected an intermediate b cell population potentially acting as lymphoma precursors. in human melanoma data, it identified a rare mait-like population impacted by a high ifn-i response and revealed the mechanism of immunotherapy. hence, marsgt offers biological insights and suggests potential strategies for early detection and therapeutic intervention of disease."
https://doi.org/10.1101/2023.10.14.562348,2024-01-06,minimally sufficient experimental design using identifiability analysis,"['Kareva, I.; Gevertz, J. L.']","mathematical models are increasingly being developed and calibrated in tandem with data collection, empowering scientists to intervene in real time based on quantitative model predictions. well-designed experiments can help augment the predictive power of a mathematical model but the question of when to collect data to maximize its utility for a model is non-trivial. here we define data as model-informative if it results in a unique parametrization, assessed through the lens of practical identifiability. the framework we propose identifies an optimal experimental design (how much data to collect and when to collect it) that ensures parameter identifiability (permitting confidence in model predictions), while minimizing experimental time and costs. we demonstrate the power of the method by applying it to a modified version of a classic site-of-action pharmacokinetic/pharmacodynamic model that describes distribution of a drug into the tumor microenvironment (tme), where its efficacy is dependent on the level of target occupancy in the tme. in this context, we identify a minimal set of time points when data needs to be collected that robustly ensures practical identifiability of model parameters. the proposed methodology can be applied broadly to any mathematical model, allowing for the identification of a minimally sufficient experimental design that collects the most informative data."
https://doi.org/10.1101/2023.04.18.537346,2024-01-30,a statistical learning method for simultaneous copy number estimation and subclone clustering with single cell sequencing data,"['Qin, F.; Cai, G.; Xiao, F.']","the availability of single cell sequencing (scs) enables us to assess intra-tumor heterogeneity and identify cellular subclones without the confounding effect of mixed cells. copy number aberrations (cnas) have been commonly used to identify subclones in scs data using various clustering methods, since cells comprising a subpopulation are found to share genetic profile. however, currently available methods may generate spurious results (e.g., falsely identified cnas) in the procedure of cna detection, hence diminishing the accuracy of subclone identification from a large complex cell population. in this study, we developed a cna detection method based on a fused lasso model, referred to as flcna, which can simultaneously identify subclones in single cell dna sequencing (scdna-seq) data. spike-in simulations were conducted to evaluate the clustering and cna detection performance of flcna benchmarking to existing copy number estimation methods (scope, hmmcopy) in combination with the existing and commonly used clustering methods. interestingly, application of flcna to a real scdna-seq dataset of breast cancer revealed remarkably different genomic variation patterns in neoadjuvant chemotherapy treated samples and pre-treated samples. we show that flcna is a practical and powerful method in subclone identification and cna detection with scdna-seq data."
https://doi.org/10.1101/2023.05.18.541274,2024-01-07,plantc2u: deep learning of cross-species sequence landscapes predicts plastid c-to-u rna editing in plants,"['Xu, C.; Li, J.; Song, L.-Y.; Guo, Z.-J.; Song, S.-W.; Zhang, L.-D.; Zheng, H.-L.']","in plants, c-to-u rna editing is mainly occurred in the plastids and mitochondria transcripts, which contributes to complex transcriptional regulatory network. more evidences reveal that rna editing plays critical roles in plant growth and development. however, rna editing sites accurately detected by transcriptome sequencing data alone are still challenging. in the present study, we developed plantc2u, which is a convolutional neural network to predict plastid c-to-u rna editing based on the genomic sequence. plantc2u achieves over 95% sensitivity and 99% specificity, which outperforms random forest and support vector machine. plantc2u not only further checks rna editing sites from transcriptome data to reduce the possible false positives, but also assesses the effect of different mutations on c-to-u rna editing status based on the flanking sequences. moreover, we found the patterns of tissue-specific rna editing in mangrove plant kandelia obovata, and observed reduced c-to-u rna editing rates in cold stress response of k. obovata, suggesting their potential regulatory roles in the plants stress adaption. in addition, we present rnaeditdb, available online at https://jasonxu.shinyapps.io/rnaeditdb/. together, plantc2u and rnaeditdb would help researchers explore the rna editing events in plants and thus would be of broad utility for the plant research community.  highlightwe develop a convolutional neural network based deep learning, plantc2u program, which help researchers explore the plastids c-to-u rna editing events in plants and thus would be of broad utility for the plant research community."
https://doi.org/10.1101/2023.11.17.567577,2024-01-19,metanorm: incorporating meta-analytic priors into normalization of nanostring ncounter data,"['Barth, J. P.; Yang, Y.; Xiao, G.; Wang, X.']","non-informative or diffuse prior distributions are widely employed in bayesian data analysis to maintain objectivity. however, when meaningful prior information exists and can be identified, using an informative prior distribution to accurately reflect current knowledge may lead to superior outcomes and great efficiency. we propose metanorm, a bayesian algorithm for normalizing nanostring ncounter gene expression data. metanorm is based on rcrnorm, a powerful method designed under an integrated series of hierarchical models that allow various sources of error to be explained by different types of probes in the ncounter system. however, a lack of accurate prior information, weak computational efficiency, and instability of estimates that sometimes occur weakens the approach despite its impressive performance. metanorm employs priors carefully constructed from a rigorous meta-analysis to leverage information from large public data. combined with additional algorithmic enhancements, metanorm improves rcrnorm by yielding more stable estimation of normalized values, better convergence diagnostics and superior computational efficiency. r code for replicating the meta-analysis and the normalization function can be found at github.com/jbarth216/metanorm."
https://doi.org/10.1101/2023.11.23.568455,2024-01-30,fluorescent human rpa to track assembly dynamics on dna,"['Kaushik, V.; Chadda, R.; Kuppa, S.; Pokhrel, N.; Vayyeti, A.; Grady, S.; Arnatt, C.; Antony, E.']","dna metabolic processes including replication, repair, recombination, and telomere maintenance occur on single-stranded dna (ssdna). in each of these complex processes, dozens of proteins function together on the ssdna template. however, when double-stranded dna is unwound, the transiently open ssdna is protected and coated by the high affinity heterotrimeric ssdna binding replication protein a (rpa). almost all downstream dna processes must first remodel/remove rpa or function alongside to access the ssdna occluded under rpa. formation of rpa-ssdna complexes trigger the dna damage checkpoint response and is a key step in activating most dna repair and recombination pathways. thus, in addition to protecting the exposed ssdna, rpa functions as a gatekeeper to define functional specificity in dna maintenance and genomic integrity. rpa achieves functional dexterity through a multi-domain architecture utilizing several dna binding and protein-interaction domains connected by flexible linkers. this flexible and modular architecture enables rpa to adopt a myriad of configurations tailored for specific dna metabolic roles. to experimentally capture the dynamics of the domains of rpa upon binding to ssdna and interacting proteins we here describe the generation of active site-specific fluorescent versions of human rpa (rpa) using 4-azido-l-phenylalanine (4azp) incorporation and click chemistry. this approach can also be applied to site-specific modifications of other multi-domain proteins. fluorescence-enhancement through non-canonical amino acids (fencaa) and forster resonance energy transfer (fret) assays for measuring dynamics of rpa on dna are also described.  highlightso_lirpa is an essential protein for most dna metabolic processes including replication, repair, and recombination. c_lio_lirpa is a ssdna binding protein made of six domains situated across rpa70, rpa32 and rpa14 subunits. four high affinity dna binding domains engage the dna. c_lio_lisite-specific fluorescent probes were incorporated into two domains of rpa and report on ssdna binding dynamics. c_lio_libulk-level kinetic and single-molecule assays are described to monitor the binding and remodeling of individual rpa domains on ssdna. c_li"
https://doi.org/10.1101/2023.04.30.537485,2024-01-30,ancestral genome reconstruction enhances transposable element annotation by identifying degenerate integrants,"['Matsushima, W.; Planet, E.; Trono, D.']","growing evidence indicates that transposable elements (tes) play important roles in evolution by providing genomes with coding and non-coding elements. identification of te-derived functional elements, however, has relied on te annotations in individual species, which limits its scope to relatively intact te sequences and misses elements derived from evolutionarily old tes. here, we report a novel approach to uncover previously unannotated degenerate tes (degtes) by probing multiple ancestral genomes reconstructed from hundreds of species. we applied this method to the human genome and discovered 1,452,810 degtes, representing a 10.8% increase over the most recent human te coverage. further, we discovered that degtes contribute to various cis-regulatory elements as well as transcription factor binding sites, including those of a known te-controlling family, the krab zinc-finger proteins. we also report unannotated chimeric transcripts between degtes and human genes expressed in embryos. this study provides a novel methodology and a freely available resource that will facilitate the investigation of te co-option events on a full scale."
https://doi.org/10.1101/2023.08.31.555680,2024-01-30,improving split reporters of protein-protein interactions through orthology-based protein engineering,"['Rakotoarison, L.-M.; Tebo, A. G.; Böken, D.; Gautier, A.']","protein-protein interactions (ppi) can be detected through selective complementation of split fluorescent reporters made of two complementary fragments that reassemble into a functional fluorescent reporter when in close proximity. we previously introduced splitfast, a chemogenetic ppi reporter with rapid and reversible complementation. here, we present the engineering of rspa-splitfast, an improved reporter displaying higher brightness, lower self-complementation and higher dynamic range for optimal monitoring of ppi using an original protein engineering strategy that exploits proteins with orthology relationships. our study allowed the identification of a system with improved properties and enabled a better understanding of the molecular features controlling the complementation properties. because of the rapidity and reversibility of its complementation, its low self-complementation, high dynamic range, and improved brightness, rspa-splitfast is well suited to study ppi with high spatial and temporal resolution, opening great prospects to decipher the role of ppi in various biological contexts."
https://doi.org/10.1101/2023.06.29.547134,2023-12-27,flexible protein-protein docking with a multi-track iterative transformer,"['Chu, L.-S.; Ruffolo, J. A.; Harmalkar, A.; Gray, J. J.']","conventional protein-protein docking algorithms usually rely on heavy candidate sampling and re-ranking, but these steps are time-consuming and hinder applications that require high-throughput complex structure prediction, e.g., structure-based virtual screening. existing deep learning methods for protein-protein docking, despite being much faster, suffer from low docking success rates. in addition, they simplify the problem to assume no conformational changes within any protein upon binding (rigid docking). this assumption precludes applications when binding-induced conformational changes play a role, such as allosteric inhibition or docking from uncertain unbound model structures. to address these limitations, we present geodock, a multi-track iterative transformer network to predict a docked structure from separate docking partners. unlike deep learning models for protein structure prediction that input multiple sequence alignments (msas), geodock inputs just the sequences and structures of the docking partners, which suits the tasks when the individual structures are given. geodock is flexible at the protein residue level, allowing the prediction of conformational changes upon binding. for a benchmark set of rigid targets, geodock obtains a 41% success rate, outperforming all the other tested methods. for a more challenging benchmark set of flexible targets, geodock achieves a similar number of top-model successes as the traditional method cluspro [1], but fewer than replicadock2 [2]. geodock attains an average inference speed of under one second on a single gpu, enabling its application in large-scale structure screening. although binding-induced conformational changes are still a challenge owing to limited training and evaluation data, our architecture sets up the foundation to capture this backbone flexibility. code and a demonstration jupyter notebook are available at https://github.com/graylab/geodock."
https://doi.org/10.1101/2023.11.15.567243,2024-01-06,multiplexed live-cell imaging for drug responses in patient-derived organoid models of cancer,"['Colling, K. E.; Symons, E. L.; Buroni, L.; Sumanasiri, H. K.; Andrew-Udoh, J.; Witt, E.; Losh, H. A.; Morrison, A. M.; Leslie, K. K.; Dunnill, C. J.; De Bono, J. S.; Thiel, K. W.']","patient-derived organoid (pdo) models of cancer are a multifunctional research system that better recapitulates human disease as compared to cancer cell lines. pdo models can be generated by culturing patient tumor cells in extracellular basement membrane extracts (bme) and plating as three-dimensional domes. however, commercially available reagents that have been optimized for phenotypic assays in monolayer cultures often are not compatible with bme. herein we describe a method to plate pdo models and assess drug effects using an automated live-cell imaging system. in addition, we apply fluorescent dyes that are compatible with kinetic measurements to simultaneously quantitate cell health and apoptosis. image capture can be customized to occur at regular time intervals over several days. users can analyze drug effects in individual z-plane images or a z projection of serial images from multiple focal planes. using masking, specific parameters of interest are calculated, such as pdo number, area, and fluorescence intensity. we provide proof-of-concept data demonstrating the effect of cytotoxic agents on cell health, apoptosis and viability. this automated kinetic imaging platform can be expanded to other phenotypic readouts to understand diverse therapeutic effects in pdo models of cancer.  summarypatient-derived tumor organoids are a sophisticated model system for basic and translational research. this methods article details the use of multiplexed fluorescent live-cell imaging for simultaneous kinetic assessment of different organoid phenotypes."
https://doi.org/10.1101/2022.11.14.516396,2024-01-05,tracking lexical and semantic prediction error underlying the n400 using artificial neural network models of sentence processing,"['Lopopolo, A.; Rabovsky, M.']","recent research has shown that the internal dynamics of an artificial neural network model of sentence comprehension displayed a similar pattern to the amplitude of the n400 in several conditions known to modulate this event-related potential. these results led rabovsky, hansen, and mcclelland (2018) to suggest that the n400 might reflect change in an implicit predictive representation of meaning corresponding to semantic prediction error. this explanation stands as an alternative to the hypothesis that the n400 reflects lexical prediction error as estimated by word surprisal (frank, otten, galli, & vigliocco, 2015). in the present study, we directly model the amplitude of the n400 elicited during naturalistic sentence processing by using as predictor the update of the distributed representation of sentence meaning generated by a sentence gestalt model (mcclelland, st. john, & taraban, 1989) trained on a large-scale text corpus. this enables a quantitative prediction of n400 amplitudes based on a cognitively motivated model, as well as quantitative comparison of this model to alternative models of the n400. specifically, we compare the update measure from the sg model to surprisal estimated by a comparable language model trained on next word prediction. our results suggest that both sentence gestalt update and surprisal predict aspects of n400 amplitudes. thus, we argue that n400 amplitudes might reflect two distinct but probably closely related sub-processes that contribute to the processing of a sentence."
https://doi.org/10.1101/2023.01.17.524432,2024-01-09,dynamic effects of ventral hippocampal nrg3/erbb4 signaling on nicotine withdrawal-induced responses,"[""Fisher, M. L.; Prantzalos, E.; O'Donovan, B.; Anderson, T.; Sahoo, P. K.; Twiss, J. L.; Ortinski, P. I.; Turner, J. R.""]","tobacco smoking remains a leading cause of preventable death in the united states, with a less than 5% success rate for smokers attempting to quit. high relapse rates have been linked to several genetic factors, indicating that the mechanistic relationship between genes and drugs of abuse is a valuable avenue for the development of novel smoking cessation therapies. for example, various single nucleotide polymorphisms (snps) in the gene for neuregulin 3 (nrg3) and its cognate receptor, the receptor tyrosine-protein kinase erbb-4 (erbb4), have been linked to nicotine addiction. our lab has previously shown that erbb4 plays a role in anxiety-like behavior during nicotine withdrawal (wd); however, the neuronal mechanisms and circuit-specific effects of nrg3-erbb4 signaling during nicotine and wd are unknown. the present study utilizes genetic, biochemical, and functional approaches to examine the anxiety-related behavioral and functional role of nrg3-erbb4 signaling, specifically in the ventral hippocampus (vh). we report that 24hwd from nicotine is associated with altered synaptic expression of vh nrg3 and erbb4, and genetic disruption of vh erbb4 leads to an elimination of anxiety-like behaviors induced during 24hwd. moreover, we observed attenuation of gabaergic transmission as well as alterations in ca2+-dependent network activity in the ventral ca1 area of vh erbb4 knock-down mice during 24hwd. our findings further highlight contributions of the nrg3-erbb4 signaling pathway to anxiety-related behaviors seen during nicotine wd."
https://doi.org/10.1101/2022.11.17.516880,2024-01-02,ecole: learning to call copy number variants on whole exome sequencing data,"['Mandiracioglu, B.; Ozden, F.; Alkan, C.; Cicek, A. E.']","copy number variants (cnv) are shown to contribute to the etiology of several genetic disor{-}ders. accurate detection of cnvs on whole exome sequencing (wes) data has been a long sought-after goal for use in clinics. this was not possible despite recent improvements in performance because algo{-}rithms mostly suffer from low precision and even lower recall on expert-curated gold standard call sets. here, we present a deep learning-based somatic and germline cnv caller for wes data, named ecole. based on a variant of the transformer architecture, the model learns to call cnvs per exon, using high-confidence calls made on matched wgs samples. we further train and fine-tune the model with a small set of expert calls via transfer learning. we show that ecole achieves high performance on human ex{-}pert labeled data for the first time with 68.7% precision and 49.6% recall. this corresponds to precision and recall improvements of 18.7% and 30.8% over the next best-performing methods, respectively. we also show that the same fine-tuning strategy using tumor samples enables ecole to detect rt-qpcr validated variations in bladder cancer samples without the need for a control sample. ecole is available at https://github.com/ciceklab/ecole."
https://doi.org/10.1101/2023.08.24.554574,2024-01-05,genome replication in asynchronously growing microbial populations,"['Pflug, F.; Bhat, D.; Pigolotti, S.']","biological cells replicate their genomes in a well-planned manner. the dna replication program of an organism determines the timing at which different genomic regions are replicated, with fundamental consequences for cell homeostasis and genome stability. qualitatively, in a growing cell culture, one expects that genomic regions that are replicated early should be more abundant than regions that are replicated late. this abundance pattern can be experimentally measured using deep sequencing. however, a general quantitative theory to explain these data is still lacking. in this paper, we predict the abundance of dna fragments in asynchronously growing cultures from any given stochastic model of the dna replication program. as key examples, we present stochastic models of the dna replication programs in escherichia coli and in budding yeast. in both cases, our approach leads to analytical predictions that are in excellent agreement with experimental data and permit to infer key information about the replication program. in particular, our method is able to infer the locations of known replication origins in budding yeast with high accuracy. these examples demonstrate that our method can provide insight into a broad range of organisms, from bacteria to eukaryotes."
https://doi.org/10.1101/2023.10.26.564210,2024-01-17,a facile method for determining lanthipeptide stereochemistry,"['Luo, Y.; Xu, S.; van der Donk, W.']","lanthipeptides are a large group of natural products that belong to the ribosomally synthesized and post-translationally modified peptides (ripps). lanthipeptides contain lanthionine and methyllanthionine bis amino acids that have varying stereochemistry. the stereochemistry of new lanthipeptides is often not determined because current methods require equipment that is not standard in most laboratories. in this study, we developed a facile, efficient, and user-friendly method for detecting lanthipeptide stereochemistry utilizing advanced marfeys analysis. under optimized conditions, 0.05 mg peptide is sufficient to characterize the stereochemistry of five (methyl)lanthionines of different stereochemistry using a simple liquid chromatography set-up, which is a much lower detection limit than current methods. in addition, we describe methods to readily access standards of the three different methyllanthionine stereoisomers and the two different lanthionine stereoisomers that have been reported in known lanthipeptides. the developed workflow uses commonly used non-chiral column system and offers a scalable platform to assist antimicrobial discovery. we illustrate its utility with an example of a lanthipeptide discovered by genome mining."
https://doi.org/10.1101/2023.06.22.546080,2023-12-30,deeprank-gnn-esm: a graph neural network for scoring protein-protein models using protein language model,"['Xu, X.; Bonvin, A. M.']","motivationprotein-protein interactions (ppis) play critical roles in numerous cellular processes. by modelling the three-dimensional structures of the correspond protein complexes valuable insights can be obtained, providing, for example, starting points for drug and protein design. one challenge in the modelling process is however the identification of near-native models from the large pool of generated models. to this end we previously developed deeprank-gnn, a graph neural network that integrates structural and sequence information to enable effective pattern learning at ppi interfaces. its main features are related to the position specific scoring matrices (pssm), which are computationally expensive to generate and significantly limit the algorithms usability.  resultswe introduce here deeprank-gnn-esm that includes as additional features protein language model embeddings from the ems-2 model. we show that the esm-2 embeddings can actually replace the pssm features at no cost in-, or even better performance on two ppi-related tasks: scoring docking poses and detecting crystal artifacts. this new deeprank version bypasses thus the need of generating pssm, greatly improving the usability of the software and opening new application opportunities for systems for which pssm profiles cannot be obtained or are irrelevant (e.g. antibody-antigen complexes).  availability and implementationdeeprank-gnn-esm is freely available from https://github.com/deeprank/deeprank-gnn-esm"
https://doi.org/10.1101/2022.01.25.477771,2024-01-02,the alt pathway generates telomere fusions that can be detected in the blood of cancer patients,"['Muyas, F.; Gomez Rodriguez, M. J.; Cortes-Ciriano, I.; Flores, I.']","telomere fusions (tfs) can trigger the accumulation of diverse genomic rearrangements and the acquisition of oncogenic alterations leading to malignant transformation and resistance to chemotherapy. despite their relevance in tumour evolution, our understanding of the patterns and consequences of tfs in human cancer remains limited. here, we have characterized the rates and spectrum of somatic tfs across >30 cancer types using whole-genome sequencing data. tfs are pervasive in human tumours with rates varying markedly across and within cancer types. in addition to end-to-end fusions, we find novel patterns of tfs that we mechanistically link to the activity of the alternative lengthening of telomeres (alt) pathway. we show that tfs can be detected in the blood of cancer patients, which enables cancer detection with high specificity and sensitivity even for early-stage tumours and cancer types for which early detection remains a high unmet clinical need, such as pancreatic cancer and brain tumours. overall, we report a novel genomic footprint that enables characterization of the telomere maintenance mechanism of tumours and liquid biopsy analysis, which has implications for early detection, prognosis, and treatment selection."
https://doi.org/10.1101/2022.02.21.481356,2024-01-04,supervised rank aggregation (sra): a novel rank aggregation approach for ensemble-based feature selection,"['Jain, R.; Xu, W.']","backgroundfeature selection (fs) is critical for high dimensional data analysis. ensemble based feature selection (efs) is a commonly used approach to develop fs techniques. rank aggregation (ra) is an essential step of efs where results from multiple models are pooled to estimate feature importance. however, the literature primarily relies on rule-based methods to perform this step which may not always provide an optimal feature set.  method and resultsthis study proposes a novel supervised rank aggregation (sra) approach to allow ra step to dynamically learn and adapt the model aggregation rules to obtain feature importance. the approach creates a performance matrix containing feature and model performance value from all models and prepares a supervised learning model to get the feature importance. then, unsupervised learning is performed to select the features using their importance. we evaluate the performance of the algorithm using simulation studies and implement it into real research studies, and compare its performance with various existing ra methods. the proposed sra method provides better or at par performance in terms of feature selection and predictive performance of the model compared to existing methods.  conclusionsra method provides an alternative to the existing approaches of ra for efs. while the current study is limited to the continuous cross-sectional outcome, other endpoints such as longitudinal, categorical, and time-to-event medical data could also be used."
https://doi.org/10.1101/2023.07.27.550881,2024-01-29,transfer learning to leverage larger datasets for improved prediction of protein stability changes,"['Dieckhaus, H.; Brocidiacono, M.; Randolph, N.; Kuhlman, B.']","amino acid mutations that lower a proteins thermodynamic stability are implicated in numerous diseases, and engineered proteins with enhanced stability are important in research and medicine. computational methods for predicting how mutations perturb protein stability are therefore of great interest. despite recent advancements in protein design using deep learning, in silico prediction of stability changes has remained challenging, in part due to a lack of large, high-quality training datasets for model development. here we introduce thermompnn, a deep neural network trained to predict stability changes for protein point mutations given an initial structure. in doing so, we demonstrate the utility of a newly released mega-scale stability dataset for training a robust stability model. we also employ transfer learning to leverage a second, larger dataset by using learned features extracted from a deep neural network trained to predict a proteins amino acid sequence given its three-dimensional structure. we show that our method achieves competitive performance on established benchmark datasets using a lightweight model architecture that allows for rapid, scalable predictions. finally, we make thermompnn readily available as a tool for stability prediction and design."
https://doi.org/10.1101/2023.09.13.557631,2024-01-04,searching for structure: characterizing the protein conformational landscape with clustering-based algorithms,"['Macke, A. C.; Stump, J. E.; Kelly, M. S.; Rowley, J.; Herath, V.; Mullen, S.; Dima, R. I.']","the identification and characterization of the main conformations from a protein population is a challenging, inherently high-dimensional problem. we introduce the secondary structural ensembles with machine learning (stela) double clustering method, which clusters protein structures based on the underlying ramachandran plot. our approach takes advantage of the relationship between the phi and psi dihedral angles in a protein backbone and the secondary structure of the protein. the classification of states as vectors composed of the clusters indices arising naturally from the ramachandran plot, followed by the hierarchical clustering of the vectors, enables the identification of the minima from the corresponding free energy landscape (fel) by lifting the high structure degeneracy found with existing approaches such as the rmsd-based clustering gromos. we compare the performance of stela with not only gromos but also with cats, the combinatorial averaged transient structure clustering method based on distributions of the phi and psi dihedral angle coordinates. using ensembles of conformations from molecular dynamics (md) simulations of either intrinsically disordered proteins (idps) of various lengths (tau protein fragments) or from local structures from a globular protein, we show that stela is the only clustering method that identifies nearly all the minima from the corresponding fels. in contrast, gromos yields a large number of clusters that cover the entire fel and cats, even with an additional clustering step, is unable to sample well the fel for long idps and for fragments from globular proteins as it misses important minima.  toc graphic  o_fig o_linksmallfig width=200 height=111 src=""figdir/small/557631v1_ufig1.gif"" alt=""figure 1""> view larger version (36k): org.highwire.dtl.dtlvardef@634f90org.highwire.dtl.dtlvardef@1fca966org.highwire.dtl.dtlvardef@d59bd9org.highwire.dtl.dtlvardef@1eadb94_hps_format_figexp  m_fig c_fig"
https://doi.org/10.1101/2023.07.10.548392,2024-01-02,unreviewed science in the news: the evolution of preprint media coverage from 2014-2021,"['Fleerackers, A.; Shores, K.; Chtena, N.; Alperin, J. P.']","it has been argued that preprint coverage during the covid-19 pandemic constituted a paradigm shift in journalism norms and practices. this study examines whether, in what ways, and to what extent this is the case using a sample of 11,538 preprints posted on four preprint servers--biorxiv, medrxiv, arxiv, and ssrn--that received coverage in 94 english-language media outlets between 2014-2021. we compared mentions of these preprints with mentions of a comparison sample of 397,446 peer reviewed research articles indexed in the web of science to identify changes in the share of media coverage that mentioned preprints before and during the pandemic. we found that preprint media coverage increased at a slow but steady rate pre-pandemic, then spiked dramatically. this increase applied only to covid-19-related preprints, with minimal or no change in coverage of preprints on other topics. in addition, the rise in preprint coverage was most pronounced among health and medicine-focused media outlets, which barely covered preprints before the pandemic but mentioned more covid-19 preprints than outlets focused on any other topic. these results suggest that the growth in coverage of preprints seen during the pandemic period may imply a shift in journalistic norms, including a changing outlook on reporting preliminary, unvetted research."
https://doi.org/10.1101/2023.09.28.560069,2024-01-29,comparison of high-throughput single-cell rna-seq methods for ex vivo drug screening,"['Gezelius, H.; Enblad, A. P.; Lundmark, A.; Aberg, M.; Blom, K.; Rudfeldt, J.; Raine, A.; Harila, A.; Rendo, V.; Heinaniemi, M.; Andersson, C.; Nordlund, J.']","functional precision medicine (fpm) aims to optimize patient-specific drug selection based on the unique characteristics of their cancer cells. recent advancements in high throughput ex vivo drug profiling have accelerated interest in fpm. here, we present a proof-of-concept study for an integrated experimental system that incorporates ex vivo treatment response with a single-cell gene expression output enabling barcoding of several drug conditions in one single-cell sequencing experiment. we demonstrate this through a proof-of-concept investigation focusing on the glucocorticoid-resistant acute lymphoblastic leukemia (all) e/r+ reh cell line. three different single-cell transcriptome sequencing (scrna-seq) approaches were evaluated, each exhibiting high cell recovery and accurate tagging of distinct drug conditions. notably, our comprehensive analysis revealed variations in library complexity, sensitivity (gene detection), and differential gene expression detection across the methods. despite these differences, we identified a substantial transcriptional response to fludarabine, a highly relevant drug for treating high-risk all, which was consistently recapitulated by all three methods. these findings highlight the potential of our integrated approach for studying drug responses at the single-cell level and emphasize the importance of method selection in scrna-seq studies. finally, our data encompassing 27,327 cells are freely available to extend to future scrna-seq methodological comparisons."
https://doi.org/10.1101/043794,2024-01-08,modeling methyl-sensitive transcription factor motifs with an expanded epigenetic alphabet,['Coby Viner;James Johnson;Nicolas Walker;Hui Shi;Marcela Sjöberg;David J. Adams;Anne C. Ferguson-Smith;Timothy L. Bailey;Michael M. Hoffman;'],"transcription factors bind dna in specific sequence contexts. in addition to distinguishing one nucleobase from another, some transcription factors can distinguish between unmodified and modified bases. current models of transcription factor binding tend not take dna modifications into account, while the recent few that do often have limitations. this makes a comprehensive and accurate profiling of transcription factor affinities difficult.  here, we developed methods to identify transcription factor binding sites in modified dna. our models expand the standard a/c/g/t dna alphabet to include cytosine modifications. we developed cytomod to create modified genomic sequences and enhanced the multiple em for motif elicitation (meme) suite by adding the capacity to handle custom alphabets. we adapted the well-established position weight matrix (pwm) model of transcription factor binding affinity to this expanded dna alphabet.  using these methods, we identified modification-sensitive transcription factor binding motifs. we confirmed established binding preferences, such as the preference of zfp57 and c/ebp{beta} for methylated motifs and the preference of c-myc for unmethylated e-box motifs. using known binding preferences to tune model parameters, we discovered novel modified motifs for a wide array of transcription factors. finally, we validated predicted binding preferences of oct4 using cleavage under targets and release using nuclease (cut&run) experiments across conventional, methylation-, and hydroxymethylation-enriched sequences. our approach readily extends to other dna modifications. as more genome-wide single-base resolution modification data becomes available, we expect that our method will yield insights into altered transcription factor binding affinities across many different modifications."
https://doi.org/10.1101/2022.12.27.522071,2024-01-04,"prop3d: a flexible, python-based platform for machine learning with protein structural properties and biophysical data","['Draizen, E. J.; Murillo, L. F.; Readey, J.; Mura, C.; Bourne, P. E.']","backgroundmachine learning (ml) has a rich history in structural bioinformatics, and modern approaches, such as deep learning, are revolutionizing our knowledge of the subtle relationships between biomolecular sequence, structure, function, dynamics and evolution. as with any advance that rests upon statistical learning approaches, the recent progress in biomolecular sciences is enabled by the availability of vast volumes of sufficiently-variable data. to be useful, such data must be well-structured, machine-readable, intelligible and manipulable. these and related requirements pose challenges that become especially acute at the computational scales typical in ml. furthermore, in structural bioinformatics such data generally relate to protein three-dimensional (3d) structures, which are inherently more complex than sequence-based data. a significant and recurring challenge concerns the creation of large, high-quality, openly-accessible datasets that can be used for specific training and benchmarking tasks in ml pipelines for predictive modeling projects, along with reproducible splits for training and testing.  resultshere, we report  prop3d, a platform that allows for the creation, sharing and extensible reuse of libraries of protein domains, featurized with biophysical and evolutionary properties that can range from detailed, atomically-resolved physicochemical quantities (e.g., electrostatics) to coarser, residue-level features (e.g., phylogenetic conservation). as a community resource, we also supply a  prop3d-20sf protein dataset, obtained by applying our approach to cath. we have developed and deployed the prop3d framework, both in the cloud and on local hpc resources, to systematically and reproducibly create comprehensive datasets via the highly scalable data service (hsds). our datasets are freely accessible via a public hsds instance, or they can be used with accompanying python wrappers for popular ml frameworks.  conclusionprop3d and its associated prop3d-20sf dataset can be of broad utility in at least three ways. firstly, the prop3d workflow code can be customized and deployed on various cloud-based compute platforms, with scalability achieved largely by saving the results to distributed hdf5 files via hsds. secondly, the linked prop3d-20sf dataset provides a hand-crafted, already-featurized dataset of protein domains for 20 highly-populated cath families; importantly, provision of this pre-computed resource can aid the more efficient development (and reproducible deployment) of ml pipelines. thirdly, prop3d-20sfs construction explicitly takes into account (in creating datasets and data-splits) the enigma of  data leakage, stemming from the evolutionary relationships between proteins."
https://doi.org/10.1101/2023.03.13.532279,2023-12-29,chemem: flexible docking of small molecules in cryo-em structures using difference maps,"['Sweeney, A.; Mulvaney, T.; Topf, M.']","the rapid advancement of the ""resolution revolution"" has propelled cryo-electron microscopy (cryo-em) to the forefront of structure-based drug discovery. however, the majority of cryo-em structures are solved at medium resolution (3-4[a]), an unexplored territory for small-molecule docking, due to difficulty in positioning ligands and the surrounding side-chains. therefore, the development of software capable of reliably and automatically docking ligands into cryo-em maps at such resolutions is of utmost importance. chemem is a novel method that employs cryo-em data, difference mapping, and a physico-chemical scoring function to flexibly dock one or multiple ligands in a protein binding site. to validate its effectiveness, chemem was assessed using a highly curated benchmark containing 33 experimental cryo-em structures, spanning a resolution range of 2.2-5.6 [a]. in all but one case, the method placed the ligands in the density in an accurate conformation, often better than the pdb deposited solutions. even without the use of cryo-em density, the chemem scoring function outperformed the well-established docking software autodock vina. furthermore, the study demonstrates that useful information is present in the map even at low resolutions. chemem unlocks the potential of medium-resolution cryo-em structures for drug discovery."
https://doi.org/10.1101/2023.09.28.560057,2024-01-04,the role of atp hydrolysis and product release in the translocation mechanism of sars-cov-2 nsp13,"['Lawal, M. M.; Roy, P.; McCullagh, M.']","in response to the emergence of covid-19, caused by sars-cov-2, there has been a growing interest in understanding the functional mechanisms of the viral proteins to aid in the development of new therapeutics. non-structural protein 13 (nsp13) helicase is an attractive target for antivirals because it is essential for viral replication and has a low mutation rate; yet, the structural mechanisms by which this enzyme binds and hydrolyzes atp to cause unidirectional rna translocation remain elusive. using gaussian accelerated molecular dynamics (gamd), we generated a comprehensive conformational ensemble of all substrate states along the atp-dependent cycle. shapegmm clustering of the protein yields four protein conformations that describe an opening and closing of both the atp pocket and rna cleft. this opening and closing is achieved through a combination of conformational selection and induction along the atp cycle. furthermore, three protein-rna conformations are observed that implicate motifs ia, iv, and v as playing a pivotal role in an atp-dependent inchworm translocation mechanism. finally, based on a linear discriminant analysis of protein conformations, we identify l405 as a pivotal residue for the opening and closing mechanism and propose a l405d mutation as a way of testing our proposed mechanism. this research enhances our understanding of nsp13s role in viral replication and could contribute to the development of antiviral strategies.  toc graphic  o_fig o_linksmallfig width=200 height=111 src=""figdir/small/560057v1_ufig1.gif"" alt=""figure 1""> view larger version (29k): org.highwire.dtl.dtlvardef@17afdfeorg.highwire.dtl.dtlvardef@1b95faborg.highwire.dtl.dtlvardef@137d575org.highwire.dtl.dtlvardef@9f3593_hps_format_figexp  m_fig c_fig"
https://doi.org/10.1101/2022.11.18.516120,2024-01-29,nlp-based tools for localization of the epileptogenic zone in patients with drug-resistant focal epilepsy,"['Mora, S.; Turrisi, R.; Chiarella, L.; Tassi, L.; Mai, R.; Nobili, L.; Barla, A.; Arnulfo, G.']","backgrounddrug-resistant focal epilepsy, defined by failure of two antiepileptic drugs, affects about 30% of patients with epilepsy. epilepsy surgery may represent an alternative options for this population. however, defining the epileptogenic zone to be surgically removed requires highly specialised medical expertise as well as advanced technologies. the aim of this work is building a cost-effective support system based on text, in particular based on the semiological descriptions of the seizures (temporal vs extratemporal lobe; right vs left hemisphere), in order to predict the localization of seizure origin.  methodsamong a population of 121 surgically treated and seizure-free drug-resistant patients suffering with focal epilepsy, recruited at the niguarda hospital in milan, we extracted a total number of 509 descriptions of seizures. after a data pre-processing phase, we used natural language processing tools to build numerical representations of the seizures descriptions, both using embedding and countbased methods. we then used machine learning models performing a binary classification into right/left and temporal/extra-temporal.  resultsall predictive models show a better performance when using the representations relying on embedding models respect to count-based ones. between all the combinations of representations and classifiers, the best performance obtained in terms of f1-score is 84.7% {+/-} 0.6.  discussionthis preliminary work reached encouraging results considering both localization tasks. the main advantage is that no specific knowledge about epilepsy is used to build the models, rendering our pipeline applicable also in other scenarios. the major limitation lies in the fact that the text is highly specific to the writer."
https://doi.org/10.1101/2023.07.06.547878,2024-01-04,mosaic-picasso: accurate crosstalk removal for multiplex fluorescence imaging,"['Cang, H.; Liu, Y.; Xing, J.']","ultra-multiplexed fluorescence imaging has revolutionized our understanding of biological systems, enabling the simultaneous visualization and quantification of multiple targets within biological specimens. a recent breakthrough in this field is picasso, a mutual-information-based technique capable of demixing up to 15 fluorophores without their spectra, thereby significantly simplifying the application of ultra-multiplexed fluorescence imaging. however, this study has identified a limitation of mutual information-based techniques. they do not differentiate between spatial colocalization and spectral mixing. consequently, mutual information-based demixing may incorrectly interpret spatially co-localized targets as non-colocalized, leading to overcorrection. we found that selecting regions within a multiplex image with low spatial similarity for measuring spectroscopic mixing results in more accurate demixing. this method effectively minimizes overcorrections and promises to accelerate the broader adoption of ultra-multiplex imaging."
https://doi.org/10.1101/2023.04.06.535847,2024-01-01,m-ionic: prediction of metal ion binding sites from sequence using residue embeddings,"['Shenoy, A.; Yogesh Kalakoti, Y.; Sundar, D.; Elofsson, A.']","motivationunderstanding metal-protein interaction can provide structural and functional insights into cellular processes. as the number of protein sequences increases, developing fast yet precise computational approaches to predict and annotate metal binding sites becomes imperative. quick and resource-efficient pre-trained protein language model (plm) embeddings have successfully predicted binding sites from protein sequences despite not using structural or evolutionary features (multiple sequence alignments). using residue-level embeddings from the plms, we have developed a sequence-based method (m-ionic) to identify metal-binding proteins and predict residues involved in metal-binding.  resultson independent validation of recent proteins, m-ionic reports an area under the curve (auroc) of 0.83 (recall=84.6%) in distinguishing metal-binding from non-binding proteins compared to auroc of 0.74 (recall =61.8%) of the next best method. in addition to comparable performance to the state-of-the-art method for identifying metal-binding residues (ca2+, mg2+, mn2+, zn2+), m-ionic provides binding probabilities for six additional ions (i.e., cu2+, po43-, so42-, fe2+, fe3+, co2+). we show that the plm embedding of a single residue contains sufficient information about its neighbours to predict its binding properties.  availability and implementationm-ionic can be used on your protein of interest using a google colab notebook (https://bit.ly/40frrbk). github repository (https://github.com/teamsundar/m-ionic) contains all code and data.  contactarne@bioinfo.se  supplementary informationsupplementary data are available at bioinformatics online."
https://doi.org/10.1101/2022.03.25.485816,2024-01-04,petascale pipeline for precise alignment of images from serial section electron microscopy,"['Popovych, S.; Macrina, T.; Kemnitz, N.; Castro, M. A.; Nehoran, B.; Jia, Z.; Bae, J. A.; Mitchell, E.; Mu, S.; Trautman, E. T.; Saalfeld, S.; Li, K.; Seung, H. S.']","the reconstruction of neural circuits from serial section electron microscopy (ssem) images is being accelerated by automatic image segmentation methods. segmentation accuracy is often limited by the preceding step of aligning 2d section images to create a 3d image stack. precise and robust alignment in the presence of image artifacts is challenging, especially as datasets are attaining the petascale. we present a computational pipeline for aligning ssem images with several key elements. self-supervised convolutional nets are trained via metric learning to encode and align image pairs, and they are used to initialize iterative fine-tuning of alignment. a procedure called vector voting increases robustness to image artifacts or missing image data. for speedup the series is divided into blocks that are distributed to computational workers for alignment. the blocks are aligned to each other by composing transformations with decay, which achieves a global alignment without resorting to a time-consuming global optimization. we apply our pipeline to a whole fly brain dataset, and show improved accuracy relative to prior state of the art. we also demonstrate that our pipeline scales to a cubic millimeter of mouse visual cortex. our pipeline is publicly available through two open source python packages."
https://doi.org/10.1101/2023.09.14.557719,2024-01-28,equipnas: improved protein-nucleic acid binding site prediction using protein-language-model-informed equivariant deep graph neural networks,"['Roche, R.; Moussad, B.; Shuvo, M. H.; Tarafder, S.; Bhattacharya, D.']","protein language models (plms) trained on a large corpus of protein sequences have shown unprecedented scalability and broad generalizability in a wide range of predictive modeling tasks, but their power has not yet been harnessed for predicting protein-nucleic acid binding sites, critical for characterizing the interactions between proteins and nucleic acids. here we present equipnas, a new plm-informed e(3) equivariant deep graph neural network framework for improved protein-nucleic acid binding site prediction. by combining the strengths of plm and symmetry-aware deep graph learning, equipnas consistently outperforms the state-of-the-art methods for both protein-dna and protein-rna binding site prediction on multiple datasets across a diverse set of predictive modeling scenarios ranging from using experimental input to alphafold2 predictions. our ablation study reveals that the plm embeddings used in equipnas are sufficiently powerful to dramatically reduce the dependence on the availability of evolutionary information without compromising on accuracy, and that the symmetry-aware nature of the e(3) equivariant graph-based neural architecture offers remarkable robustness and performance resilience. equipnas is freely available at https://github.com/bhattacharya-lab/equipnas."
https://doi.org/10.1101/2023.10.09.561498,2024-01-28,shiftr enables the unbiased and multiplexed identification of proteins bound to specific rna regions in live cells,"['Aydin, J.; Gabel, A.; Zielinski, S.; Ganskih, S.; Schmidt, N.; Hartigan, C. R.; Schenone, M.; Carr, S. A.; Munschauer, M.']","rna-protein interactions determine the cellular fate of rna and are central to regulating gene expression outcomes in health and disease. to date, no method exists that is able to identify proteins that interact with specific regions within endogenous rnas in live cells. here, we develop shiftr (selective rnase h-mediated interactome framing for target rna regions), an efficient and scalable approach to identify proteins bound to selected regions within endogenous rnas using mass spectrometry. compared to state-of-the-art techniques, shiftr is superior in accuracy, captures close to zero background interactions and requires orders of magnitude lower input material. we establish shiftr workflows for targeting rna classes of different length and abundance, including short and long non-coding rnas, as well as mrnas and demonstrate that shiftr is compatible with sequentially mapping interactomes for multiple target rnas in a single experiment. using shiftr, we comprehensively identify interactions of cis-regulatory elements located at the 5' and 3'- terminal regions of the authentic sars-cov-2 rna genome in infected cells and accurately recover known and novel interactions linked to the function of these viral rna elements. shiftr enables the systematic mapping of region-resolved rna interactomes for any rna in any cell type and has the potential to revolutionize our understanding of transcriptomes and their regulation."
https://doi.org/10.1101/2023.09.07.556690,2023-12-30,towards interpretable learned representations for ecoacoustics using variational auto-encoding,"['Gibb, K. A.; Eldridge, A.; Sandom, C. J.; Simpson, I. J. A.']","ao_scplowbstractc_scplowecoacoustics is an emerging science that seeks to understand the role of sound in ecological processes. passive acoustic monitoring is increasingly being used to collect vast quantities of whole-soundscape audio recordings in order to study variations in acoustic community activity across spatial and temporal scales. however, extracting relevant information from audio recordings for ecological inference is non-trivial. recent approaches to machine-learned acoustic features appear promising but are limited by inductive biases, crude temporal integration methods and few means to interpret downstream inference. to address these limitations we developed and trained a self-supervised representation learning algorithm -a convolutional variational auto-encoder (vae) -to embed latent features from acoustic survey data collected from sites representing a gradient of habitat degradation in temperate and tropical ecozones and use prediction of survey site as a test case for interpreting inference. we investigate approaches to interpretability by mapping discriminative descriptors back to the spectro-temporal domain to observe how soundscape components change as we interpolate across a linear classification boundary traversing latent feature space; we advance temporal integration methods by encoding a probabilistic soundscape descriptor capable of capturing multi-modal distributions of latent features over time. our results suggest that varying combinations of soundscape components (biophony, geophony and anthrophony) are used to infer sites along a degradation gradient and increased sensitivity to periodic signals improves on previous research using time-averaged representations for site classification. we also find the vae is highly sensitive to differences in recorder hardwares frequency response and demonstrate a simple linear transformation to mitigate the effect of hardware variance on the learned representation. our work paves the way for development of a new class of deep neural networks that afford more interpretable machine-learned ecoacoustic representations to advance the fundamental and applied science and support global conservation efforts."
https://doi.org/10.1101/2023.01.18.524531,2024-01-28,a method for predicting linear and conformational b-cell epitopes in an antigen from its primary sequence,"['Kumar, N.; Tripathi, S.; Sharma, N.; Patiyal, S.; Devi, N. L.; Raghava, G. P. S.']","b-cell is an essential component of the immune system that plays a vital role in providing the immune response against any pathogenic infection by producing antibodies. existing methods either predict linear or conformational b-cell epitopes in an antigen. in this study, a single method was developed for predicting both types (linear/conformational) of b-cell epitopes. the dataset used in this study contains 3875 b-cell epitopes and 3996 non-b-cell epitopes, where b-cell epitopes consist of both linear and conformational b-cell epitopes. our primary analysis indicates that certain residues (like asp, glu, lys, asn) are more prominent in b-cell epitopes. we developed machine-learning based methods using different types of sequence composition and achieved the highest auc of 0.80 using dipeptide composition. in addition, models were developed on selected features, but no further improvement was observed. our similarity-based method implemented using blast shows a high probability of correct prediction with poor sensitivity. finally, we came up with a hybrid model that combine alignment free (dipeptide based random forest model) and alignment-based (blast based similarity) model. our hybrid model attained maximum auc 0.83 with mcc 0.49 on the independent dataset. our hybrid model performs better than existing methods on an independent dataset used in this study. all models trained and tested on 80% data using cross-validation technique and final model was evaluated on 20% data called independent or validation dataset. a webserver and standalone package named ""clbtope"" has been developed for predicting, designing, and scanning b-cell epitopes in an antigen sequence (https://webs.iiitd.edu.in/raghava/clbtope/)."
https://doi.org/10.1101/531558,2024-01-28,same-species contamination detection with variant calling information from next generation sequencing,"['Jiang, T.; Buchkovich, M.; Motsinger-Reif, A.']","motivationsame-species contamination detection is an important quality control step in genetic data analysis. compared with widely discussed cross-species contamination, same-species contamination is more challenging to detect, and there is a scarcity of methods to detect and correct for this quality control issue. same-species contamination may be due to contamination by lab technicians or samples from other contributors. here, we introduce a novel machine learning algorithm to detect same species contamination in next generation sequence data using support vector machines. our approach uniquely detects such contamination using variant calling information stored in the variant call format (vcf) files (either dna or rna), and importantly can differentiate between same species contamination and mixtures of tumor and normal cells.  methodsin the first stage of our approach, a change-point detection method is used to identify copy number variations or copy number aberrations (cnvs or cnas) for filtering prior to testing for contamination. next, single nucleotide polymorphism (snp) data is used to test for same species contamination using a support vector machine model. based on the assumption that alternative allele frequencies in next generation sequencing follow the beta-binomial distribution, the deviation parameter {rho} is estimated by maximum likelihood method. all features of a radial basis function (rbf) kernel support vector machine (svm) are generated using either publicly available or private training data. lastly, the generated svm is applied in the test data to detect contamination. if training data is not available, a default rbf kernel svm model is used.  resultswe demonstrate the potential of our approach using simulation experiments, creating datasets with varying levels of contamination. the datasets combine, in silico, exome sequencing data of dna from two lymphoblastoid cell lines (na12878 and na10855). we generated vcf files using variants identified in these data, and then evaluated the power and false positive rate of our approach to detect same species contamination. our simulation experiments show that our method can detect levels of contamination as low as 5% with reasonable false positive rates. results in real data have sensitivity above 99.99% and specificity at 90.24%, even in the presence of dna degradation that has similar features to contaminated samples. additionally, the approach can identify the difference between mixture of tumor-normal cells and contamination. we provide an r software implementation of our approach using the defcon()function in the vanquish: variant quality investigation helper r package on cran."
https://doi.org/10.1101/2023.04.25.538248,2024-01-04,development and mobile deployment of a stair recognition system for human-robot locomotion,"['Kurbis, A. G.; Mihailidis, A.; Laschowski, B.']","environment sensing and recognition can improve the safety and autonomy of human-robot locomotion, especially during transitions between environmental states such as walking to and from stairs. however, accurate and real-time perception on edge devices with limited computational resources is an open problem. here we present the development and mobile deployment of stairnet - a vision-based automated stair recognition system powered by deep learning. building on exonet - the largest open-source dataset of egocentric images of real-world walking environments - we designed a new dataset specifically for stair recognition with over 515,000 images. we then developed a lightweight and efficient convolutional neural network for image classification, which accurately predicted complex stair environments with 98.4% accuracy. we also studied different model compression and optimization methods and deployed our system on several mobile devices running a custom-designed ios application with onboard accelerators using cpu, gpu, and/or npu backend computing. of the designs that we tested, our highest performing system showed negligible reductions in classification accuracy due to the model conversion for mobile deployment and achieved an inference time of 2.75 ms on an iphone 11. the high speed and accuracy of the stairnet system on edge devices opens new opportunities for autonomous control and planning of robotic prosthetic legs, exoskeletons, and other assistive technologies for human locomotion."
https://doi.org/10.1101/2023.04.21.537886,2024-01-04,uniform cerebral organoid culture on a pillar plate by simple and reproducible spheroid transfer from an ultralow attachment well plate,"['Acharya, P.; Joshi, P.; Shrestha, S.; Choi, N. Y.; Lee, M.-Y.']","human induced pluripotent stem cell (ipscs)-derived brain organoids have potential to recapitulate the earliest stages of brain development, serving as an effective in vitro model for studying both normal brain development and disorders. however, current brain organoid culture methods face several challenges, including low throughput, high variability in organoid generation, and time-consuming, multiple transfer and encapsulation of cells in hydrogels throughout the culture. these limitations hinder the widespread application of brain organoids including high-throughput assessment of compounds in clinical and industrial lab settings. in this study, we demonstrate a straightforward approach of generating multiple cerebral organoids from ipscs on a pillar plate platform, eliminating the need for labor-intensive, multiple transfer and encapsulation steps to ensure the reproducible generation of cerebral organoids. we formed embryoid bodies (ebs) in an ultra-low attachment (ula) 384-well plate and subsequently transferred them to the pillar plate containing matrigel, using a straightforward sandwiching and inverting method. each pillar on the pillar plate contains a single spheroid, and the success rate of spheroid transfer was in a range of 95 - 100%. by differentiating the ebs on the pillar plate, we achieved robust generation of cerebral organoids with a coefficient of variation (cv) below 19%. notably, our spheroid transfer method in combination with the pillar plate allows miniaturized culture of cerebral organoids, alleviates the issue of organoid variability, and has potential to significantly enhance assay throughput by allowing in situ organoid assessment as compared to conventional organoid culture in 6-/24-well plates, petri dishes, and spinner flasks."
https://doi.org/10.1101/2023.10.10.561755,2024-01-04,amberff at scale: multimillion-atom simulations with amber force fields in namd,"['Antolinez, S.; Jones, P. E.; Phillips, J. C.; Hadden-Perilla, J. A.']","all-atom molecular dynamics (md) simulations are an essential structural biology technique with increasing application to multimillion-atom systems, including viruses and cellular machinery. classical md simulations rely on parameter sets, such as the amber family of force fields (amberff), to accurately describe molecular motion. here, we present an implementation of amberff for use in namd that overcomes previous limitations to enable high-performance, massively-parallel simulations encompassing up to two billion atoms. single-point potential energy comparisons and case studies on model systems demonstrate that the implementation produces results that are as accurate as running amberff in its native engine."
https://doi.org/10.1101/2023.08.20.554030,2023-12-22,ascertainment bias in the genomic test of positive selection on regulatory sequences,"['Jiang, D.; Zhang, J.']","evolution of gene expression mediated by cis-regulatory changes is thought to be an important contributor to organismal adaptation, but identifying adaptive cis-regulatory changes is challenging due to the difficulty in knowing the expectation under no positive selection. a new approach for detecting positive selection on transcription factor binding sites (tfbss) was recently developed, thanks to the application of machine learning in predicting transcription factor (tf) binding affinities of dna sequences. given a tfbs sequence from a focal species and the corresponding inferred ancestral sequence that differs from the former at n sites, one can predict the tf binding affinities of many n-step mutational neighbors of the ancestral sequence and obtain a null distribution of the derived binding affinity, which allows testing whether the binding affinity of the real derived sequence deviates significantly from the null distribution. applying this test genomically to all experimentally identified binding sites of three tfs in humans, a recent study reported positive selection for elevated binding affinities of tfbss. here we show that this genomic test suffers from an ascertainment bias because, even in the absence of positive selection for strengthened binding, the binding affinities of known human tfbss are more likely to have increased than decreased in evolution. we demonstrate by computer simulation that this bias inflates the false positive rate of the selection test. we propose several methods to mitigate the ascertainment bias and show that almost all previously reported positive selection signals disappear when these methods are applied."
https://doi.org/10.1101/2023.10.10.561792,2024-01-21,hetmm: a michaelis-menten model for non-homogeneous enzyme mixtures,"['Douglas, J.; Carter, C. W.; Wills, P. R.']","the michaelis-menten model requires its reaction velocities to be measured from a preparation of homogeneous enzymes, with identical or near-identical catalytic activities. however, there are many cases where enzyme preparations do not satisfy this condition, or where one may wish to test the validity of this assumption. we introduce a kinetic model that relaxes this requirement, by assuming there are an unknown number of enzyme species drawn from an unknown probability distribution. this model features one additional parameter over the michaelis-menten model, describing the standard deviation of this distribution. we show that the assumption of homogeneity is usually sufficient even in non-homogeneous solutions, and only fails under extreme conditions where km spans orders of magnitude. we validate this method through simulation studies, demonstrating the method does not overfit to random noise, despite its increase in dimensionality. the two models can be accurately discriminated between even with moderate levels of experimental error. we applied this model to three homogeneous and three heterogeneous biological systems, showing that the standard and heterogeneous models outperform in either case, respectively. lastly, we show that heterogeneity is not readily distinguished from negatively-cooperative binding under the hill model. these two fundamentally distinct properties - inequality in catalytic ability and interference between binding sites - give similar michaelis-menten curves that are not readily resolved without further experimentation. our method allows testing for homogeneity and performing parameter inference in a bayesian framework, and is available online in the user-friendly hetmm package at https://github.com/jordandouglas/hetmm."
https://doi.org/10.1101/2022.02.23.481601,2023-12-29,performance reserves in brain-imaging-based phenotype prediction,"['Schulz, M.-A.; Bzdok, D.; Haufe, S.; Haynes, J.-D.; Ritter, K.']","machine learning studies have shown that various phenotypes can be predicted from structural and functional brain images. however, in most such studies, prediction performance ranged from moderate to disappointing. it is unclear whether prediction performance will substantially improve with larger sample sizes or whether insufficient predictive information in brain images impedes further progress. here, we systematically assess the effect of sample size on prediction performance using sample sizes far beyond what is possible in common neuroimaging studies. we project 3-9 fold improvements in prediction performance for behavioral and mental health phenotypes when moving from one thousand to one million samples. moreover, we find that moving from single imaging modalities to multimodal input data can lead to further improvements in prediction performance, often on par with doubling the sample size. our analyses reveal considerable performance reserves for neuroimaging-based phenotype prediction. machine learning models may benefit much more from extremely large neuroimaging datasets than currently believed."
https://doi.org/10.1101/2022.11.06.514786,2023-12-26,an integrated bayesian framework for multi-omics prediction and classification,"['Mallick, H.; Porwal, A.; Saha, S.; Svetnik, V.; Paul, E.']","with the growing commonality of multi-omics datasets, there is now increasing evidence that integrated omics profiles lead to the more efficient discovery of clinically actionable biomarkers that enable better disease outcome prediction and patient stratification. several methods exist to perform host phenotype prediction from crosssectional, single-omics data modalities but decentralized frameworks that jointly analyze multiple time-dependent omics data to highlight the integrative and dynamic impact of repeatedly measured biomarkers are currently limited. in this article, we propose a novel bayesian ensemble method to consolidate prediction by combining information across several longitudinal and cross-sectional omics data layers. unlike existing frequentist paradigms, our approach enables uncertainty quantification in prediction as well as interval estimation for a variety of quantities of interest based on posterior summaries. we apply our method to four published multi-omics datasets and demonstrate that it recapitulates known biology in addition to providing novel insights while also outperforming existing methods in estimation, prediction, and uncertainty quantification. our open-source software is publicly available at https://github.com/himelmallick/integratedlearner."
https://doi.org/10.1101/2023.06.21.545871,2024-01-27,co-linear chaining on pangenome graphs,"['Rajput, J.; Chandra, G.; Jain, C.']","pangenome reference graphs are useful in genomics because they compactly represent the genetic diversity within a species, a capability that linear references lack. however, efficiently aligning sequences to these graphs with complex topology and cycles can be challenging. the seed-chain-extend based alignment algorithms use co-linear chaining as a standard technique to identify a good cluster of exact seed matches that can be combined to form an alignment. recent works show how the co-linear chaining problem can be efficiently solved for acyclic pangenome graphs by exploiting their small width [makinen et al., talg19] and how incorporating gap cost in the scoring function improves alignment accuracy [chandra and jain, recomb23]. however, it remains open on how to effectively generalize these techniques for general pangenome graphs which contain cycles. here we present the first practical formulation and an exact algorithm for co-linear chaining on cyclic pangenome graphs. we rigorously prove the correctness and computational complexity of the proposed algorithm. we evaluate the empirical performance of our algorithm by aligning simulated long reads from the human genome to a cyclic pangenome graph constructed from 95 publicly available haplotype-resolved human genome assemblies. while the existing heuristic-based algorithms are faster, the proposed algorithm provides a significant advantage in terms of accuracy.  implementationhttps://github.com/at-cg/panaligner"
https://doi.org/10.1101/2023.11.07.566142,2024-01-17,ffcm-mrf: an accurate and generalizable cerebrovascular segmentation pipeline for humans and rhesus monkeys based on tof-mra,"['Cui, Y.; Huang, H.; Liu, J.; Zhao, M.; Li, C.; Han, X.; Luo, N.; Gao, J.; Yan, D.; Zhang, C.; Jiang, T.; Yu, S.']","purposecerebrovascular segmentation and quantification of vascular morphological features on humans and rhesus monkeys are essential for prevention, diagnosis, and treatment of brain diseases. however, current automated whole-brain vessel segmentation methods are often not generalizable to independent datasets, limiting their usefulness in real-world environments with their heterogeneity in participants, scanners, and species.  materials and methodsin this study, we proposed an automated, accurate and generalizable segmentation method for magnetic resonance angiography images called ffcm-mrf. this method integrates fast fuzzy c-means clustering and markov random field optimization using blood vessel shape priors and spatial constraints. we used a total of 123 human and 44 macaque mra images scanned at 1.5 t, 3 t, and 7 t mri from 9 datasets to develop and validate the method.  resultsthe average dice score coefficients for multiple independent datasets were 69.16-89.63%, with the improvements in ffcm-mrf ranged from 0.16-16.14% compared with state-of-the-art machine learning methods. quantitative analysis showed that ffcm-mrf can accurately segment major arteries in the circle of willis at the base of the brain and smaller distal pial arteries while effectively suppressing noise. test-retest analysis showed that the model yielded high vascular volume and diameter reliability.  conclusionsour results demonstrate that the proposed method is highly accurate and reliable and largely independent of variations in field strength, scanner platforms, acquisition parameters, and species. the macaque mra data and user-friendly open-source toolbox are freely available at openneuro and github to facilitate studies of imaging biomarkers for cerebrovascular and neurodegenerative diseases."
https://doi.org/10.1101/2023.10.23.563472,2024-01-26,"pytme (python template matching engine): a fast, flexible, and multi-purpose template matching library for cryogenic electron microscopy data","['Maurer, V. J.; Siggel, M.; Kosinski, J.']","cryogenic electron microscopy (cryo-em) is a key method in structural and cell biology. analysis of cryo-em images requires interpretation of noisy, low-resolution densities which relies on identifying the most probable orientation of macromolecules in a target using template matching. many method-specific template matching software exist for single-particle cryo-em, cryo-electron tomography (cryo-et), or fitting atomic structures into averaged 3d maps of macromolecules. here, we report the python template matching engine (pytme), a software engine that consolidates method-specific template matching problems. the underlying library provides highly efficient template-matching implementation and abstract data structures for storing and manipulating input and output data. it scales favorable to large datasets, both with multiple cpus and gpus, compared to existing software enabling template matching of even unbinned cryo-et data in hours, which was previously nearly impossible due to technical restraints. any hardware-specific optimization needed for dealing with large data is automatically performed to increase ease of use and minimize user intervention. the efficiency and simplicity of pytme will enable high throughput mining of a variety of cryo-em and et datasets in the future."
https://doi.org/10.1101/2024.01.08.574189,2024-01-26,machine learning-based classification of transcriptome signatures of non-ulcerative bladder pain syndrome,"['Akshay, A.; Besic, M.; Kuhn, A.; Burkhard, F. C.; Bigger-Allen, A. A.; Adam, R.; Monastyrskaya, K.; Hashemi Gheinani, A.']","lower urinary tract dysfunction (lutd) presents a global health challenge with symptoms impacting a substantial percentage of the population. the absence of reliable biomarkers complicates the accurate classification of lutd subtypes with shared symptoms such as non- ulcerative bladder pain syndrome (bps) and overactive bladder caused by bladder outlet obstruction with detrusor overactivity (do). this study introduces a machine learning (ml)- based approach for the identification of mrna signatures specific to non-ulcerative bps.  using next-generation sequencing (ngs) transcriptome data from bladder biopsies of patients with bps, benign prostatic obstruction with do and controls, our statistical approach successfully identified 13 candidate genes capable of discerning bps from control and do patients. this set was subsequently validated using quantitative polymerase chain reaction (qpcr) in a larger patient cohort. to confirm our findings, we applied both supervised and unsupervised ml approaches to the qpcr dataset. notably, a three-mrna signature tppp3, fat1, and ncald, emerged as a robust classifier, effectively distinguishing patients with non- ulcerative bps from controls and patients with do. this signature was universally selected by both supervised and unsupervised approaches.  the ml-based framework used to define bps classifiers not only establishes a solid foundation for comprehending the specific gene expression changes in the bladder of the patients with bps but also serves as a valuable resource and methodology for advancing signature identification in other fields. the proposed ml pipeline demonstrates its efficacy in handling challenges associated with limited sample sizes, offering a promising avenue for applications in similar domains."
https://doi.org/10.1101/2023.10.19.563033,2024-01-14,clusterv-web: a user-friendly tool for profiling hiv quasispecies and generating drug resistance reports from nanopore long-read data,"['Su, J.; Zheng, Z.; Li, S.; Lam, T.-W.; Luo, R.']","summarythird-generation long-read sequencing is an increasingly utilized technique for profiling hiv quasispecies and detecting drug resistance mutations due to its ability to cover the entire viral genome in individual reads. recently, the clusterv tool has demonstrated accurate detection of hiv quasispecies from nanopore long-read sequencing data. however, the need for scripting skills and a computational environment may act as a barrier for many potential users. to address this issue, we have introduced clusterv-web, a user-friendly web-based application that enables easy configuration and execution of clusterv, both remotely and locally. our tool provides interactive tables and data visualizations to aid in the interpretation of results. this development is expected to democratize access to long-read sequencing data analysis, enabling a wider range of researchers and clinicians to efficiently profile hiv quasispecies and detect drug resistance mutations.  availability and implementationclusterv-web is freely available and open source, with detailed documentation accessible at http://www.bio8.cs.hku.hk/clustervw/. the standalone docker image and source code are also available at https://github.com/hku-bal/clusterv-web.  contactrbluo@cs.hku.hk, department of computer science, the university of hong kong, hong kong, china; jhsu@cs.hku.hk, department of computer science, the university of hong kong, hong kong, china  supplementary informationnone"
https://doi.org/10.1101/2023.10.01.560379,2023-12-29,bbq methods: streamlined workflows for bacterial burden quantification in infected cells by confocal microscopy,"['Augenstreich, J.; Shuster, M. J.; Lyu, Z.; Fan, Y.; Ling, J.; Briken, V.']","accurate quantification of bacterial burden within macrophages, termed bacterial burden quantification (bbq), is crucial for understanding host-pathogen interactions. various methods have been employed, each with strengths and weaknesses. this article addresses limitations in existing techniques and introduces two novel automated methods for bbq within macrophages based on confocal microscopy data analysis. the first method refines total fluorescence quantification by incorporating filtering steps to exclude uninfected cells, while the second method calculates total bacterial volume per cell to mitigate potential biases in fluorescence-based readouts. these workflows utilize pyimagej and cellpose software, providing reliable, unbiased, and rapid quantification of bacterial load. the proposed workflows were validated using salmonella enterica serovar typhimurium and mycobacterium tuberculosis models, demonstrating their effectiveness in accurately assessing bacterial burden. these automated workflows offer valuable tools for studying bacterial interactions within host cells and provide insights for various research applications."
https://doi.org/10.1101/2023.06.20.545093,2024-01-03,predicting the impact of rare variants on rna splicing in cagi6,"['Lord, J.; Jaramillo Oquendo, C.; Wai, H. A.; Douglas, A. G. L.; Bunyan, D. J.; Wang, Y.; Hu, Z.; Zeng, Z.; Danis, D.; Katsonis, P.; Williams, A.; Lichtarge, O.; Chang, Y.; Bagnall, R. D.; Mount, S. M.; Matthiasardottir, B.; Lin, C.; van Overeem Hansen, T.; Leman, R.; Martins, A.; Houdayer, C.; Krieger, S.; Bakolitsa, C.; Peng, Y.; Kamandula, A.; Radivojac, P.; Baralle, D.']","backgroundvariants which disrupt splicing are a frequent cause of rare disease that have been under-ascertained clinically. accurate and efficient methods to predict a variants impact on splicing are needed to interpret the growing number of variants of unknown significance (vus) identified by exome and genome sequencing. here we present the results of the cagi6 splicing vus challenge, which invited predictions of the splicing impact of 56 variants ascertained clinically and functionally validated to determine splicing impact.  resultsthe performance of 12 prediction methods, along with spliceai and cadd, was compared on the 56 functionally validated variants. the maximum overall accuracy achieved was 82% from two different approaches, one weighting spliceai scores by minor allele frequency, and one applying the recently published splicing prediction pipeline (spip). spip performed optimally in terms of sensitivity, while an ensemble method combining multiple prediction tools and information from databases exceeded all others for specificity.  conclusionsseveral challenge methods equalled or exceeded the performance of spliceai, with ultimate choice of prediction method likely to depend on experimental or clinical aims. one quarter of the variants were incorrectly predicted by at least 50% of the methods, highlighting the need for further improvements to splicing prediction methods for successful clinical application."
https://doi.org/10.26434/chemrxiv-2024-fzrgp,2024-03-14,copddb: a descriptor database for copolymers and its application to the machine learning  ,Takayoshi Yoshimura Hiromoto  Kato Shunto Oikawa Taichi Inagaki Tetsunori Sugawara Tomonori Miyao Takamitsu Matsubara Hiroharu Ajiro Mikiya Fujii Yu-ya Ohnishi Miho Hatanaka,"polymer informatics, which involves the application of data-driven science to polymers, has attracted considerable interest. however, developing adequate descriptors for polymers, particularly copolymers, to facilitate machine learning models with limited datasets remains a challenge. to address this issue, we computed sets of parameters, including reaction energies and activation barriers of elementary reactions in the early stage of radical polymerization, for 2500 radical-monomer pairs derived from 50 commercially available monomers, and constructed an open database named “copolymer descriptor database (copddb).” furthermore, we built machine learning models using our descriptors as explanatory variables and physical properties such as the reactivity ratio, monomer conversion, monomer composition ratio, and molecular weight as objective variables. these models achieved high predictive accuracies, demonstrating the potential of our descriptors to advance the field of polymer informatics."
https://doi.org/10.26434/chemrxiv-2024-g9047-v2,2024-03-13,projected atomic orbitals as optimal virtual space for excited state projection-based embedding calculations,Ádám B. Szirmai Bence Hégely Attila Tajti Mihály Kállay Péter G. Szalay,"the projected atomic orbitals (pao) technique is presented for the construction of virtual orbital spaces in projection-based embedding (pbe) applications. the proposed straightforward procedure produces a set of virtual orbitals, which is used in the final, high-level calculation of the embedded active subsystem. the pao scheme is demonstrated on intermolecular potentials of bimolecular complexes, in ground and excited states, including rydberg excitations. the results show the outstanding performance of the pbe method when used with pao virtual orbitals compared to those produced using common orbital localization techniques. the good agreement of the resulting pbe potential curves with those from high-level \ai dimer calculations, also in diffuse basis sets, confirms that the pao technique can be suggested for future applications using top-down embedding methods. "
https://doi.org/10.26434/chemrxiv-2022-zq8x7-v2,2024-03-13,predicting dna reactions with a quantum chemistry-based deep learning model,Likun Wang Na Li Mengyao Cao Yun Zhu Xiewei Xiong Li Li Tong Zhu Hao Pei,"dna reactions are crucial in biology, synthetic biology, and dna computing. accurate prediction of thermodynamic and kinetic parameters is vital for understanding molecular interactions and designing functional dna-based systems. existing models have limitations due to simplifications and approximations that may deviate from experimental measurements. in this study, we propose a quantum chemistry-based deep learning model to enhance accuracy and efficiency in predicting dna reaction parameters. the model integrates quantum chemistry calculations, new designed descriptor matrices, and deep learning algorithms. it comprehensively describes energy variations by expanding stacks and considering relevant factors. to address limited labeled data, an active learning method selects informative samples iteratively, optimizing data utilization. the results demonstrate the superior predictive capabilities of our model in accurately determining dna hybridization free energies and strand displacement rate constants. this integration of quantum chemistry and deep learning improves our understanding of dna reactions and facilitates precise design and optimization of dna-based systems."
https://doi.org/10.26434/chemrxiv-2024-nbx35,2024-03-13,synthetic route design & assessment using vectors derived from similarity and complexity,Gareth Howell Samuel Genheden,"with the aim of improving the machine-interpretation of synthetic routes we describe a new theoretical approach to visualising and assessing synthetic pathways in the absence of empirical data such as yield, cost and waste. the representation of molecular structures as coordinates derived from molecular (fingerprint) similarity and complexity allows individual transformations to be viewed as vectors (reactant to product) whereby the magnitude and direction of travel can be used to assess and quantify transformation efficiency. vectors derived in this way are shown to follow logical trends when grouped by reaction type/class. synthetic routes can thus be visualised as a series of head-to-tail vectors (one per transformation or step) traversing the range between starting material and target whereby the efficiency with which this range is covered can be quantified. our approach is built upon the analysis of > 350k literature syntheses (> 1.4m reactions), is readily machine-interpretable and avoids the challenges associated with automated reaction class assignment and atom-mapping.       "
https://doi.org/10.26434/chemrxiv-2024-zz50n,2024-03-13,receptor for specific detection of tkx 50 and integration of the switch on fluorescence responses into an iot-based smart device,Somnath Bej Sourav Dutta Sheik  Salim Pasha Anik  K. De Debmalya Roy Lopamudra Roy Ria Ghosh Amrita  Banerjee Nivedita Pan Samir  Kumar Pal Sumit  K. Pramanik Amitava Das,"tkx-50 is one of the new generatation secondary explosives that has been introduced recently in the modern weapon system due to its low impact/friction sensitivity, and high thermal stability, density, and detonation velocity. in response to the growing concerns for the illegitimate use of certain explosives by various organized agencies, there is an urgent need for effective recognition and quantification of such explosives for efficient security screening, homeland security and human safety. however, it is imperative that such detection process is free from scope of any false positive or false negative response. herein, we report a new polymeric receptor derived from hydroxyl-functionalized tetra phenyl ethylene (tpe-oh) for specific detection of tkx 50 by monitoring its fluorescence-on responses on binding to tkx 50. a details spectroscopic (steady-state and time resolved fluorescence) studies confirm that the fluorescence ‘on’ response is attributed to an interrupted non-radiative deactivation process for tkx-50 bound receptor iv (ka = 2.4 x 103 m-1), which otherwise is operation in receptor iv. a control monomeric derivative of tpe-oh is also synthesized and is used for our studies to further corroborate our proposition. furthermore, a smart and user-friendly iot-based device has been developed, allowing the integration of optical responses into a digital output. this light weight portable device enables iot based remote detection and appropriate for surveillance application. to the best of our knowledge, example of such smart integration of the optical responses to an user datagram protocol (udp)-based wi-fi communication and data processing for remote monitoring of tkx 50 is scarce and in the contemporary literature."
https://doi.org/10.26434/chemrxiv-2024-kld98,2024-03-13,scalable synthesis of tio2@irox core-shell catalyst for proton exchange membrane water electrolysis with low iridium loading,Darius Hoffmeister Selina Finger Lena Fiedler Tien-Ching Ma Andreas Körner Matej Zlatar Birk Fritsch Kerstin Witte-Bodnar Simon Carl Alexander Götz Benjamin Apeleo Zubiri Johannes Will Erdmann Spiecker Serhiy Cherevko Anna T. S. Freiberg Karl J. J. Mayrhofer Simon Thiele Andreas Hutzler Chuyen Van Pham,"the widespread application of green hydrogen production technologies requires cost reduction of crucial elements. to achieve this, a viable pathway to reduce the iridium loading in proton exchange membrane water electrolysis (pemwe) is explored. herein, we present a scalable synthesis method based on a photodeposition process for a tio2@irox core-shell catalyst with a reduced iridium content as low as 40 wt%. using this synthesis route, we obtain titania support particles homogeneously coated with a thin iridium oxide shell of only 2.1 ± 0.4 nm. the catalyst exhibits not only high ex situ activity, but also decent stability compared to commercially available catalysts. furthermore, the unique core-shell structure provides a threefold increased electrical powder conductivity compared to structures without the shell. in addition, the low iridium content facilitates the fabrication of sufficiently thick catalyst layers at decreased iridium loadings mitigating the impact of crack formation in the catalyst layer during pemwe operation. we demonstrate that the novel tio2@irox core-shell catalyst clearly outperforms the commercial reference in single-cell tests with an iridium loading below 0.3 mgir cm 2 exhibiting a superior iridium-specific power density of 17.9 kw gir-1 compared to 10.4 kw gir-1 for the commercial reference."
https://doi.org/10.26434/chemrxiv-2024-58tt1-v2,2024-03-13,efficient acceleration of the convergence of the minimum free energy path via path-planning generated initial guess,Yi Sun,"we demonstrate how the combination of a shifted clustering algorithm and a fast-marching- based algorithm is able to generate good approximations of the minimum free energy path (mfep) if a free energy landscape (fel) is given. then, we show that using this kind of approximation as the mfep’s first guess and the string method for further refinement (also called the fmt-string combined approach) cuts down on the number of iterations needed for the mfep to converge by a large amount. this saves a lot of time compared to using a linear interpolation as the first guess. such a method provides an efficient alternative to the growing string method for obtaining a good initial guess of the mfep. "
https://doi.org/10.26434/chemrxiv-2024-lcm83,2024-03-12,augmenting genetic algorithms with machine learning for inverse molecular design,Hannes Kneiding David Balcells,"evolutionary and machine learning methods have been successfully applied to the generation of molecules and materials exhibiting desired properties. the combination of these two paradigms in inverse design tasks can yield powerful methods that explore massive chemical spaces more efficiently, improving the quality of the generated compounds. however, such synergistic approaches are still an incipient area of research and appear underexplored in the literature. this review covers different ways of incorporating machine learning approaches into evolutionary learning frameworks, with the overall goal of increasing the optimization efficiency of genetic algorithms. in particular, machine learning surrogate models for faster fitness function evaluation, discriminator models to control population diversity on-the-fly, machine learning based crossover operations, and evolution in latent space are discussed. the further potential of these synergistic approaches in generative tasks is also assessed, outlining promising directions for future developments."
https://doi.org/10.26434/chemrxiv-2024-r9ljm,2024-03-12,evaluation of reinforcement learning in transformer-based molecular design,Jiazhen He Alessandro Tibo Jon Paul Janet Eva Nittinger Christian Tyrchan Werngard Czechtizky Ola Engkvist,"designing compounds with a range of desirable properties is a fundamental challenge in drug discovery. in pre-clinical early drug discovery, novel compounds are often designed based on an already existing promising starting compound thorough structural modifications for further property optimization. recently, transformer-based deep learning models have been explored for the task of molecular optimization by training on pairs of similar molecules. this provides a starting point for generating similar molecules to a given input molecule, but has limited flexibility regarding user-defined property profiles. here, we evaluate the effect of reinforcement learning on transformer-based molecular generative models. the generative model can be considered as a pre-trained model with knowledge of the chemical space close to an input compound, while reinforcement learning can be viewed as a tuning phase, steering the model towards chemical space with user-specific desirable properties. the evaluation of two distinct tasks - molecular optimization and scaffold hopping - suggest that reinforcement learning could guide the transformer-based generative model towards the generation of more compounds of interest. additionally, the impact of pre-trained models, learning steps and learning rates are investigated."
https://doi.org/10.26434/chemrxiv-2024-8bxs3,2024-03-12,end-to-end high-throughput approach for data-driven internal donor development in heterogeneous ziegler-natta propylene polymerization,Toshiaki Taniike Felicia Daniela Cannavacciuolo Mostafa Khoshsefat Diego  De Canditiis Giuseppe Antinucci Patchanee Chammingkwan Roberta Cipullo Vincenzo Busico,"internal donors (ids) play a decisive role in shaping the structure and performance of ziegler-natta catalyst formulations for the isotactic polypropylene production. unfortunately, their diverse and intricate functions remain elusive, and rational id discovery therefore is still problematic. exploitation of artificial intelligence methods such as machine learning, in turn, has been hindered by the lack of training datasets with adequate quality and size. this study proposes an integrated high-throughput workflow encompassing catalyst synthesis, propylene polymerization, and polypropylene characterization. its application to an id library of 35 molecules generated a robust and consistent dataset, which highlighted important and intriguing quantitative structure-properties relations (qsprs). furthermore, by fingerprinting id molecular structure in combination with feature selection, a black-box qspr model correlating id molecular structure and catalytic performance was successfully implemented. this study demonstrates that the combination of high-throughput experimentation and machine learning is a promising asset for accelerating the research and development of ziegler-natta catalysts."
https://doi.org/10.26434/chemrxiv-2024-k06gb-v2,2024-03-11,"lsm1-ms2: a self-supervised foundation model for tandem mass spectrometry applications, encompassing extensive chemical property predictions and spectral matching",Gabriel Asher Jennifer M. Campbell Jack Geremia Timothy Kassis,"we introduce lsm1-ms2, a self-supervised foundation model pre-trained on extensive unlabeled ms2 data for tandem mass spectrometry. lsm1-ms2 leverages the complex data within ms2 spectra, bypassing the need for detailed identification and efficiently processing traditional ms2 data. the model's architecture, based on a specialized transformer and custom tokenization scheme, enables masked peak modeling, which trains the model to reconstruct ms2 peaks, preparing it for various downstream applications. fine-tuning lsm1-ms2 with a smaller, labeled dataset focuses on compound property prediction and identification through spectral matching, enhancing its practicality. preliminary results demonstrate that the model predicts 209 rdkit-extracted descriptors with a mean absolute error (mae) of 3.35 on the casmi 2022 dataset, outperforming recent supervised models. notably, it requires only 1% of the labeled data used by traditional methods. lsm1-ms2's rich embeddings yield similar results to supervised models even without fine-tuning. in compound identification, lsm1-ms2 excels in database lookups, achieving a 0.07 mae in tanimoto similarity measurements on casmi 2022, surpassing supervised models and traditional methods. it also retrieves close matches for known and unknown molecule queries more efficiently than conventional approaches. overall, lsm1-ms2 demonstrates a transformative impact on mass spectrometry analysis, offering effective fine-tuning with minimal data and robust predictive abilities, reshaping traditional approaches in ms2 data utilization. "
https://doi.org/10.26434/chemrxiv-2024-787rx,2024-03-11,ddx: polarizable continuum solvation from small molecules to proteins,Michele Nottoli Michael F. Herbst Aleksandr Mikhalev Abhinav Jha Filippo Lipparini Benjamin Stamm,"polarizable continuum solvation models are popular in both, quantum chemistry and in biophysics, though typically with different requirements for the numerical methods. however, the recent trend of multiscale modeling can be expected to blur field-specific differences. in this regard, numerical methods based on domain decomposition (dd) have been demonstrated to be sufficiently flexible to be applied all across these levels of theory while remaining systematically accurate and efficient. in this contribution, we present ddx, an open-source implementation of dd-methods for various solvation models, which features a uniform interface with classical as well as quantum descriptions of the solute, or any hybrid versions thereof. we explain the key concepts of the library design and its api, and demonstrate the use of ddx for integrating into standard chemistry packages. numerical tests illustrate the performance of ddx and its interfaces."
https://doi.org/10.26434/chemrxiv-2024-zphc4,2024-03-11,hemithioindigo-based histone deacetylase inhibitors induce a light-dependent anticancer effect,Laia Josa-Culleré Carla Aira Rodríguez Amadeu Llebaria,"photoswitchable molecules exhibit light-dependent biological activity which allow us to control the therapeutic effect of drugs with high precision. such molecules could solve some of the limitations of anticancer drugs by providing a localised effect in the tumour. histone deacetylase inhibitors (hdacis) constitute a promising drug class for oncology whose application is often limited by a lack of selectivity. herein, we developed photoswitchable hdacis based on a hemithioindigo scaffold. we established synthetic routes to access them and determined the optimal conditions for isomerisation and their thermal stability. we then optimised their enzyme activity through three rounds of re-design to identify examples that are up to 6-fold more active under illumination than in the dark. we also confirmed that our best derivative reduces the viability of hela cells only under illumination. all in all, we disclose a series of derivatives containing an hemithioindigo moiety, which display a light-dependent effect on both hdac inhibition and cancer cell viability."
https://doi.org/10.26434/chemrxiv-2023-9dvlm-v3,2024-03-11,an electrochemical series for materials,Tim Mueller Joseph Montoya Weike Ye Xiangyun Lei Linda Hung Jens Hummelshøj Michael Puzon Daniel Martinez Chris Fajardo Rachel Abela,"the electrochemical series is a useful tool in electrochemistry, but its effectiveness in materials chemistry is limited by the fact that the standard electrochemical series is based on a relatively small set of reactions, many of which are measured in aqueous solutions. to address this problem, we have used machine learning to create an electrochemical series for inorganic materials from tens of thousands of entries in the inorganic crystal structure database. we demonstrate that this series is generally more consistent with oxidation states in solid-state materials than the series based on aqueous ions. the electrochemical series was constructed by developing and parameterizing a physical, human-interpretable model of oxidation states in materials.  we show that this model enables the prediction of oxidation states from composition in a way that is more accurate than a state-of-the-art transformer-based neural network model. we present applications of our approach to structure prediction, materials discovery, and materials electrochemistry, and we discuss possible additional applications and areas for improvement.  to facilitate the use of our approach, we introduce a freely available web site and api."
https://doi.org/10.26434/chemrxiv-2023-98n6q-v2,2024-03-11,reproducible mass spectrometry data processing and compound annotation in mzmine 3,Tito Damiani Steffen Heuckeroth Aleksandr Smirnov Olena Mokshyna Corinna Brungs Ansgar Korf Joshua Smith Paolo Stincone Nicola Dreolin Louis-Félix Nothias Tuulia Hyötyläinen Matej Orešič Uwe Karst Pieter Dorrestein Daniel Petras Xiuxia Du Justin van der Hooft Robin Schmid Tomáš Pluskal,"untargeted ms experiments produce complex, multi-dimensional data that are practically impossible to investigate manually. for this reason, computational pipelines are needed to extract relevant information from raw spectral data and convert it into a more comprehensible format. based on the sample type and/or goal of the study, a variety of ms platforms can be used for such analysis. mzmine is open-source software for the processing of raw spectral data generated by different ms platforms: liquid chromatography–ms (lc–ms), gas chromatography–ms (gc–ms), and ms–imaging. moreover, the third version of the software, described herein, supports the processing of ion mobility spectrometry (ims) data. the present protocol provides three distinct procedures to perform feature detection and annotation of untargeted ms data produced by different instrumental setups: lc–(ims–)ms, gc–ms, and (ims–)ms imaging. for training purposes, example datasets are provided together with configuration batch files (i.e. list of processing steps and parameters) to allow new users to easily replicate the described workflows. depending on the number of data files and available computing resources, we anticipate this to take between 2 and 24 hours for new mzmine users and non-experts. within each procedure, we provide a detailed description for all processing parameters together with instructions/recommendations for their optimization. the main generated outputs are represented by aligned feature tables and fragmentation spectra lists that can be used by other third-party tools for further downstream analysis. "
https://doi.org/10.26434/chemrxiv-2024-ssr6z,2024-03-08,machine learning of 27al nmr quadrupolar tensors for crystalline structures from dft ,He Sun Shyam  Dwaraknath Michael E.  West Handong Ling Kristin  Persson Sophia Hayes,"nmr crystallography has emerged as promising technique for the determination and refinement of crystal structures. the crystal structure of compounds containing quadrupolar nuclei, such as 27al, can be improved by directly comparing solid-state nmr measurements to dft computations of the electric field gradient (efg). the high computational cost of these first-principles calculations limits the applicability of this method to all but the most well-defined structures. we developed a fast, low-cost machine learning model to predict efg parameters based on local structural motifs and elemental parameters. we computed 8081 efg tensors using dft and benchmarked them against 105 experimental parameters. surprisingly, simple local geometric features dominate the predictive performance of the resulting random-forest model, yielding an r2 value of 0.98 and an rmse of 0.61 mhz. this model accuracy should enable pre-refining future structural assignments before finally validating with first-principles calculations. such a catalogue of 27al nmr tensors can serve as a tool for researchers assigning complex quadrupolar spectra."
https://doi.org/10.26434/chemrxiv-2024-8489b,2024-03-08,efficient and accurate pka prediction enabled by pre-trained machine-learned interatomic potentials,Corin Wagen Arien Wagen,"quickly and accurately predicting the pka of small molecules is an important unsolved challenge in computational chemistry: while approaches based on electronic structure theory have shown great promise, the utility of these methods is limited by the considerable expense of the requisite computations. in this study, we investigate aimnet2, a machine-learned interatomic potential, as a low-cost surrogate for electronic structure theory in pka prediction. the accuracy of the aimnet2-based pka prediction workflow is evaluated over a wide range of compounds and functional groups, and potential sources of error are discussed. "
https://doi.org/10.26434/chemrxiv-2024-ln7tx,2024-03-08,computation-efficient approach to eis feature extraction for battery informatics and big data,Xueying Quinn Daniel W. Shaw Yujia Liang Zachary Jeannotte Guesang Lee David Shock,"electrochemical impedance spectroscopy (eis) has the potential for improved prediction of battery performance and lifespan, but often has costly computation requirements. current soc/soh prediction methods rely on data-driven or model-based matrix approaches. in advancing towards eis's big data applications, we propose an efficient and unambiguous curve feature extraction method, surpassing traditional ecm fitting."
https://doi.org/10.26434/chemrxiv-2024-th0d9,2024-03-08,a renormalized doubly hybrid method enhanced with machine learning for a unified treatment of static and dynamic correlations,Yizhen Wang Zihan Lin Runhai Ouyang Bin Jiang Igor Ying Zhang Xin Xu,"the accurate description of the static correlation poses a persistent challenge for electronic structure theory, particularly when it has to be concurrently considered with the dynamic correlation. we develop here a renormalized xyg3-type doubly hybrid method, named r-xdh7, which effectively captures a large portion of the static correlation alongside a broad range of the dynamic correlation, showing a marked improvement over contemporary state-of-the-art doubly hybrid (dh) density functional theory methods. by utilizing a hybrid machine learning algorithm that synergistically combines symbolic and nonlinear parameter regression, we further devise a general-purpose model for the static correlation correction (scc) specifically adapted to r-xdh7. the resultant r-xdh7-scc15 method achieves an unprecedented accuracy in capturing the static correlation, while maintaining a good description of the dynamic correlation on par with the best dh approximations. extensive benchmark studies validate the robustness and transferability of r-xdh7-scc15 across diverse chemical systems. notably, it displays exceptional aptitude in providing precise characterizations of complex reaction mechanisms beyond the equilibrium regions where static correlation effects are significant. these findings signify a substantial enhancement in the predictive power of computational chemistry, marking a significant stride in the field of electronic-structure theory."
https://doi.org/10.26434/chemrxiv-2024-lx8dn,2024-03-08,using dynamic interaction fingerprints to derive baseline machine learning model for the prediction of protein-ligand dissociation rate constant,Muhammad Jan  Akhunzada Hyun Jung  Yoon Abdennour  Braka Indrajit  Deb Sangwook Wu,"model building for the prediction of protein-ligand unbinding kinetics gaining popularity with the increasing availability of experimental structural data for the protein-ligand complexes and their relevant kinetic parameters. limited but major effort has been already put forward in choosing appropriate machine learning (ml) methods among the popular ones like least squares (ls), support vector machine (svm), random forest (rf), and a few more. the rf and bayesian neural network (bnn) algorithms have been reported to be promising when combined with advanced descriptors representing ligand properties and protein-ligand interactions. however, the selection of descriptors that would correlate well with the unbinding kinetic properties is still a challenge. in this work, we derived a baseline rf model using descriptors representing the protein-ligand interaction fingerprints (ifps) along the ligand unbinding pathway otherwise can be called dynamic ifps. we found that the dynamic ifps in addition to the static or binding pocket ifps significantly improved the quality of our model for the prediction of ligand dissociation rate constant (koff). to the best of our knowledge, this work is the first attempt towards using the dynamic ifps in deriving a quantitative structure-kinetics relationship (qskr) model for the prediction of koff."
https://doi.org/10.26434/chemrxiv-2024-808lg,2024-03-08,“quantum-chemoinformatics” for design and discovery of new molecules and reactions,Hiroko Satoh Vincenz-Maria Steiner Jürg Hutter,"we give an overview of the role of “quantum-chemoinformatics” in drug development. quantum- chemoinformatics is a data-driven chemistry using descriptors on the basis of theoretical chemistry, especially quantum chemistry (qc) and ab initio molecular dynamics (md) simulations. we focus especially on quantum-chemoinformatics for chemical reaction design and prediction, which is one of the important processes in basic research of drug development. we start with a brief historical overview and then introduces two projects of quantum-cheminformatics. the rmap project uses qc-based chemical reaction route networks for discovery and design of new molecules and reactions. the other project is related to environmental pollution by drug molecules, a property which should be taken into account in drug design and evaluation. the last section describes our recent attempt to accelerate qc-data acquisition by utilizing a limited amount of experimental data and machine learning (ml) technology. "
https://doi.org/10.26434/chemrxiv-2024-9qv8w,2024-03-08,active learning of alchemical adsorption simulations; towards a universal adsorption model.,Etinosa Osaro Fernando Fajardo-Rojas Gregory Cooper Diego Gómez-Gualdrón Yamil Colón,"adsorption is a fundamental process studied in materials science and engineering because it plays a critical role in various applications, including gas storage and separation. understanding and predicting gas adsorption within porous materials demands comprehensive computational simulations that are often resource intensive, limiting the identification of promising materials. active learning (al) methods offer an effective strategy to reduce the computational burden by selectively acquiring critical data for model training. metal-organic frameworks (mofs) exhibit immense potential across various adsorption applications due to their porous structure and their modular nature, leading to diverse pore sizes and chemistry that serve as an ideal platform to develop adsorption models. here, we demonstrate the efficacy of al in predicting gas adsorption within mofs using “alchemical” molecules and their interactions as surrogates for real molecules. we first applied al separately to each mof, reducing the training dataset size by 57.5% while retaining predictive accuracy. subsequently, we amalgamated the refined datasets across 1800 mofs to train a multilayer perceptron (mlp) model, successfully predicting adsorption of real molecules. furthermore, by integrating mof features into the al framework using principal component analysis (pca), we navigated mof space effectively, achieving high predictive accuracy with only a subset of mofs. our results highlight al's efficiency in reducing dataset size, enhancing model performance, and offering insights into adsorption phenomenon in large datasets of mofs. this study underscores al's crucial role in advancing computational material science and developing more accurate and less data intensive models for gas adsorption in porous materials."
https://doi.org/10.26434/chemrxiv-2024-k5v0s,2024-03-07,highly selective novel heme oxygenase-1-targeting molecules discovered by dna-encoded library-machine learning model beyond the del chemical space,Shuai  Han Xinyun Guo Min Wang Huan Liu Yidan  Song Yunyun  He Kuang-Lung Hsueh Weiren  Cui Wenji  Su Letian Kuai Jason  Deng," dna-encoded library (del) technology has proven to be a powerful method for discovering novel inhibitors for diverse targets. particularly when combined with machine learning (ml), the del-ml workflow expands the chemical space and enhances cost-effectiveness, offering new opportunities to find desired hit molecules. heme oxygenase-1 (ho-1), primarily a heme-degrading enzyme, has been identified as a potential therapeutic target in diseases such as cancer and neurodegenerative disorders. despite years of study, the ho-1 inhibitor toolbox remains limited. here, we report the discovery of five series of novel scaffold ho-1 inhibitors using a del-ml workflow that emphasizes the model’s uncertainty quantification and its domain of applicability. the del-ml model demonstrated a strong ability to extrapolate to novel chemical spaces by identifying new structures. approximately 33% of the predicted molecules, validated by biophysical assays, had a binding affinity of k¬d < 15 µm, with the strongest affinity being 141 nm. fourteen tested molecules showed over 100-fold selectivity towards ho-1 over heme oxygenase-2 (ho-2). these molecules are also structurally novel compared to the reported ho-1 inhibitors. further, binding mode simulations via docking provided insights into the possible selectivity rationale of some selective series."
https://doi.org/10.26434/chemrxiv-2024-rz914,2024-03-07,gold-thiolate nanocluster dynamics and intercluster reactions enabled by a machine learned interatomic potential,Caitlin McCandler Antti Pihlajamäki Sami Malola Hannu Häkkinen Kristin Persson,"mono-layer protected metal clusters comprise a rich class of molecular systems, and are promising candidate materials for a variety of applications. while a growing number of protected nanoclusters have been synthe- sized and characterized in crystalline forms, their dynamical behavior in solution, including pre-nucleation cluster formation, is not well understood due to limitations both in characterization and first-principles mod- eling techniques. recent advancements in machine-learned interatomic potentials are rapidly enabling the study of complex interactions such as dynamical behavior and reactivity at the nanoscale. here, we develop an au-s-c-h atomic cluster expansion (ace) interatomic potential for efficient and accurate molecular dynamics simulations of thiolate-protected gold nanoclusters (aun (sch3)m ). trained on more than 30,000 density functional theory calculations of gold nanoclusters, the interatomic potential exhibits ab initio level accuracy in energies and forces, and replicates nanocluster dynamics including thermal vibration and chiral inversion. long dynamics simulations (up to 0.1 μs time scale) reveal a novel mechanism explaining the ther- mal instability of neutral au25(sr)18 clusters. specifically, we observe multiple stages of isomerization of the au25(sr)18 cluster, including a novel chiral isomer. additionally we simulate coalescence of two au25(sr)18 clusters and observe series of new clusters where the formation mechanisms are critically mediated by ligand exchange in the form of [au–s]n rings."
https://doi.org/10.26434/chemrxiv-2024-c375f,2024-03-07,training transferable interatomic neural network potentials for reactive chemistry: improved chemical space sampling ,Quin Hu Adrian Gordon Andrew Johanessen Leqian Tan Jason Goodpaster,"large, condensed phased, and extended systems remain a challenge for theoretical studies due to the compromise between accuracy and computational cost in their calculations.  machine learning methods are on the rise to solve this trade off by training on large datasets of highly accurate calculations that are traditionally hard to obtain.  the development of interatomic machine learning potentials has resulted in the ability to model high-quality potential energy surfaces with near ab initio level of accuracy at low computational cost.  however, just like other machine learning applications, such methods face challenges when it comes to quality training data and transferability, specifically to systems of chemical space beyond its training.  in this work, we present the continuous exploration of utilizing machine learning methods to build and achieve accurate and efficient potential energy surface for bond dissociation and reactive chemistry, and explore sampling techniques that can allow interatomic neural network potentials designed to model potential energy surfaces, such as ani and nequip, to accurately predict bond dissociation energy and model reactive chemistry, and to obtain transferability beyond its training data across chemical space.  "
https://doi.org/10.26434/chemrxiv-2024-10xzj-v2,2024-03-07,embedding physics into neural odes to learn kinetics from integral reactors,Tim Kircher Felix Döppel Martin Votsmeier,"while the digitalization of chemical research and industry is vastly increasing the amountof data for developing kinetic models, model parametrization is not keeping up. to take advantage of the full potential of this data, machine learning tools are required that autonomously learn kinetics from reactor data. previously, we introduced global reaction neural networks with embedded stoichiometry and thermodynamics for kinetic modelling. when trained as a neural ordinary differential equation (neural ode), they discovered kinetics from integral reactor measurements of an equilibrium limited steam reforming reactor whereas conventional neural odes failed. we now extend their application to another industrially relevant case of reactors operating at full conversion. using the preferential oxidation of co in h2 rich streams as an example, we show that the physics-embedded neural network discovers kinetics from stiff systems containing cases of both full conversion and equilibrium limitation using integral reactor data"
https://doi.org/10.26434/chemrxiv-2023-321vf-v4,2024-03-06,a story of three levels of sophistication in scf/ks-dft orbital optimization procedures,Daniel Sethio Emily Azzopardi Ignacio Fernández Galván Roland Lindh,"in this report, three versions of scf/ks-dft orbital optimization are described and benchmarked. the methods are a modified version of the geometry version of the direct inversion in the iterative subspace approach (which we call r-gdiis), the modified restricted step rational function optimization method (rs-rfo), and the novel subspace gradient enhanced kriging method, combined with restricted variance optimization (s-gek/rvo). the modifications introduced are aimed to improve the robustness and computational scaling of the procedures. in particular, the subspace approach in s-gek/rvo allows the application to scf/ks-dft optimization of a machine technique that has proved successful in geometry optimizations. the performance of the three methods is benchmarked for a large number of small to medium-sized organic molecules, at equilibrium structures and close to a transition state, and a second set of molecules containing closed- and open-shell transition metals. the results indicate the importance of the resetting technique in boosting the performance of the r-gdiis procedure. moreover, it is demonstrated that already at the inception of the subspace version of gek to optimize scf wave functions, it displays superior and robust convergence properties as compared to standard state-of-the-art scf/ks-dft optimization methods."
https://doi.org/10.26434/chemrxiv-2024-vgbxq,2024-03-06,actionable predictions of human pharmacokinetics at the drug design stage,Leonid Komissarov Nenad Manevski Katrin Groebke Zbinden Torsten Schindler Marinka Zitnik Lisa Sach-Peltason,"we present a novel computational approach for predicting human pharmacokinetics (pk) that addresses the challenges of early-stage drug design. our study introduces and describes a large-scale dataset of 11 clinical pk endpoints, encompassing over 2700 unique chemical structures to train machine learning models. to that end multiple advanced training strategies are compared, including the integration of in vitro data and a novel self-supervised pre-training task. in addition to the predictions, our final model provides meaningful epistemic uncertainties for every data point. this allows us to successfully identify regions of exceptional predictive performance, with an absolute average fold error (aafe/gmfe) of less than 2.5 across multiple endpoints. these advancements represent a significant leap towards actionable pk predictions, which can be utilized early on in the drug design process to expedite development and reduce reliance on nonclinical studies."
https://doi.org/10.26434/chemrxiv-2024-cq702,2024-03-06,deltagzip: computing biopolymer-ligand binding affinity via kolmogorov complexity and lossless compression,Tao Liu Lena Simine,"the design of bio-sequences for biosensing and therapeutics is a challenging multi-step search and optimization task. in principle, computational modeling may speed up the design process by virtual screening of sequences based on their binding affinities to target molecules. however, in practice, existing machine-learned models trained to predict binding affinities lack the flexibility with respect to reaction conditions, and molecular dynamics simulations that can incorporate reaction conditions suffer from high computational costs. here, we describe a computational approach called deltagzip that evaluates the free energy of binding in biopolymer-ligand complexes from ultra-short equilibrium molecular dynamics simulations. the entropy of binding is evaluated using the kolmogorov complexity definition of entropy and approximated using a lossless compression algorithm, gzip. we benchmark the method on a well-studied dataset of protein-ligand complexes comparing the predictions of deltagzip to the free energies of binding obtained using the jarzynski equality and experimental measurements. "
https://doi.org/10.26434/chemrxiv-2024-mhs6s,2024-03-06,"data-efficient, chemistry-aware machine learning predictions of diels–alder reaction outcomes",Angus Keto Taicheng Guo Morgan Underdue Thijs Stuyver Connor Coley Xiangliang Zhang Elizabeth Krenske Olaf Wiest,"the application of machine learning models to the prediction of reaction outcomes currently needs large and/or highly featurized datasets. we show that a chemistry-aware model, nerf, which mimics the bonding changes that occur during reactions, allows for highly accurate predictions of the outcomes of diels–alder reactions using a relatively small training set, with no pretraining and no additional features. we establish a diverse dataset of 9,537 intramolecular, hetero-, aromatic, and inverse electron demand diels–alder reactions. this dataset is used to train a nerf model and the performance is compared against state-of-the-art classification and generative machine learning models across low- and high-data regimes, with and without pretraining. the predictive accuracy (regio- and site selectivity in the major product) achieved by nerf exceeds 90% when as little as 40% of the dataset is used for training. another high-performing model, chemformer, requires a larger training dataset (>45%) and pretraining to reach 90% top-1 accuracy. accurate predictions of less-represented reaction subclasses, such as those involving heteroatomic or aromatic substrates, require higher percentages of training data. we also show how nerf can use small amounts of additional training data to quickly learn new systems and improve its overall understanding of reactivity. synthetic chemists stand to benefit as this model can be rapidly expanded and tailored to areas of chemistry corresponding to the low data regime."
https://doi.org/10.26434/chemrxiv-2024-bxqmn,2024-03-06,electrocatalytic co2 reduction on amorphous cu surfaces: unveiling structure-activity relationships,Akshayini  Muthuperiyanayagam  Devis Di Tommaso,"we present a computational investigation combining machine learning forcefields (ml-ff) and dft calculations into the potential of amorphous copper (cu) surfaces towards electrochemical co2 reduction (eco2r) to one-carbon (c1) and two-carbon (c2) products. the “on-the-fly” ml-ff developed for cu replicate dft energies and structures, offering a computationally efficient tool for simulating amorphous cu systems. these ml-ffs were used to generate atomistic amorphous models of bulk and surfaces, and the amorphous bulk cu exhibited slightly higher stability than crystalline cu. the amorphous cu surface provide a wider range of cu coordination sites (5-9) compared to crystalline cu, which offered a multitude of active centres for co2 adsorption. some of the amorphous surfaces investigated in this study spontaneously activated co2, evidenced by the stable chemisorption, highlighting their potential for efficient co2 conversion. the intermediates formed during the eco2r on amorphous cu surfaces are stabilized compared to crystalline surfaces, leading to a lower overpotentials, and improved faradaic efficiency. this study demonstrates for the first time theoretically, the potential of amorphous cu-based catalysts towards sustainable co2 conversion and paves the way for further research and development in this promising field."
https://doi.org/10.26434/chemrxiv-2024-7dw8z,2024-03-06,high throughput methodology for investigating green hydrogen generating processes using colorimetric detection films and machine vision,Savannah Talledo Andrew Kubaney Mitchell A.  Baumer Keegan Pietrak Stefan Bernhard,"the generation of hydrogen from abundant and renewable precursors driven by sunlight will be a cornerstone of a future, sustainable hydrogen infrastructure.  current methods to monitor the evolution of hydrogen in such photocatalytic systems such as gas chromatography, mass spectrometry, manometry or raman spectroscopy are either expensive and low throughput or lack sensitivity and selectivity over other gasses. this impediment hinders the generation of photo-driven hydrogen evolution data necessary for machine learning and artificial intelligence-based protocols. this work presents an open-source approach for studying solar-driven hydrogen evolution reactions (hers) in parallel that uses colorimetric hydrogen detection films in tandem with an image analysis software capable of providing metrics such as hydrogen amount, hydrogen evolution rates, incubation times, and plateau times, and more.  the sensing medium is composed of 0.05 % (w/w) pt impregnated molybdenum (vi) oxide or tungsten (vi) oxide which was incorporated into poly(vinyl alcohol) films placed under clear, gas impermeable septa. to conduct experiments, users require only blue reaction-driving high intensity leds, a camera, and uniform lighting to take pictures as the septa darken. this work introduces a sample configuration in which nine samples in hydrogen sensitive septa-capped vials were illuminated and the gas evolution is monitored using a raspberrypi for image capture and storage. two calibration methods are presented, one uses a gravimetric hydrogen evolution with zn/hcl that is compared to a direct hydrogen injection. both methods allow the accurate correlation of normalized intensity values of film photographs to mole fractions of h2 ranging from 0 to 50%. four light-driven hers are described that highlight the capabilities of the detection method, two of which were conducted using the novel septa-based instrumentation while the other two experiments used the films on a 108 multiwell plate using a previously discussed photoreactor. "
https://doi.org/10.26434/chemrxiv-2024-6srm4,2024-03-05,low-cost hourly ambient black carbon measurements at multiple cities in africa,Abhishek Anand N'Datchoh Evelyne Touré Julien Bahino Sylvain Gnamien Allison Felix Hughes Raphael E Arku Victoria Owusu Tawiah Araya Asfaw Tesfaye Mamo Sina Hasheminassab Solomon Bililign Daniel M Westervelt Albert A Presto,"there is a notable lack of continuous monitoring of air pollutants in the global south, especially for measuring chemical composition, due to high cost of regulatory monitors. using our previously developed low-cost method to quantify black carbon (bc) in fine particulate matter (pm2.5) by analyzing reflected red light from ambient particle deposits on glass fiber filters, we estimated hourly ambient bc concentrations with filter tapes from beta attenuation monitors (bams). bc measurements obtained through this method were validated against a reference aethalometer in addis ababa, ethiopia, demonstrating a very strong agreement (r2 = 0.95 and slope = 0.97). we present hourly bc for three cities in sub-saharan africa (ssa) and one in north america: abidjan (côte d’ivoire), accra (ghana), addis ababa (ethiopia), and pittsburgh (usa). the average bc for the measurement period at the abidjan, accra, addis ababa central summer, addis ababa central winter, addis ababa jacros winter and pittsburgh sites were 3.85 µg.m-3, 5.33 µg.m-3, 5.63 µg.m-3, 3.89 µg.m-3, 9.14 µg.m-3 and 0.52 µg.m-3, respectively. bc made up 14 – 20% of pm2.5 mass in the african cities, compared to only 5.6% in pittsburgh. the hourly bc data at these sites show a pronounced diurnal pattern with prominent peaks during the morning and evening rush hours on workdays. comparison between our measurements and the goddard earth observing system composition forecast (geos-cf) estimates shows that the model performs well in predicting pm2.5 for most sites but struggles to predict bc at an hourly resolution. adding more ground measurements could help calibrate and improve performance of chemical transport models. this method can potentially use existing bam networks, especially bams at us embassies around the globe, to measure hourly bc with our method. the pm2.5 composition data, thus acquired, can be crucial in identifying emission sources and help in effective policymaking in the global south."
https://doi.org/10.26434/chemrxiv-2024-vpw8x,2024-03-05,efficient parahydrogen induced 13c hyperpolarization on a microfluidic device,Sylwia Barker Laurynas Dagys Malcolm Levitt Marcel Utz,"in this work we demonstrate the direct production and detection of carbon-hyperpolarized fumarate by parahydrogen-induced polarization (phip) in a microfluidic lab-on-a-chip (loc) device and achieve 8.5% 13c polarization. this is the first demonstration of 13c-hyperpolarization of a metabolite by phip in a microfluidic device. loc technology allows the culture of mammalian cells in a highly controlled environment, providing an important tool for the life sciences. in-situ preparation of hyperpolarized metabolites greatly enhances the ability to quantify metabolic processes in such systems by microfluidic nmr.  phip of 1h nuclei has been successfully implemented in microfluidic systems, with mass sensitivities in the range of pmol/√s but only with low yields. metabolic nmr requires high-yield production of hyperpolarized metabolites with longer spin life times than is possible with1h. this can be achieved by transfer of the polarization onto 13c nuclei, which exhibit much longer t1 relaxation times.  we report an improved microfluidic phip device that has been optimised using a finite element model that achieves a more than 20-fold improvement in the yield of the hyperpolarized product. it is shown that this enables detailed kinetic studies of the hydrogenation of propargyl acetate to allyl acetate, as well as the direct production of 13c hyperpolarized fumarate."
https://doi.org/10.26434/chemrxiv-2024-1dlt9,2024-03-05,machine-learning discovered crystallization model for two-dimensional covalent organic frameworks: towards precise control of the crystal quality,Jiaxin Tian Haoyuan Li,"the rational molecular design and experimental condition optimizations for two-dimensional covalent organic frameworks (2d cofs) call for a crystallization model capable of capturing experimental time and size scales. however, accurately describing their crystallization process remains a significant challenge due to the presence of non-classical pathways. here, we demon-strate the implementation of a machine-learning approach, overcoming the difficulties associ-ated with bottom-up model derivation. the resulting model, referred to as negen1, establishes correlations among the induction time, nucleation rate, growth rate, material parameters, and common solution synthesis conditions for 2d cofs that belong to the nucleation-elongation category. negen1 represents the emergence of practical crystallization models for 2d cofs, enabling the direct calculation of their crystallization processes in both experimental times and sizes. the results elucidate the detailed competition between the nucleation and growth dynam-ics in solution, which has been inappropriately apprehended via classical, empirical models with assumptions invalid for 2d cofs. importantly, we demonstrate the potential application of the negen1 model in optimizing the synthesis conditions, which has predominantly relied on empirical knowledge to date. the identification of conditions superior to those routinely used experimentally reveals a promising strategy of gradually increasing monomer addition speed for growing large 2d cof crystals while maintaining a reasonable synthesis time. these results highlight the potential for systematically improving the crystal quality of 2d cofs for wider applications."
https://doi.org/10.26434/chemrxiv-2024-wr3kj,2024-03-05,streamlining lateral flow assay (lfa) data analysis: a matlab® live script for integrated batch image processing and limit of detection determination,Karan Saxena Bhushan Toley,"lateral flow assays (lfas) are widely utilized for rapid point-of-care diagnostics in various fields; however, their quantitative image analysis is frequently hindered by repetitive manual processes and inconsistencies, typically involving the use of combinations of image processing and data analysis software tools. this protocol introduces a matlab live-script-based code for the integrated batch processing of lfa images and standard curve development. this code simplifies quantitative lfa analysis by automating image processing, peak detection, statistical analysis, and data fitting to a 4-parameter logistic curve for accurate limit of detection (lod) determination. this approach significantly minimizes manual intervention and the potential for human error, thereby enhancing reproducibility. this addresses the critical need for efficient and reliable lfa data analysis in research laboratories. by offering a standardized method for lfa image processing and data analysis, this protocol facilitates faster and more consistent research outcomes."
https://doi.org/10.26434/chemrxiv-2024-vt6z8,2024-03-05,from eyes to cameras: computer vision for high-throughput liquid-liquid separation,Rama El-khawaldeh Abhijoy Mandal Naruki Yoshikawa Wenyu  Zhang Ryan Corkery Paloma Prieto Alán Aspuru-Guzik Kourosh Darvish Jason Hein,"we present a modular, high-throughput (ht) automation platform for screening liquid-liquid extraction (lle) workup processes. our automated hardware platform simultaneously screens up to 12 vials, and is coupled with a computer vision (cv) system for real-time monitoring of macroscopic visual cues. our cv system, named heinsight3.0, leverages machine learning and image analysis to classify and quantify multivariate visual cues such as liquid level, phase split clarity, turbidity, homogeneity, volume, and color. these cues, combined with process parameters like stir rate and temperature, enable real-time analysis of key workup processes (e.g., separation time, phase split quality, volume ratio of layers, color, and emulsion presence) to aid in the optimization of separation parameters. we demonstrate our system on three case-studies: impurity recovery, excess reagent removal, and grignard workup. our application of heinsight3.0 on literature data also suggests high potential for generalizability and adaptability across different platforms and contexts. overall, our work represents a significant step towards achieving end-to-end autonomous lle screening guided by visual cues, contributing to the realization of a self-driving lab for workup processes."
https://doi.org/10.26434/chemrxiv-2024-wtjt6,2024-03-05,generate what you can make: achieving in-house synthesizability with readily available resources in de novo drug design,Alan Kai Hassen Martin Sicho Yorick J. van Aalst Mirjam C.W. Huizenga Darcy N.R. Reynolds Sohvi Luukkonen Andrius Bernatavicius Djork-Arné Clevert Antonius P.A. Janssen Gerard J.P. van Westen Mike Preuss,"molecules generated by computer-aided drug design often lack synthesizability to be valuable because computer-aided synthesis planning (casp) and casp-based approximated synthesizability scores have rarely been used as generation objectives, despite facilitating the in-silico generation of synthesizable molecules. published scores approximate a general notion of casp-based synthesizability with nearly unlimited building block resources. however, this approach is disconnected from the reality of small laboratory drug design, where building block resources are limited, making a notion of in-house synthesizability that uses already available resources highly desirable. in this work, we show a successful de novo drug design workflow generating active and in-house synthesizable ligands of monoglyceride lipase (mgll). we demonstrate the successful transfer of casp from 17.4 million commercial building blocks to a small laboratory setting of roughly 6,000 building blocks with only a decrease of -12% in casp success. moreover, we present a rapidly retrainable in-house synthesizability score, successfully capturing our in-house synthesizability without relying on external building block resources. we show that including our in-house synthesizability score in a multi-objective de novo drug design workflow, alongside a simple qsar model, provides thousands of potentially active and easily in-house synthesizable molecules. further, we highlight differences between general and in-house synthesizability scores and demonstrate potential problems with the out-of-distribution predictive performance of synthesizability scores on generated molecules. finally, we experimentally evaluate the synthesis and biochemical activity of three de novo candidates using their casp-suggested synthesis routes using only in-house building blocks. we find one candidate with evident activity, suggesting potential new ligand ideas for mgll inhibitors while showcasing the usefulness of our in-house synthesizability score."
https://doi.org/10.26434/chemrxiv-2023-c4867-v2,2024-03-05,molscore: a scoring and evaluation framework for de novo drug design,Morgan Thomas Noel M. O'Boyle Andreas Bender Chris de Graaf,"molscore is an open-source python framework for scoring and evaluating molecules in the context of goal-directed generative models as used in de novo drug design. molscore includes many relevant scoring functions for de novo drug design such as molecular similarity, docking software, predictive models, and synthesizability, as well as commonly used performance metrics to evaluate generative model performance based on chemistry generated. integration into an existing generative model framework is simple, requiring just three lines of code, and graphical user interfaces are provided for objective configuration and for monitoring de novo molecules generated. as a real-world demonstration of its use, we use it to design selective 5-ht2a ligands using 266 pre-trained off-target predictive models, as well as docking into two co-crystal structures. molscore can also be used for generative model evaluation as we demonstrate by analysing and selecting fine-tuning epochs of an rnn-based generative model. moreover, the use of configuration files allows the sharing of objectives within the community for the purposes of reproducibility, comparison, and benchmarking; making it easier to propose drug discovery relevant objective functions as benchmark tasks. the code is freely available and hosted on github, https://github.com/morgancthomas/molscore."
https://doi.org/10.26434/chemrxiv-2023-llpnk-v3,2024-03-05,augmentation of structure information to the sequence-based machine learning-assisted directed protein evolution,Lane Yutzy Kenny Nguyen Peter Vallet Jianxiong Li Jielin Yu Ronggui He Le Yan Joohyun  Kim Jangwook  Jung,"directed evolution (de) mimics natural selection to improve the functions of a target protein. machine learning (ml) has significantly streamlined de by aiding in several steps, which includes identifying starting variants, generating diverse libraries and modeling sequence-fitness relationships. to date, the majority of ml-assisted de (mlde) approaches has relied predominantly on sequence information due to the challenges and cost of obtaining protein structure information. here, we introduce a structure-augmented mlde (samlde) approach for selecting high fitness variants from a library of protein g b1 domain. we adopted and applied a zero-shot sequence-based prediction method (offering the potential to discover new insights without extensive training data) to select an initial training library of 96 variants for the samlde campaign. to leverage protein structure information, we used protein structure prediction with alphafold2 and molecular docking simulations performed with rosetta flexpepdock, resulting in structure-based features derived with an induced fit model. after three rounds of the samlde campaign, we demonstrated that samlde incorporating structural information gradually improves the average fitness scores and the precision of predicted binders. in addition, we found that the initial library selection with zero-shot subset selection methods significantly impacted the average fitness scores and precision, consequently influencing the overall directed evolutionary trajectories."
https://doi.org/10.26434/chemrxiv-2024-hs7sf,2024-03-04,long-range interactions in salt-in-ionic liquids,Xuhui  Zhang Goodwin Zachary A. H. Alexis G. Hoane Alex Deptula Daniel M. Markiewitz Nicola  Molinari Qianlu  Zheng  Hua Li Michael  McEldrew Boris  Kozinsky  Martin Z. Bazant Cecilia Leal Rob Atkin Andrew Gewirth Mark W. Rutland Rosa M.  Espinosa-Marzal,"ionic liquids (ils) are a promising class of electrolytes owing to a unique combination of properties, such as extremely low vapour pressures, non-flammability and being universal solvents. doping ils with alkali metal salts creates an electrolyte that is of interest for batteries, among others. these salt-in-ionic liquids (siils) are a class of super-concentrated, strongly correlated and asymmetric electrolytes. the transference number of the alkali metal cations has been found to be negative, owing to the small but highly negatively charged aggregates which form between alkali metal ions and the anions. here, we investigate na-based siils with a surface forces apparatus and by atomic force microscopy. we find evidence of confinement induced structural changes, giving rise to unprecedented long-range (non-exponentially decaying) interactions. this observation is supported by the soft structure revealed by the force curves, and supplemented by theory and simulations. the long-ranged interactions in siils are reminiscent of polymer-like interactions, suggesting analogous high aspect ratio aggregates at the mica interfaces, rather than a purely electrostatic origin. remarkably, our aggregation framework and conclusions can also explain the negative transference number, often observed in these systems by the battery community."
https://doi.org/10.26434/chemrxiv-2024-q12fw-v2,2024-03-04,single-entity protein electrochemistry of  diffusion-limited enzymes ,Ziwen Zhao Nikolaos Kostopoulos Sagar  Ganguli Paul  Bergstrom Alina Sekretareva,"single-entity electrochemistry has recently emerged as a promising method for label-free exploration of the catalytic functions of individual enzymes. however, skepticism within the scientific community regarding the applicability of the method for single enzyme measurements has arisen due to issues in the experimental data presented in the literature and limited theoretical modeling of such data. here, we address these concerns through a thorough experimental investigation of two diffusion-limited enzymes, catalase and superoxide dismutase, employing a combination of protein film voltammetry and single-entity protein electrochemistry measurements. we then introduce a novel theoretical model for simulating the current responses, generated by the reduction of the product of the enzymatic reaction of single enzyme molecules at the electrode. this model is based on a combination of finite element simulations using comsol multiphysics and random walk simulations. it incorporates the diffusion-limited enzymatic kinetics of the investigated enzymes and introduces a geometry that mimics the substrate diffusion channel of the enzyme. our work demonstrates that the experimentally detected current signals align with the simulated current signals, affirming that they can be attributed to the catalytic activity of single enzymes detected via the product of the enzymatic reaction."
https://doi.org/10.26434/chemrxiv-2024-05g0b,2024-03-04,cluster analysis as a tool for quantifying structure-transport properties in simulations of superconcentrated electrolyte,Sheng Bi Mathieu Salanne,"using molecular dynamics simulations and graph-theory-based cluster analysis, we investigate the structure-transport properties of typical water-in-salt electrolytes. we demonstrate that ions exhibit distinct dynamics across different ionic clusters—namely, solvent-separated ion pairs (ssips), contact ion pairs (cips), and aggregates (aggs). we assess the average proportions of various ionic species and their lifetimes. our method reveals a dynamic decoupling of ion kinetics, with each species independently contributing to the overall molecular motion. this is evidenced by the fact that the total velocity autocorrelation function (vacf) and power spectrum can be expressed as a weighted sum of independent functions for each species. the experimental data on the ionic conductivity of the studied litfsi electrolytes align well with our theoretical predictions at various concentrations, based on the proportions and diffusion coefficients of free ions derived from our analysis. the insights gained into the solvation structures and dynamics of different ionic species enable us to elucidate the physical mechanisms driving ion transport in such superconcentrated electrolytes, providing a comprehensive framework for the future design and optimization of electrolytes. "
https://doi.org/10.26434/chemrxiv-2024-1bg6h,2024-03-04,colorimetric detection of fentanyl using a supramolecular displacement assay,Andrea C. Mora Madeline  Vara Patrick Reust Amanda Code Piercen Oliver Charles R. Mace,"fentanyl is a potent synthetic opioid with an alarmingly low lethal dosage of 2 mg. the equipment necessary to detect fentanyl in field settings (e.g., handheld spectrometers) is restricted to highly trained, well-funded, and specialized personnel. established point-of-need technologies, such as lateral flow immunochromatographic strips, are available; however, they often involve multiple contact-based steps (e.g., collection, mixing) that pose a higher risk to users handling unknown substances. herein, we developed a colorimetric displacement assay capable of contactless detection of fentanyl as liquid or solid samples. the basis of our assay relies on the presence of fentanyl to displace a redox mediator, ferrocene carboxylic acid, inclusively bound in the cavity of a supramolecular host, cb[7]. the displacement is only possible in the presence of high affinity binding guests, like fentanyl (ka  ~ 106 m-1). the liberated redox guest can then react with indicator reagents that are free in solution, producing either: (i) a distinct blue color to indicate the presence of fentanyl or (ii) remain a pale blue tint in the absence of fentanyl. we demonstrate rapid and specific detection of fentanyl free base and fentanyl derivatives (e.g., acetyl fentanyl, furanyl fentanyl) against a panel of 9 other common drugs of abuse (e.g., morphine, cocaine, heroin). furthermore, we highlight the intended use of this assay by testing grains of fentanyl derivatives on a surface with a drop (i.e., 25 µl) of assay reagent. we anticipate this approach can be applied broadly to identify the presence of fentanyl at the point of need."
https://doi.org/10.26434/chemrxiv-2024-fh132-v2,2024-03-01,universal descriptors of 'quasi transition states' for small-data-driven asymmetric catalysis prediction in machine learning model,Guanming Chen Zihao Ye Zhiming Li Junliang Zhang,"in contemporary enantioselectivity prediction models, the demand for numerous descriptors and extensive datasets poses a substantial challenge. descriptor selection, fraught with uncertainty, compounds the issue while amassing requisite data remains a daunting task. the introduction of descriptors derived from quasi-transition-states (qts) offers a promising avenue to alleviate this burden. however, the challenge of descriptor selection persists. herein, a novel small-data-driven model based on universal descriptors (ud-qts) is proposed. key differentiating properties between diastereomeric qtss, encompassing energies, frontier orbital energies, cartesian forces and charges of core atoms, are proposed as ud. the model's efficacy is validated through its application to the asymmetric aldol reaction, utilizing 3 experimental variables and merely 9 uds, and fewer than 150 training samples. moreover, a novel method is presented using cartesian forces to rectify discrepancies between qts and true ts. this ud-qts strategy circumvents tedious large-scale descriptor exploration and screening, offering an efficient choice for small-data-driven enantioselectivity prediction."
https://doi.org/10.26434/chemrxiv-2023-t6zrj-v2,2024-03-01,the importance of reaction energy in predicting chemical reaction barriers with machine learning models ,Nithin Lalith Aayush Singh Joseph Gauthier,"improving our fundamental understanding of complex heterocatalytic processes increasingly relies on electronic structure simulations and microkinetic models based on calculated energy differences. in particular, calculation of activation barriers, usually achieved through compute-intensive saddle point search routines, remains a serious bottleneck in understanding trends in catalytic activity for highly branched reaction networks. although the well-known brønsted-evans-polyani (bep) scaling – a one-dimensional linear regression model – has been widely applied in such microkinetic models, they still rely on calculated reaction energies and may not generalize beyond a single facet on a single class of materials, e.g., a terrace sites on transition metals. for highly branched and energetically shallow reaction networks, such as electrochemical co2 reduction or waste remediation, calculating even reaction energies on many surfaces can become computationally intractable due to the combinatorial explosion of states that must be considered. here, we investigate the feasibility of activation barrier prediction without knowledge of the reaction energy using linear and nonlinear machine learning (ml) models trained on a new database of over 500 dehydrogenation activation barriers. we and find that inclusion of the reaction energy significantly improves both classes of ml models, but complex nonlinear models can achieve performance similar to the simplest bep scaling when predicting activation barriers on new systems. additionally, inclusion of the reaction energy significantly improves generalizability to new systems beyond the training set. our results suggest that the reaction energy is a critical feature to consider when building models to predict activation barriers, indicating that efforts to reliably predict reaction energies reliably through, e.g., the open catalyst project and others, will be an important route to effective model development for more complex systems.   "
https://doi.org/10.26434/chemrxiv-2024-bf64r,2024-02-29,"cosalen-catalyzed radical hydrofunctionalization of unactivated internal olefins at low catalyst loading: method development, regioselectivity, and applications in post-polymerization modification",Yun-Nian Yin Bang-Sen Zhao Han-Yuan Liu Rui-Qing Sheng Dong-Chen Ouyang Rong Zhu,"the past two decades witnessed the rapid development of cosalen-catalyzed hydrofunctionalization reactions, mostly on terminal and conjugation-activated olefins. however, the use of 1,2-dialkylsubstituted alkenes continues to pose challenges. in this study, we revisit this substrate class in the context of carreira-type hydrofunctionalization reactions and introduce a simple yet effective modification (over 250-fold-increase in ton). near-quantitative yields can be achieved at a low catalyst loading, typically 0.05 mol%. the key lies in inhibiting the degradation of the salen backbone using molecular sieves. this new protocol facilitates a study on the mhat regioselectivity of this type of alkenes. we found that allylic electron-negative groups and hyperconjugation have profound effects, yielding regioisomeric ratios ranging from 6.5:1 to < 1:20. the high ton, mild conditions, and versatility of this method further enable its application in the post-polymerization modification of several olefin-rich, commodity-relevant polymers."
https://doi.org/10.26434/chemrxiv-2024-28vsv,2024-02-29,dna motifs hydrogel microparticles as gold nanocatalyst support material,Daisuke Ishikawa Koki Maruyama Masahiko Hara,"gold nanoparticles (au nps) are unique catalysts because the nanoparticulation increases the number high coordination unsaturation sites. for au nps to act as catalysts, the choice of a suitable support for the desired reaction is essential to ensure homogeneous dispersion of the particles while maintaining their nanosize. as a support material, hydrogels obtained by three-dimensional crosslinking of hydrophilic polymers are suitable dispersants for au nps. in addition, their high affinity for water-rich environments and high biocompatibility have attracted attention for biological and biomedical reactions. however, application in small, confined spaces such as in vivo requires micronization of the hydrogel. here, we have developed a gold catalyst supported on micro-sized dna hydrogels with variable internal crosslinking densities composed of simple dna nanostructures called motifs. the short strand length of the dna motif resulted in dna hydrogel particles with high crosslinking density and au nps with small particle size. furthermore, our micro-sized dna hydrogel particle-supported gold catalyst system sufficiently advanced the model reaction with a very small amount of gold catalyst compared to a gel-supported gold catalyst system with a comparable au np size. the crosslinking density of the dna hydrogel particles, and thus the size of the resulting au nps, can be easily varied and the catalytic activity can be enhanced by increasing the surface area of the hydrogel catalyst support through microparticulation. our findings based on dna nanotechnology suggest that the physical properties of the dna hydrogel particles, such as particle size and crosslinking density, and the catalytic activity of the au nps, can be easily designed using computer software, as can the arrangement and length of the motifs that form the constituent units."
https://doi.org/10.26434/chemrxiv-2024-1r9tb,2024-02-28,(semi-) automatic review process for common compound characterization data in organic synthesis,Yu-Chieh Huang Pierre Tremouilhac Stefan Kuhn Pei-Chi Huang Chia-Lin Lin Nils Schlörer Oskar Taubert Markus Götz Nicole Jung Stefan Bräse,"a method for data review in chemical sciences with a focus on data for the characterization of synthetic molecules is described. as current procedures for data curation in chemistry rely almost exclusively on manual checking or peer reviewing, a (semi-)automatic procedure for the evaluation of data assigned to molecular structures is proposed and demonstrated. the information usually required for the identification of isolated compounds is used to clarify whether the data is complete with respect to the available data types and metadata, if it is consistent with the proposed structure and if it is plausible in comparison to simulated data. spectra prediction and automatic signal comparison are applied to nmr evaluation, mass spectrometry data are evaluated by signal extraction, and machine learning is used for ir analysis. the proposed protocol shows how an integration of different tools for data analysis can help to overcome the challenges of the currently purely manual reviewing and curation efforts for data in synthetic chemistry. "
https://doi.org/10.26434/chemrxiv-2024-k9w3s,2024-02-28,ring-opening polymerization of amino acid n‑carboxyanhydrides with unprotected/reactive side groups. ii. l‑hydroxyproline n‑carboxyanhydride,Letian Wang Xinyi Zhu Chenming Tang Xiaodong Jing Yahui He Hua Lu,"poly-l-hydroxyproline (phyp) is a synthetic analogue of collagen, the most abundant protein for animals, and holds immense potential for broad biomedical applications. the synthesis of phyp, however, involves inefficient protection-deprotection steps and has been restricted to relatively low molecular weight (mw) and linear topology. here, we report the ring-opening polymerization (rop) of unprotected hydroxyproline n-carboxyanhydrides (hyp-nca) for the facile one-step synthesis of phyp with tunable linear or branching topologies. employing an innovative water-assisted ultrafast polymerization technique, the research achieves the synthesis of linear phyp with mw up to 7.5 kda, featuring adjustable terminal groups and narrow dispersity. the study further introduces a tertiary amine-triggered one-pot polymerization method in dmso, which leads to the preparation of branched phyp (b-phyp) with mw up to 438 kda, ~40 times higher than previous record of phyp. facile post-polymerization modification of b-phyp affords injectable hydrogels with a critical gelization concentration as low as 1.0%. the polymers, characterized by their distinctive collagen-like polyproline type ii (ppii) helices, offer significant prospects in drug delivery, wound healing, and other biomedical applications."
https://doi.org/10.26434/chemrxiv-2024-173dp,2024-02-28,annotating materials science text: a semi-automated approach for crafting outputs with gemini pro,Hasan M Sayeed Trupti Mohanty Taylor Sparks,"recent advancements in large language models (llms) have paved the way for automated information extraction in the materials science domain. however, fine-tuning these models, crucial for effective machine learning pipelines in materials science, is hindered by a lack of pre-annotated data. manual annotation, a laborious process, exacerbates the challenge. to address this, we introduce a tailored semi-automated annotation process, using google's gemini pro language model. our approach focuses on two key tasks: extracting information in structured json format and generating abstractive summaries from materials science texts. the collaborative process, a symbiotic effort between human annotators and the llm, driven by structured prompts and user-guided examples, enhances the annotation quality and augments the llm's capacity to comprehend materials science intricacies. importantly, it streamlines human annotation efforts by leveraging the llm's proficient starting point."
https://doi.org/10.26434/chemrxiv-2024-x93r1,2024-02-27,a mode evolution metric to extract converged reaction coordinates for biomolecular conformational transitions,Mitradip Das Ravindra Venkatramani,"the complex multidimensional energy landscape of biomolecules makes the extraction of suitable non-intuitive collective variables (cvs) which describe their conformational transitions challenging. at present dimensionality reduction approaches and machine learning schemes are employed to obtain reaction coordinates from datasets sampled either from techniques like molecular dynamics (md) simulations or structural databanks for biomolecules. however, a poor understanding of sampling convergence and completeness of the dataset seriously limits assessment of the quality of the extracted cvs. here, we build upon statistically rigorous ideas of local equilibration to develop a mode evolution metric (mem) which can extract quantitatively converged cvs from non-equilibrated md simulations using dimensionality reduction or machine learning approaches. specifically, we apply mem to extract converged principal components for transitions in model potential energy landscapes of varying complexities and in solvated alanine dipeptide. finally, we demonstrate a possible application of mem in designing efficient biased sampling schemes to construct accurate energy landscape slices which link transitions between two states. mem can help speed up the search for new minima around a biomolecular conformational state and enable the accurate estimation of thermodynamics for states lying on the energy landscape and descriptions of associated transitions."
https://doi.org/10.26434/chemrxiv-2024-mfpkx,2024-02-27,rapid flow-based synthesis of post-translationally modified peptides and proteins:  a case study on myc’s transactivation domain ,Elyse T. Williams Kevin Schiefelbein Matthias Schuster Ikhlas M. M. Ahmed Marije De Vries Rebecca Beveridge Oliver Zerbe Nina Hartrampf,"protein-protein interactions of c-myc (myc) are often regulated by post-translational modifications (ptms), such as phosphorylation, and crosstalk thereof. studying these interactions requires proteins with unique ptm patterns, which are challenging to obtain by recombinant methods. standard peptide synthesis and native chemical ligation can produce such modified proteins, but are time-consuming and therefore typically limited to the study of individual ptms. herein, we report the development of flow-based methods for the rapid synthesis of phosphorylated myc sequences (up to 84 aa), and demonstrate the versatility of this approach for the incorporation of other ptms (nε methylation, sulfation, acetylation, glycosylation) and combinations thereof. peptides containing up to seven ptms and five phosphorylations were successfully prepared and isolated in high yield and purity. our methodology was then applied in the production of ten ptm-decorated analogues of the myc transactivation domain (tad) to screen for binding to the tumor suppressor protein, bin1, using heteronuclear nmr and native mass spectrometry. we determined the effects of phosphorylation and glycosylation on the strength of the myc:bin1 interaction, and reveal an influence of myc sequence length on binding. our platform for the rapid synthesis of myc sequences up to 84 aa with distinct ptm patterns thereby enables the systematic study of ptm function at a molecular level, and offers a convenient way for an expedited screening of constructs."
https://doi.org/10.26434/chemrxiv-2024-wlx21,2024-02-27,high-throughput quantum theory of atoms in molecules (qtaim) for geometric deep learning of molecular and reaction properties,Santiago Vargas Winston  Gee Anastassia Alexandrova,"we present a package, generator, for geometric molecular property prediction based on topological features of quantum mechanical electron density. generator computes quantum theory of atoms in molecules (qtaim) features, at density functional theory (dft) level, for sets of molecules or reac- tions in a high-throughput manner, and compiles features into a single data structure for processing, analysis, and geometric machine learning. an accompanying graph neural network package can be used for property prediction and allows users to readily use computed features for learning tasks. to test the efficacy of electron density-based data for machine learning, we benchmark several datasets including qm8, qm9, libe, tox21, and a green 2022 reaction dataset. this wide dataset diversity underscores the flexibility of qtaim descriptors and our package. in addition, we made our code high-throughput methods compatible with new versions of bondnet and chemprop architectures to allow for both reaction and molecular property prediction out-of-the-box. to motivate the use of qtaim features for varied prediction tasks we also perform extensive benchmarking of our new mod- els to existing benchmark models as well as to our own models without qtaim features. we show that almost universally, qtaim features improve model performance on our algorithms, chemprop, and bondnet. we also determine that qtaim can aid in generalizing model performance to out-of- domain (ood) datasets and improve learning at smaller data regimes. combined, we hope that this framework could enable qtaim-enhanced structure-to-property predictions - especially in domains with less data, including experimental or reaction-level datasets with complex underlying chemistries"
https://doi.org/10.26434/chemrxiv-2024-83h6h-v2,2024-02-27,capture-and-release of a sulfoquinovose-binding protein on sulfoquinovose-modified agarose,Thimali  Arumapperuma Alexander Snow Mihwa Lee Mahima Sharma Yunyang Zhang James Lingford Ethan Goddard-Borger Gideon Davies Spencer Williams,"the solute-binding protein (sbp) components of periplasmic binding protein-dependent atp-binding cassette (abc)-type transporters often possess exquisite selectivity for their cognate ligands. maltose binding protein (mbp), the best studied of these sbps, has been extensively used as a fusion partner to enable the affinity purification of recombinant proteins. however, other sbps and sbp-ligand based affinity systems remain underexplored. the sulfoquinovose-binding protein smof, is a substrate-binding protein component of the abc transporter cassette in agrobacterium tumefaciens involved in importing sq and its derivatives for sq catabolism. here, we show that smof binds with high affinity to the octyl glycoside of sq (octyl-sq), demonstrating remarkable tolerance to extension of the anomeric substituent. the 3d x-ray structure of the smof•octyl-sq complex reveals accommodation of the octyl chain, which projects to the protein surface, providing impetus for the synthesis of a linker-equipped sq-amine using a thiol-ene reaction as a key step, and its conjugation to cyanogen bromide modified agarose. we demonstrate the successful capture and release of smof from sq-agarose resin using sq as competitive eluant, and selectivity for release versus other organosulfonates. we demonstrate that smof can be captured and purified from a cell lysate, demonstrating the utility of sq-agarose in capturing sq binding proteins from complex mixtures. the present work provides a pathway for development of ‘capture-and-release’ affinity resins for the discovery and study of sbps."
https://doi.org/10.26434/chemrxiv-2023-6rng3-v2,2024-02-27,emle-engine: a flexible electrostatic machine learning embedding package for multiscale molecular dynamics simulations,Kirill Zinovjev Lester Hedges Rubén Montagud Andreu Christopher Woods Iñaki Tuñón Marc W. van der Kamp,"we present in this work the emle-engine package (https://github.com/chemle/emle-engine) – the implementation of a new machine learning embedding scheme for hybrid machine learning potential / molecular mechanics (ml/mm) dynamics simulations. the package is based on an embedding scheme that uses a physics-based model of the electronic density and induction with a handful of tuneable parameters derived from in vacuo properties of the subsystem to be embedded.  this scheme is completely independent of the in vacuo potential and requires only the positions of the atoms of the machine learning subsystem and the positions and partial charges of the molecular mechanics environment. these characteristics allow emle-engine to be employed in existing qm/mm software. we demonstrate that the implemented electrostatic machine learning embedding scheme (named emle) is stable in enhanced-sampling molecular dynamics simulations. through calculation of free energy surfaces of alanine dipeptide in water with two different ml options for the in vacuo potential and three embedding models, we test the performance of emle. when compared to the reference dft/mm surface, the emle embedding is clearly superior to the mm one based on fixed partial charges. the configurational dependence of the electronic density and the inclusion of the induction energy introduced by the emle model leads to a systematic reduction in the average error of the free energy surface when compared to mm embedding. by enabling usage of emle embedding in practical ml/mm simulations, emle-enline will make it possible to accurately model systems and processes that feature significant variations in the charge distribution of the ml subsystem and/or the interacting environment."
https://doi.org/10.26434/chemrxiv-2024-80d93,2024-02-26,learning qm/mm potential using equivariant multiscale model,Yao-Kun Lei Kiyoshi Yagi Yuji Sugita,"the machine learning (ml) method emerges as an efficient and precise surrogate model for high-level electronic structure theory. its application has been limited to closed chemical systems without considering external potentials from the surrounding environments. to address this limitation and incorporate the influence of external potentials, polarization effects, and long-range interactions between a chemical system and its environment, the first two terms of the taylor expansion of an electrostatic operator have been used as extra input to the existing ml model to represent the electrostatic environments. however, high-order electrostatic interaction is often essential to account for external potentials from the environment. the existing models based only on the invariant features cannot capture significant distribution patterns of the external potentials. here, we propose a novel ml model that includes high-order terms of the taylor expansion of an electrostatic operator and uses an equivariant model, which can generate high-order tensors covariant with rotations as a base model. thus, we can use the multipole-expansion equation to derive a useful representation by accounting for the polarization and intermolecular interaction. moreover, to deal with long-range interactions, we follow the same strategy adopted to derive long-range interaction between a target system and its environment media. our model achieves higher prediction accuracy and transferability among various environment media with these modifications."
https://doi.org/10.26434/chemrxiv-2024-r81c8,2024-02-26,linear graphlet models for accurate and interpretable cheminformatics,Michael Tynes Michael G Taylor Jan Janssen Daniel J Burrill Danny Perez Ping Yang Nicholas Lubbers,"advances in machine learning have given rise to a plurality of data-driven methods for estimating chemical properties from molecular structure. for many decades, the cheminformatics field has relied heavily on structural fingerprinting, while in recent years much focus has shifted leveraging highly parameterized deep neural networks which usually maximize accuracy. beyond accuracy, machine learning techniques need intuitive and useful explanations for the predictions of models and uncertainty quantification techniques so that a practitioner might know when a model is appropriate to apply to new data. here we show that linear models built on unfolded molecular-graphlet-based fingerprints attain accuracy that is competitive with the state of the art while retaining an explainability advantage over black-box approaches. we show how to produce precise explanations of predictions by exploiting the relationships between molecular graphlets and show that these explanations are consistent with chemical intuition, experimental measurements, and theoretical calculations. finally we show how to use the presence of unseen fragments in new molecules to adjust predictions and quantify uncertainty. "
https://doi.org/10.26434/chemrxiv-2024-mdpw8,2024-02-26,what is the appropriate data representation of electrochemical impedance spectroscopy in machine-learning analysis?,Jingwen Sun Weitong Zhang Yuanzhou Chen Benjamin Hoar Hongyuan Sheng Jenny Yang Cyrille Costentin Quanquan Gu Chong Liu,"electrochemical impedance spectroscopy (eis) is an important analytic technique for the understanding of electrochemical systems. with the recent advent and burgeoning deployment of machine learning (ml) in eis analysis, a critical yet hitherto unanswered question emerges: what is the appropriate data representation of eis for ml-based analysis? while the representation of a model’s input data is known to be critical for a successful deployment of ml model, eis is known to possess multiple classical venues of data representation and it remains unclear how different eis data should be compared following a proper data normalization protocol. here we report the methodology and the outcomes that evaluate the efficacy of multiple data representation methods in ml-based eis analysis. at least within our proof-of-concept parameter space, plotting the input training data’s impedance magnitude (|z|) against phase angle (φ) while individually normalizing each eis curve yields the highest accuracy and robustness in the correspondingly established residual neural network (resnet) model. rationalized by additional ""importance"" analysis of the input data, such a data representation method extracts information and hidden features more effectively. while nyquist plot is more widely used in manual analysis, we found that ml-based analysis may require a different data representation and offered a clear guideline for future researchers to evaluate on a case-by-case basis."
https://doi.org/10.26434/chemrxiv-2024-17w01,2024-02-26,fragmenstein: predicting protein-ligand structures of compounds derived from known crystallographic fragment hits using a strict conserved-binding–based methodology,Matteo P. Ferla Rubén Sánchez-García Rachael E. Skyner Stefan Gahbauer Jenny C. Taylor Frank von Delft Brian D. Marsden Charlotte M. Deane,"current strategies centred on either merging or linking initial hits from fragment-based drug design (fbdd) crystallographic screens ignore 3d structural information. we show that an algorithmic approach (fragmenstein) that ‘stitches’ the ligand atoms from this structural information together can provide more accurate and reliable predictions for protein-ligand complex conformation than existing methods such as pharmacophore-constrained docking. this approach works under the assumption of conserved binding: when a larger molecule is designed containing the initial fragment hit, the common substructure between the two will adopt the same binding mode. fragmenstein either takes the coordinates of ligands from a experimental fragment screen and stitches the atoms together to produce a novel merged compound, or uses them to predict the complex for a provided compound. the compound is then energy minimised under strong constraints to obtain a structurally plausible compound. this method is successful in showing the importance of using the coordinates of known binders when predicting the conformation of derivative compounds through a retrospective analysis of the covid moonshot data. it has also had a real-world application in hit-to-lead screening, yielding a sub-micromolar merger from parent hits in a single round."
https://doi.org/10.26434/chemrxiv-2024-hk34t,2024-02-26,compas-3: a data set of peri-condensed polybenzenoid hydrocarbons,Alexandra Wahab Renana Gershoni-Poranne,"we introduce the third installment of the compas project – a computational database of polycyclic aromatic systems, focused on peri-condensed polybenzenoid hydrocarbons. in this installement, we develop two data sets containing the optimized ground-state structures and a selection of molecular properties of ∼39k and ∼9k peri -condensed polybenzenoid hydrocarbons (at the gfn2-xtb and cam-b3lyp-d3bj/cc-pvdz//cam-b3lyp-d3bj/def2-svp levels, respectively). the manuscript details the enumeration and data generation processes and describes the information available within the data sets. an in-depth comparison between the two types of computation is performed, and it is found that the geometric disagreement is maximal for slightly-distorted molecules. in addition, a data-driven analysis of the structure-property trends of peri-condensed pbhs is performed, highlighting the effect of the size of peri-condensed islands and linearly annulated rings on the homo-lumo gap. the insights described herein are important for rational design of novel functional aromatic molecules for use in, e.g., organic electronics. the generated data sets provide a basis for additional data- driven machine- and deep-learning studies in chemistry"
https://doi.org/10.26434/chemrxiv-2024-26xtf,2024-02-26,evaluating the interactions between vibrational modes and electronic transitions using frontier orbital derivatives,Lisa A. Schröder Harry L.  Anderson Igor Rončević,"vibrations can significantly affect molecular properties, even at zero kelvin. accounting for these effects when using computational modelling is costly, as it requires many calculations at geometries distorted from equilibrium. here, we propose a low-cost method for identifying vibrations most strongly coupled to the electronic structure, based on using orbital derivatives as a diagnostic."
https://doi.org/10.26434/chemrxiv-2024-jcn9d,2024-02-23,for catching-by-polymerization oligo purification: scalable synthesis of the precursors to the polymerizable tagging phosphoramidites,Yipeng Yin Komal Chillar Alexander Apostle Bhaskar Halami Adikari M. D. N. Eriyagama Marina Tanasova Shiyue Fang,"the catching-by-polymerization (cbp) oligodeoxynucleotide (oligo or odn) purification method has been demonstrated suitable for large-scale, parallel, and long oligo purification. the authenticity of the oligos has been verified via dna sequencing. gene construction and expression have been demonstrated. a remaining obstacle to the practical utility of the cbp method is affordable polymerizable tagging phosphoramidites (ptps) that are needed for the method. in this article, we report scalable synthesis of the four nucleoside (da, dc, dg and dt) precursors to the ptps using a route having six steps from inexpensive starting materials. the overall yields ranged from 21% to 35%. the scales of the synthesis presented here are up to 2.1 grams of the precursors. because the syntheses are chromatography-free, they are predicted to be readily scalable. with the precursors, the ptps can be synthesized in one step using standard methods involving a chromatography purification."
https://doi.org/10.26434/chemrxiv-2024-0bsk9,2024-02-22,enantioselective carbon isotope exchange,Michael Doyle Odey Bsharat Anna Sib Volker Derdau Rylan Lundgren,"the synthesis of isotopically labeled organic molecules is vital for drug and agrochemical discovery and development. carbon isotope exchange is emerging as a leading method to generate carbon-labeled targets, which are sought due to their enhanced stability in biological systems. while many bioactive small molecules bear carbon-containing stereocenters, direct enantioselective carbon isotope exchange reactions have not been established. we describe the first example of an enantioselective carbon isotope exchange reaction, where (radio)labeled α-amino acids can be generated from their unlabeled precursors in a single step using a chiral aldehyde mediator with isotopically labeled co2. many proteinogenic and non-natural derivatives undergo enantioselective labeling, including the late-stage radiolabeling of complex drug targets."
https://doi.org/10.26434/chemrxiv-2024-xg4x8,2024-02-22,highly selective drug-derived fluorescent probes for the cannabinoid receptor type 1 (cb1r),Leonard  Mach Anahid  Omran Jara  Bouma Silke  Radetzki David A.  Sykes Wolfgang  Guba Xiaoting  Li Calvin Höffelmeyer Axel Hentsch Thais  Gazzi Yelena  Mostinski Malgorzata  Wasinska-Kalwa Fabio  de Molnier Cas  van der Horst Jens Peter  von Kries Marc  Vendrell Tian  Hua Dmitry B.  Veprintsev Laura H.  Heitman Uwe Grether Marc Nazare,"the cannabinoid receptor type 1 (cb1r) is one of the central elements of the endocannabinoid system regulating a variety of signaling cascades. extensive efforts on cb1r have validated its essential roles in physiology such as appetite regulation, pain perception, memory formation, and thermoregulation. yet, there is a surprising lack of clear understanding of its cellular signaling, distribution, and expression dynamics. cb1r visualization in real-time is therefore crucial for addressing these open questions in cannabinoid research. using various highly selective drug-like cb1r ligands with a defined pharmacological profile, we investigated their potential for constructing cb1r fluorescent probes by a reverse design-approach. a modular design concept with a diethyl glycine-based building block as centerpiece allowed the straightforward modular synthesis of novel probe candidates. supported by computational docking studies, this systematic approach led to the identification of novel pyrrole-based cb1r fluorescent probes. the probes demonstrated cb1r selectivity in radioligand binding profiling and inverse agonist activity in a camp assay. application in time-resolved fluorescence resonance target-engagement studies and cb1r live cell imaging exemplify the great versatility of the tailored pyrrole-based fluorescent probes. these validated fluorescent probes aim to deepen the understanding of mechanistic aspects of cb1r localization, trafficking, and activation essential for the function and role of this receptor in pathological conditions."
https://doi.org/10.26434/chemrxiv-2024-rk4qx,2024-02-22,machine learning-driven models for predicting co2 uptake in metal-organic frameworks (mofs) ,Sofiene Achour Zied Hosni,"this study advances the discourse on the application of machine learning (ml) algorithms for the predictive analysis of co2 uptake in metal-organic frameworks (mofs), with a nuanced focus on the catboost model's capability to navigate the complexities inherent in mofs' heterogeneous landscape. building upon and extending the comparative analysis, our investigation underscores the catboost model's remarkable predictive prowess, characterized by a significant reduction in root mean square error (rmse) and an enhanced r-squared (r²) value, thereby affirming its superior accuracy and reliability in forecasting co2 adsorption. a pivotal aspect of our research is the integration of shap values for a detailed assessment of feature importance, which not only corroborated 'pressure' and 'surface area' as pivotal determinants of co2 uptake but also illuminated the model's advanced analytical capabilities in handling categorical features and mitigating overfitting, even within a dataset marked by intricate and non-linear patterns. our quantitative and conceptual analysis, showcasing up to a 15% improvement in (rmse) over previous models, reveals the catboost model’s unparalleled efficiency in discerning the multifaceted interplay of factors influencing co2 adsorption. this is crucial for the strategic engineering of mofs with optimized properties. beyond 'pressure' and 'surface area', our shap analysis highlighted other descriptors with substantial values, elucidating their nuanced contributions to co2 uptake and providing invaluable insights for the mof design process. through this work, we aim to foster a deeper understanding and application of ml algorithms in environmental sustainability, thereby building upon the foundational research of abdi et al. and pushing the boundaries of machine learning applications in the field."
https://doi.org/10.26434/chemrxiv-2024-7zqx1,2024-02-22,quasi-degenerate extension of local n-electron valence state perturbation theory with pair-natural orbital method based on localized virtual molecular orbitals,Manami Hayashi Masaaki Saitow Kazuma Uemura Takeshi Yanai,"chemical phenomena involving near-degenerate electronic states, such as conical intersections or avoided crossing, can be properly described using quasi-degenerate perturbation theory (qdpt). this study proposed a highly scalable quasi-degenerate second-order n-electron valence state perturbation theory (qd-nevpt2) using the local pair-natural orbital (pno) method. our recent study showed an efficient implementation of the pno-based state-specific (ss) nevpt2 method using orthonormal localized virtual molecular orbitals (lvmos) as an intermediate local basis. this study derived the state-coupling (or off-diagonal) terms to implement qd-nevpt2 in an alternative manner to enhance efficiency based on the internally contracted basis (icb) and pno overlap matrices between different references. to facilitate further acceleration, a local resolution-of-the-identity (ri) three-index integral generation algorithm was developed using lmos and lvmos. although the nevpt2 theory is considered to be less susceptible to the intruder-state problem (isp), this study revealed that it can easily suffer from isp when calculating high-lying excited states. we ameliorated this instability using the imaginary level shift (ls) technique. the pno-qd-nevpt2 calculations were performed on small organic molecules for the 30 lowest-lying states, as well as photoisomerization involving the conical intersection of 1,1-dimethyldibenzo[b,f] silepin with a cis-stilbene skeleton. these calculations revealed that the pno-qd-nevpt2 method yielded negligible errors compared to the canonical qd-nevpt2 results. furthermore, we tested its applicability to a large photoisomerization system using the green fluorescent protein model and the 10-state calculation of the large transition metal complex, showcasing that off-diagonal elements can be evaluated at a relatively low cost."
https://doi.org/10.26434/chemrxiv-2024-nzt5j-v2,2024-02-22,"bat2: an open-source tool for flexible, automated and low cost absolute binding free energy calculations",Germano Heinzelmann David Huggins Michael Gilson,"absolute binding free energy (abfe) calculations with all-atom molecular dynamics (md) have the potential to greatly reduce costs in the first stages of drug discovery. here we introduce bat2, the new version of the binding affinity tool (bat.py), designed to combine full automation of abfe calculations with high-performance md simulations, making it a potential tool for virtual screening. we describe and test several changes and new features that were incorporated into the code, such as relative restraints between the protein and the ligand instead of using fixed dummy atoms, sup- port for the openmm simulation engine, a merged approach to the application/release of restraints, support for cobinders and proteins with multiple chains, and many others. we also reduced the simulation times for each abfe calculation, assessing the effect on the expected robustness and accuracy of the calculations."
https://doi.org/10.26434/chemrxiv-2024-r67mx,2024-02-21,the interplay of solvation and polarization effects on ion pairing in nanoconfined electrolytes,Kara Fong Barbara Sumic Niamh O'Neill Christoph Schran Clare Grey Angelos Michaelides,"the nature of ion-ion interactions in electrolytes confined to nanoscale pores has important implications for energy storage and separations technologies. however, the physical effects dictating the structure of nanoconfined electrolytes remain debated. here we employ machine learning-based molecular dynamics simulations to investigate ion-ion interactions with density functional theory-level accuracy in a prototypical confined electrolyte, aqueous nacl within graphene slit pores. we find that the free energy of ion pairing in highly confined electrolytes deviates substantially from that in bulk solutions, observing a decrease in contact ion pairing but an increase in solvent-separated ion pairing. these changes arise from an interplay of ion solvation effects and graphene's electronic structure. notably, the behavior observed from our first-principles-level simulations is not reproduced even qualitatively with the classical force fields conventionally used to model these systems. the insight provided in this work opens new avenues for predicting and controlling the structure of nanoconfined electrolytes."
https://doi.org/10.26434/chemrxiv-2024-60tc7,2024-02-21,alphafold meets de novo drug design: leveraging structural protein information in multi-target molecular generative models,Andrius Bernatavicius Martin Šícho Antonius Janssen Alan Kai Hassen Mike Preuss Gerard van Westen,"advances in deep learning have expanded the applications of virtual screening for drug-like compounds. more recently generative models have emerged as sources of inspiration for chemists. we introduce a multi-target model, pcmol, that leverages the latent embeddings derived from alphafold as a means of conditioning the de novo generative model on target proteins. it is known that the addition of protein descriptors is an effective strategy to extend the applicability domain and prediction capability of quantitative structure-activity relation (qsar) models, a strategy we refer to as proteochemometrics (pcm). similarly, the use of alphafold latent embeddings within a generative model for small molecules allows it to leverage structural relationships between proteins. this opens up new possibilities such as interpolation within the chemical space of known highly active compounds and extrapolation on the target side based on their similarities to other proteins, which is especially relevant for understudied or novel targets. our results indicate that pcmol can generate diverse, potentially active molecules for a wide array of proteins, including those with sparse ligand bioactivity data. we also benchmark against existing target-conditioned trans-former models to illustrate the validity of using alphafold protein representations to steer the molecular generation process and increase the generalization capabilities to unseen targets. additionally, we demonstrate the important role of data augmentation in bolstering the performance of generative models in low-data regimes. the open-source package along with a dataset of alphafold protein embeddings is available at https://github.com/cddleiden/pcmol."
https://doi.org/10.26434/chemrxiv-2024-rw2rr,2024-02-20,towards understanding trans-cleavage of natural and synthetic nucleic acids by cas12a for sensitive crispr biosensing,Fei Deng Rui  Sang  Yi  Li  Danting  Yang  Wei  Deng  Ewa  Goldys ,"crispr/cas systems have been widely utilized for the development of biosensing platforms for precision molecular diagnostics. their remarkable biosensing performance critically depends on the efficiency of sequence-independent trans-cleavage in type v and vi cas effectors. cas12a, a typical example of type v cas effector exhibits varying trans-cleavage efficiency on different types of nucleic acids, and also in response to different nucleobase sequences. however, the underlying mechanism of cas12a’s trans-cleavage characteristic remains unclear. to explore this mechanism, we introduced xeno nucleic acids (xna) as potential trans-cleavage substrates of cas12a. xnas are chemically modified nucleic acid analogues, which originate from chemical modifications of nucleobases, sugar moieties, and the backbone. we observed a progressive decrease in trans-cleavage rates by cas12a across different types of xnas, in the following sequence: nucleobase-modified xna > sugar moiety-modified xna > backbone-modified xna. in addition, more complex chemical modifications on either of the three above locations led to the lowering of the trans-cleavage rate of cas12a. these findings elucidate the mechanism behind cas12a’ trans-cleavage characteristic, which is attributed to varying molecular complexity of the sugar moieties and nucleobases. based on these findings, we also developed a colorimetric crispr/cas12a biosensing system utilizing xna for the detection of circulating tumor dna (ctdna), with a limit of detection of 10 pm and a 4 logs detection range from 10 pm to 100 nm. these results indicate that xna can serve as a novel cas12a trans-cleavage target for sensitive biosensing applications. "
https://doi.org/10.26434/chemrxiv-2024-8hfnv,2024-02-20,carbon-negative production of hydrogen through sulfur intermediates,Benjamin Bachman Robert  Hamers,"chemical fuel production from biomass represents one method for decarbonizing the global energy infrastructure. however, current technologies have significant drawbacks that limit application to select feedstocks. for example, the endothermic steam-reforming process is limited to gasified low-sulfur feedstocks that have thus far precluded the economic utilization of municipal solid wastes (msw) among other sources of biomass. the use of elemental s is one potential method to utilize these organic wastes for h2 production through the high-temperature and exothermic production of hydrogen sulfide (h2s) and carbon disulfide (cs2) gasses as chemical intermediates. these reduced sulfur species are thermodynamically unstable with respect to their oxygen-analogs, which indicates that usable chemical energy may be derived from their oxidation. when biomass is dehydrogenated with s at lower temperatures, the formation of cs2 is suppressed and sulfurized biochar is produced. in this work, we describe and analyze a possible cyclic route to producing h2 from these species through the use of well-known chemical reactions operating in continuous fashion using cellulose as an example molecule. this “sulfur-reforming” process and additional steps may allow for the economical valorization of currently unutilized organic matter in msw that may be contributing to environmental harm. if the sulfurized biochar is left sequestered, such as through use as a soil-amendment, and if the electrical energy is sourced from renewables, this process may be considered carbon-negative.  "
https://doi.org/10.26434/chemrxiv-2024-h2xgs,2024-02-20,property-guided generation of complex polymer topologies using variational autoencoders,Shengli Jiang Adji Bousso Dieng Michael Webb,"the complexity and diversity of polymer topologies, or chain architectures, present substantial challenges in predicting and engineering polymer properties. although machine learning is increasingly used in polymer science, applications to address architecturally complex polymers are nascent. here, we use a generative machine learning model based on variational autoencoders and data generated from molecular dynamics simulations to design polymer topologies that exhibit target properties. following the construction of a dataset featuring 1,342 polymers with linear, cyclic, branch, comb, star, or dendritic structures, we employ a multi-task learning framework that effectively reconstructs and classifies polymer topologies while predicting their dilute-solution radii of gyration. this framework enables the generation of novel polymer topologies with target size, which is subsequently validated through molecular simulation. these capabilities are then exploited to contrast rheological properties of topologically distinct polymers with otherwise similar dilute-solution behavior. this research opens new avenues for engineering polymers with more intricate and tailored properties with machine learning."
https://doi.org/10.26434/chemrxiv-2024-7q438,2024-02-20,when do quantum mechanical descriptors help graph neural networks predict chemical properties?,Shih-Cheng Li Haoyang Wu Angiras Menon Kevin Spiekermann Yi-Pei Li William Green,"deep graph neural networks are extensively utilized to predict chemical reactivity and molecular properties. however, because of the complexity of chemical space, such models often have difficulty extrapolating beyond the chemistry contained in the training set. augmenting model with quantum mechanical (qm) descriptors is anticipated to improve its generalizability. however, obtaining qm descriptors often requires cpu-intensive computational chemistry calculations. to identify when qm descriptors help graph neural networks predict chemical properties, we conduct a systematic investigation of the impact of atom, bond, and molecular qm descriptors on the performance of directed message passing neural networks (d-mpnns) for predicting 16 molecular properties. the analysis surveys computational and experimental targets, classification and regression tasks, and varied dataset sizes from several hundred to hundreds of thousands of datapoints. our results indicate that qm descriptors are mostly beneficial to d-mpnn performance on small datasets, provided that the descriptors correlate well with the targets and can be readily computed at high accuracy. otherwise, using qm descriptors can add cost without benefit or even introduce unwanted noise that can degrade model performance. strategic integration of qm descriptors with d-mpnn unlocks potential for physics-informed, data-efficient modeling with some interpretability that can streamline de novo drug and material designs. to facilitate the use of qm descriptors in machine learning workflows for chemistry, we provide a set of guidelines regarding when and how to best leverage qm descriptors, a high-throughput workflow to compute them, and an enhancement to chemprop, a widely adopted open-source d-mpnn implementation for chemical property prediction."
https://doi.org/10.26434/chemrxiv-2024-3wdwv,2024-02-20,improved estimates of folding stabilities and kinetics with multiensemble markov models,Si Zhang Yunhui Ge Vincent Voelz,"markov state models (msms) have been widely applied to understand folding mechanisms and predict long timescale dynamics from ensembles of short molecular simulations. most msm estimators enforce detailed balance, assuming that trajectory data is sampled at equilibrium.  this is rarely the case for ab initio folding studies, however, and as a result, msms can severely underestimate protein folding stabilities from such data.  to remedy this problem, we have developed an enhanced-sampling protocol in which  (1)  unbiased folding simulations are performed and sparse tica is used to obtain features that best capture the slowest events in folding, (2)  umbrella sampling along this reaction coordinate is performed to observe folding and unfolding transitions, and (3)  the thermodynamics and kinetics of folding are estimated using multiensemble markov models (memms). using this protocol, folding pathways, rates, and stabilities of a designed alpha-helical hairpin, z34c, can be predicted in good agreement with experimental measurements.  these results indicate that accurate simulation-based estimates of absolute folding stabilities are within reach, with implications for the computational design of folded mini-proteins and peptidomimetics."
https://doi.org/10.26434/chemrxiv-2024-5tzsf,2024-02-20,analytic gradients for the electrostatic embedding qm/mm in periodic boundary conditions using particle-mesh ewald sums and electrostatic potential fitted charge operators,Simone Bonfrate Nicolas Ferré Miquel Huix-Rotllant,"long-range electrostatic effects are fundamental for describing chemical reactivity in the condensed phase. here, we present the methodology of an efficient quantum mechanical/molecular mechanical (qm/mm) model in periodic boundary conditions (pbc) compatible with qm/mm boundaries at chemical bonds. the method combines electrostatic potential fitted (espf) charge operators and electrostatic potentials derived from the smooth particle-mesh ewald (pme) sum approach. the total energy and its analytic first derivatives with respect to qm, mm and lattice vectors allow qm/mm molecular dynamics (md) in the most common thermodynamic ensembles. we demonstrate the robustness of the method by performing a qm/mm md equilibration of methanol in water. we simulate the cis/trans isomerization free energy profiles in water of proline amino acid and a proline-containing oligopeptide, showing a correct description of the reaction barrier. our pbc-compatible qm/mm model can efficiently be used to study chemical reactivity in condensed phase and enzymatic catalysis."
https://doi.org/10.26434/chemrxiv-2024-sbv85-v2,2024-02-20,completely solvent-free synthesis of double heterohelicenes and their further ring fusion using mechanochemical reaction,Honoka Sada Daisuke Sakamaki Masayuki Gon Kazuo Tanaka Takashi Hirose Hideki Fujiwara,"in this study, we developed a simple and efficient method for synthesizing double heterohelicenes (dhhs) composed of two heteroacenes bearing an nh group, such as benzo[b]phenoxazine (bpo) and dibenzo[b,i]phenoxazines (dbpo), using mechanochemical oxidative c–n coupling reactions, allowing complete solvent-free synthesis from commercially available compounds. our new synthetic method afforded more than 1 g of dhh, which has a high dissymmetry factor for circularly polarized luminescence (gcpl) of > 1 × 10−2, in a one-pot mechanochemical reaction using bpo as a reactant. in addition, mechanochemical oxidative coupling also allows for further fusion reactions of dhhs, leading to semi- or fully planarized molecules, which have not been previously achieved through solution-phase reactions. we isolated semi-planarized heterohelicenes 5 and 6 and determined their structures using single-crystal x-ray analysis. compounds 5 and 6 exhibited enhanced electron donor properties compared to dhhs 3 and 4. the enantiomers of 6 exhibited clear cpl emissions with a |gcpl| value of 2 × 10−3. the magnitudes of the transition magnetic dipole moment (tmdm) of 5 and 6 increased compared to those of 3 and 4. transition moment density analysis revealed that large tmdm densities appeared on the newly formed c–c bonds, providing a unique molecular design guideline for enhancing the magnitude of the tmdm without expanding the molecular structure. "
https://doi.org/10.26434/chemrxiv-2024-kdx9r,2024-02-20,polycyclic aromatic hydrocarbon based electrical sensor: dimethylamine substituted alkynylated anthracene for h2 detection,Khadimul Islam Thomas Daniel Roy Paily Akshai Kumar,"the demand for hydrogen is on a continuous rise in view of its application as a clean-burning and alternate carbon-free energy source. it is flammable at concentration above 4 % in air and is odorless. fabrication of highly sensitive and selective hydrogen sensors based on small organic molecules which operate at room temperature is challenging. this work describes the fabrication of a hydrogen sensor containing -conjugated organic semiconductor based on a n,n-dimethylamine substituted tetraalkynylatedanthracene that can detect h2 at concentrations as low as 150 part per million (ppm) at room temperature. the n,n-dimethyl amine containing tetraalkynylatedanthracene (anphnme2) has been synthesized by tetra-fold sonogashira reaction employing a catalyst system based on pd(ch3cn)2cl2 + catacxium® a. a precisely con-trolled fabrication is enabled by employing a μ-gridder printing system. the gas sensor shows excellent sensitivity, fast response and high recovery to h2 at room temperature. moreover, after the interaction with h2, the surface electron of the anphnme2 gets enhanced and shows a decrease in the resistance of the fabricated device.  sensor exhibits a limit of detection of 49 ppb with the highest sensitivity of 19.95% for the detection of 900 ppm of h2, with a response time of 10 to 20 seconds. this work aims to develop a proof-of-concept for enhancing room temperature hydrogen sensing by developing a low cost printed sensor."
https://doi.org/10.26434/chemrxiv-2024-6ktbm,2024-02-20,rapid preparation of beta-ketoenamine-based covalent organic frameworks (cofs) via amino-yne click polymerization,Wei Bai Qi Tian Xinyao Fu Anjun Qin Ben Zhong Tang,"as a new class of crystalline porous organic polymer materials, covalent organic frameworks (cofs) have permanent porosity and broad application prospects. the synthesis of cofs has strict requirements for both the reaction equations and the reaction conditions. therefore, it is critical for researchers to develop new, scalable synthetic reactions. in this work, for the first time, we report an in-situ synthetic strategy toward cofs via the amino-yne click polymerization by taking advantage of its produced dynamic beta-ketoenamine bond. this strategry also enjoys the advantages of high efficiency and atomic economy. the crystallinity and bet measurements indicated that the resultant cofs have excellent crystallinity and high porosity. thus, this work not only provides a new strategy for the rapid preparation of cofs with excellent atom economy, but also enriches their family."
https://doi.org/10.26434/chemrxiv-2024-3fw8h,2024-02-19,insect larvae oil: a high value excipient for lipid-based nanocarriers to tackle atopic dermatitis,Cíntia J. Almeida Rossana Roque João Vieira Ana  Júlio Nuno Saraiva Catarina Pereira-Leite Catarina Rosado,"atopic dermatitis (ad) is a chronic inflammatory skin disorder with a complex pathogenesis involving epidermal barrier dysfunction and aberrant lipid composition, particularly ceramides and fatty acids (fa). conventional management options, such as topical glucocorticoids (gc), often lead to adverse effects upon prolonged usage, necessitating the exploration of alternative therapeutic strategies. this study investigated the potential of utilizing novel nanotechnology-based formulations to enhance the topical management of ad. specifically, we explored the use of solid lipid nanoparticles (sln) formulated with insect larvae oil as a carrier for dexamethasone (dex), a representative gc, and as an adjunctive emollient to support skin barrier repair. the lipidic fraction of black soldier fly larvae biomass, holding a rich blend of fa, holds substantial potential as a novel ingredient to tackle skin barrier impairment. through systematic optimization using box-behnken design, insect larvae oil-based sln demonstrated favorable physicochemical properties for topical application and satisfactory stability over 2 months. notably, these sln exhibited a favorable drug release kinetics, delivering the total dex payload within a therapeutically relevant timeframe. furthermore, these sln showed ability to permeate human keratinocytes without pronounced toxicity, suggesting their potential utility in enhancing drug delivery and cellular uptake. overall, our findings suggest that insect larvae oil is a promising natural and sustainable ingredient for the development of nanotechnology-driven approaches to ad management, offering a potential avenue for addressing the unmet needs in this challenging dermatologic condition."
https://doi.org/10.26434/chemrxiv-2024-j067j,2024-02-19,electrospinning as fascinating platform for teaching applied polymer science with safe and sustainable experiments,Jessica Noll Felix Leven Johannes Limberg Christoph Weidmann Rainer Ostermann,"electrospinning has been widely used as versatile technique to generate nanofibers of various materials. it is also helpful in teaching topics ranging from macromolecular chemistry to physics and safety to sustainability at various levels of difficulty and student involvement. simple and safe hands-on experiments/manual assays can be realized for less than 20 euros to demonstrate polymer viscosity and nanofiber alignment and solubility. students can further study (super)hydrophobicity and even upcycle packaging waste into useful filter materials, but also improve the electrospinning setup from a manual assay to an inexpensive arduino-based 3d printed research platform. alternatively, the latter can be used for teacher demonstrations of more challenging experiments that can also be easily done using a commercial syringe-pump."
https://doi.org/10.26434/chemrxiv-2024-n10tr-v2,2024-02-19,"monte carlo simulations of water pollutant adsorption at parts-per-billion concentration: a study on 1,4-dioxane",Samiha Sharlin Rodrigo Lozano Tyler R. Josephson,"1,4-dioxane is an emerging water pollutant with high production volumes and a probable human carcinogen. the incompetence of conventional treatment processes demonstrates a need for an effective remediation strategy. crystalline nanoporous materials are cost-effective adsorbents due to their high capacity and selective separation in mixtures. this study explores the potency of all-silica zeolites. these zeolites are highly hydrophobic and can preferentially adsorb nonpolar molecules from mixtures. we investigated six zeolite frameworks (bea, euo, fer, ifr, mfi, mor) using monte carlo simulations in the gibbs ensemble. the simulations indicate high selectivity by fer and euo, especially at low pressures, which we attribute to pore sizes and shapes with more affinity to 1,4-dioxane. we also demonstrate a monte carlo simulation workflow using gauge cells to model the adsorption of an aqueous solution of 1,4-dioxane at 0.35 ppb concentration. we quantify 1,4-dioxane and water coadsorption and observe selectivities ranging from 1.1 x 10^5 in mor to 8.7 x 10^6 in fer. we also demonstrate that 1,4-dioxane is in the infinite dilution regime in both the aqueous and adsorbed phases at this concentration. this simulation technique can be extended to model other emerging water contaminants such as per- and polyfluoroalkyl substances (pfas), chlorates, and others, which are also found in extremely low concentrations. "
https://doi.org/10.26434/chemrxiv-2024-k36mr,2024-02-16,cyclodehydrogenation catalyzed by atomic hydrogen,Rafal Zuzak Pawel Dabczynski Jesús  Castro-Esteban José  Ignacio Martínez Mads Engelund Dolores Pérez Diego Peña Szymon Godlewski,"atomically precise synthesis of nanographenes and graphene nanoribbons on semiconductors and insulators has been a formidable challenge. in particular, the metallic substrates needed to catalyze cyclodehydrogenative planarization reactions of precursor molecules limit subsequent applications that exploit the electronic structure of nanographenes. we demonstrate that, counterintuitively, atomic hydrogen can play the role of a catalyst in the cyclodehydrogenative planarization reaction regardless of the substrate type. the high efficiency of the method was demonstrated by the nanographene synthesis on metallic au, semiconducting tio2, as well as on inert and insulating si/sio2 and thin nacl layers."
https://doi.org/10.26434/chemrxiv-2024-2rlk7,2024-02-16,qptuna: an automated qsar modelling platform for molecular property prediction in drug design,Lewis Mervin Alexey Voronov Mikhail Kabeshov Ola Engkvist,"machine-learning (ml) and deep-learning (dl) approaches to predict the molecular properties of small molecules are increasingly deployed within the design-make-test-analyse (dmta) drug design cycle to predict molecular properties of interest. despite this uptake, there are only a few automated packages to aid their development and deployment that also support uncertainty estimation, model explainability and other key aspects of model usage. this represents a key unmet need within the field and the large number of molecular representations and algorithms (and associated parameters) means it is non-trivial to robustly optimise, evaluate, reproduce, and deploy models. here we present qptuna, a molecule property prediction modelling pipeline, written in python and utilising the optuna, scikit-learn, rdkit and chemprop packages, which enables the efficient and automated comparison between molecular representations and machine learning models. the platform was developed considering the increasingly important aspect of model uncertainty quantification and explainability by design. we provide details for our framework and provide illustrative examples to demonstrate the capability of the software when applied to simple molecular property, reaction/reactivity prediction and dna encoded library enrichment analyses. we hope that the release of qptuna will further spur innovation in automatic ml modelling and provide a platform for education of best practises in molecular property modelling. the code to the qptuna framework is made freely available via github."
https://doi.org/10.26434/chemrxiv-2024-p5t3l,2024-02-16,curator: building robust machine learning potentials for atomistic simulations autonomously with batch active learning,Xin Yang Martin Hoffmann Petersen Renata Sechi William Sandholt Hansen Sam Walton Norwood Yogeshwaran Krishnan Smobin Vincent Jonas Busk Francois Raymond J  Cornet Ole Winther Juan Maria Garcia Lastra Tejs Vegge Heine Anton Hansen Arghya Bhowmik,"to enable fast, resource efficient development and broad scale deployment of of high accuracy machine-learned interatomic potentials (mlips) with minimum expert involvement, we introduce curator, an autonomous batch active learning workflow for constructing mlips. curator integrates state of the art models, uncertainty quantification techniques, batch selection algorithms with user defined labeling and chemical-structure space exploration methods for data and compute efficient active learning. we also developed a novel efficient gradient computation method that calculates forces and stress based on the energy derivative with respect to accelerate curator. our evaluation across different chemical systems demonstrates that curator considerably reduces the computational resources and time required to develop reliable mlips. in practical applications in novel complex materials and interfaces, curator shows promising results, underscoring its potential in accelerating materials discovery. the flexibility and efficiency of curator mark a significant advancement in the field of computational materials science, paving the way for more efficient and larger time-length scale atomistic simulations."
https://doi.org/10.26434/chemrxiv-2024-r75jz,2024-02-16,stable and accurate atomistic simulations of flexible molecules using conformationally generalisable machine learned potentials,Christopher D Williams Jas Kalayan Neil A Burton Richard A Bryce,"computational simulation methods based on machine learned potentials (mlps) promise to revolutionise shape prediction of flexible molecules in solution, but their widespread adoption has been limited by the way in which training data is generated. here, we present an approach which allows the key conformational degrees of freedom to be properly represented in reference molecular datasets. mlps trained on these datasets using a global descriptor scheme are generalisable in conformational space, providing quantum chemical accuracy for all conformers. these mlps are capable of propagating long, stable molecular dynamics trajectories, an attribute that has remained a challenge for mlps. we deploy the mlps in obtaining converged conformational free energy surfaces for flexible molecules via well-tempered metadynamics simulations; this approach provides a hitherto inaccessible route to accurately computing the structural, dynamical and thermodynamical properties of a wide variety of flexible molecular systems. "
https://doi.org/10.26434/chemrxiv-2024-5v6gh,2024-02-16,a physics-aware neural network for protein-ligand interactions with quantum chemical accuracy,Zachary Glick Derek Metcalf Caroline Sargent Steven Spronk Alexios Koutsoukas Daniel Cheney C. David Sherrill,"quantifying intermolecular interactions with quantum chemistry (qc) is useful for many chemical problems, including understanding the nature of protein-ligand interactions. unfortunately, qc computations on protein-ligand systems are too computationally expensive for most use cases. the flourishing field of machine-learned (ml) potentials is a promising solution, but it is limited by an in- ability to easily capture long range, non-local interactions. in this work we develop an atomic-pairwise neural network (ap-net) specialized for modeling intermolecular interactions. this model benefits from a number of physical constraints, including a two-component equivariant message passing neural network architecture that predicts interaction energies via an intermediate prediction of monomer electron densities. the ap-net model also benefits from a comprehensive training dataset com- posed of paired ligand and protein fragments. this model accurately predicts qc-quality interaction energies of protein-ligand systems at a computational cost reduced by orders of magnitude."
https://doi.org/10.26434/chemrxiv-2023-jp112-v3,2024-02-16,differentiable programming for kohn-sham dft: energy derivatives with respect to the parameters of exchange-correlation functional at linear cost,Evgeny M. Kadilenko Roland Grinis,"we applied the adjoint method to kohn-sham equations taking the energy derivatives with respect to the parameters of the exchange-correlation functional.  the results obtained are completely consistent with the hellmann-feynman theorem and can serve as a starting point for the study of analytical derivatives of other quantities obtained using the kohn-sham method.  the prototype we created on the basis of python package pyscf exhibited linear complexity performance behavior and showed a significant speedup in the calculation of the derivative compared to the automatic differentiation approach such as pytorch based dqc. in a more severe sense, the development of efficient methods for computing derivatives with respect to parameters of scientific models is important in applications to machine learning where training is done via gradient-based optimization algorithms or the model is integrated with deep learning and there is a need to speed up the calculation in the backpropagation pass. "
https://doi.org/10.26434/chemrxiv-2024-gfgsm-v2,2024-02-15,selected ion monitoring for orbitrap-based metabolomics,Wenyun Lu Matthew J. McBride Won Dong Lee Xi Xing Xincheng Xu Anna M. Oschmann Xi Li Caroline Bartman Yihui Shen Joshua D. Rabinowitz,"orbitrap mass spectrometry in full scan mode enables simultaneous detection of hundreds of metabolites and their isotope-labeled forms. yet sensitivity remains limiting for many metabolites, including low concentration species, poor ionizers, and low fractional abundance isotope-labeled forms in isotope tracing studies. here we explore selected ion monitoring (sim) as a means of sensitivity enhancement. the analytes of interest are enriched in the orbitrap analyzer by using the quadrupole as a mass filter to select particular ions. in tissue extracts, sim significantly enhances the detection of ions of low intensity as indicated by improved signal-to-noise (s/n) ratios and measurement precision. in addition, sim improves the accuracy of isotope-ratio measurements. sim, however, must be deployed with care, as excessive accumulation in the orbitrap of similar m/z ions can lead, via space charge effects, to decreased performance (signal loss, mass shift, ion coalescence). ion accumulation can be controlled by adjusting settings including injection time and target ion quantity. overall, we suggest using full scan to ensure broad metabolic coverage, in tandem with sim for accurate quantitation of targeted low-intensity ions, and provide methods deploying this approach to enhance metabolome coverage. "
https://doi.org/10.26434/chemrxiv-2024-9xh38,2024-02-14,beyond predefined ligand libraries: a genetic algorithm approach for de novo discovery of catalysts for the suzuki coupling reactions,Julius Seumer Jan H. Jensen,"this study introduces a novel approach for the unrestricted de novo design of transition metal catalysts, leveraging the power of genetic algorithms (gas) and density functional theory (dft) calculations. by focusing on the suzuki reaction, known for its significance in forming carbon-carbon bonds, we demonstrate the effectiveness of fragment-based and graph-based genetic algorithms in identifying novel ligands for palladium-based catalytic systems. our research highlights the capability of these algorithms to generate ligands with desired thermodynamic properties, moving beyond the restriction of enumerated chemical libraries. limitations in the applicability of machine learning models are overcome by calculating thermodynamic properties from first principle. the inclusion of synthetic accessibility scores further refines the search, steering it towards more practically feasible ligands. through the examination of both palladium and alternative transition metal catalysts like copper and silver, our findings reveal the algorithms' ability to uncover unique catalyst structures within the target energy range, offering insights into the electronic and steric effects necessary for effective catalysis. this work not only proves the potential of genetic algorithms in the cost-effective and scalable discovery of new catalysts but also sets the stage for future exploration beyond predefined chemical spaces, enhancing the toolkit available for catalyst design."
https://doi.org/10.26434/chemrxiv-2024-82644,2024-02-14,can we achieve atmospheric chemical environments in the laboratory? an integrated model-measurement approach to chamber soa studies,Hannah Kenagy Colette Heald Nadia Tahsini Matthew Goss Jesse Kroll,"secondary organic aerosol (soa), atmospheric particulate matter formed from low-volatility products of volatile organic compound (voc) oxidation, impacts both air quality and climate. current 3d models, however, cannot reproduce the observed variability in atmospheric organic aerosol. because many soa model descriptions are derived from environmental chamber experiments, our ability to represent atmospheric conditions in chambers directly impacts our ability to assess the air quality and climate impacts of soa. here, we develop a new approach that leverages global modeling and detailed mechanisms to design chamber experiments that mimic the atmospheric chemistry of organic peroxy radicals (ro2), a key intermediate in voc oxidation. drawing on decades of laboratory experiments, we develop a framework for quantitatively describing ro2 chemistry and show that no previous experimental approaches to studying soa formation have accessed the relevant atmospheric ro2 fate distribution. we show proof-of-concept experiments that demonstrate how soa experiments can access a range of atmospheric chemical environments and propose several directions for future studies."
https://doi.org/10.26434/chemrxiv-2024-4j0rx,2024-02-14,synthesis of substituted triazole-pyrazole hybrids using triazenopyrazoles precursors,Simone Graessle Laura Holzhauer Nicolai Wippert olaf Fuhr Martin Nieger Nicole Jung Stefan Bräse,"a synthesis route to access triazole-pyrazole hybrids via pyrazolotriazenes was developed. contrary to existing methods, this route allows the facile n-functionalization of the pyrazole before the attachment of the triazole unit via a copper-catalyzed azide-alkyne cycloaddition. the developed methodology was used to synthesize a library of over fifty novel multi-substituted pyrazole-triazole hybrids. we could also demonstrate a one-pot strategy that renders the isolation of potentially hazardous azides obsolete. in addition, the compatibility of the method with solid-phase synthesis was shown exemplarily."
https://doi.org/10.26434/chemrxiv-2024-0754c,2024-02-13,unlocking the antiviral arsenal: multiparametric optimization of small-molecule inhibitors against rsv and hcov-229e,Christina Karhan Svenja Mareike Sake Antonia Patricia Gunesch Christina Grethe Benedikt Hellwinkel Alexander Felix Kiefer Uladzislau Hapko Andreas Martin Kany Thomas Pietschmann Anna Katharina Herta Hirsch,"acute respiratory diseases in humans can be caused by various viral pathogens such as respiratory syncytial virus (rsv), human coronavirus 229e (hcov-229e), and severe acute respiratory syndrome coronavirus 2 (sarscov- 2). to prevent severe cases by an early treatment, one effective strategy is to inhibit viral infection at the entry stage of the replication cycle. however, there is a lack of efficient, fda-approved small molecule drugs targeting these pathogens. previously, we identified two dual rsv/hcov-229e small molecule inhibitors with activity in the single-digit micromolar range. in this study, we focused on optimizing the more promising starting point using a multiparametric hit optimization approach. here, we present the results, including valuable insights into the structure activity relationship (sar), and report the discovery of a submicromolar rsv entry inhibitor and a highly potent compound against hcov-229e."
https://doi.org/10.26434/chemrxiv-2024-7z779,2024-02-13,ph stimuli responsive dextran gated mesoporous silica as a viable delivery vehicle,Yusuf Olatunji WAIDI,"mesoporous materials hold immense potential due to their diverse applications (catalysis, separation, drug delivery, etc.) viability. to unlock this potential, controlling the transport of molecules within their nanochannels is crucial. this study explores a novel technique to manipulate pore properties by hetero-functionalizing mesoporous silica with carboxylic acid and propylamine groups, creating ph-responsive surfaces. the negatively charged surface at basic ph attracts and loads a cationic dye through electrostatic interactions. this cargo can be efficiently released by switching to acidic ph, reversing the surface charge. furthermore, the system incorporates dextran as a ""gatekeeper"" for controlled release. the study demonstrates significant differences in release profiles between functionalized materials, highlighting the effectiveness of this approach. notably, the functionalization method strategy using 1,4-dioxane and appropriate reaction timing enables the desired properties for the first time. this work paves the way for designing advanced mesoporous materials with tailored functionalities for various applications."
https://doi.org/10.1101/2023.07.31.23293324,2023-12-20,public awareness of and opinions on the use of mathematical transmission modelling to inform public health policy in the united kingdom,"['McCabe, R.; Donnelly, C. A.']","mathematical transmission modelling is a key component of scientific evidence used to inform public health policy and became particularly prominent during the covid-19 pandemic. as key stakeholders, it is vital that the public perception of this set of tools is better understood. to complement a previously published article on the science-policy interface by the authors of this study, novel data were collected via responses to a survey via two methods: via an online panel (""representative"" sample) and via social media (""non-probability"" sample). many identical questions were asked separately for the period ""prior to"" compared to ""during"" the covid-19 pandemic.  all respondents were increasingly aware of the use of modelling in informing policy during the pandemic, with significantly higher levels of awareness among social media respondents than online panel respondents. awareness generally stemmed from the news media and social media during the pandemic. transmission modelling informing public health policy was perceived as more reliable during the pandemic compared to the pre-pandemic period in both samples, with awareness being positively associated with reliability within both samples and time points, except for social media during the pandemic. trust in government public health advice remained high across samples and time periods overall but was lower in the period of the pandemic compared to the pre-pandemic period. the decay in trust was notably greater among social media respondents. many respondents from both samples explicitly made the distinction that their trust was reserved for ""scientists"" and not ""politicians"". almost all respondents, regardless of sample, believed governments have responsibility for the communication of modelling to the public.  these results provide an important reminder of the potentially skewed conclusions that could be drawn from non-representative samples."
https://doi.org/10.1101/2023.09.23.23296012,2024-02-08,"micrometer-thick, porous, nanocomposite coating for electrochemical sensors with exceptional antifouling and electroconducting properties","['Lee, J.-C.; Kim, S. Y.; Song, J.; Jang, H.; Kim, H.; Choi, S. Q.; Kim, S.; Jolly, P.; Kang, T.; Park, S.; Ingber, D. E.']","development of coating technologies for electrochemical sensors that consistently exhibit antifouling activities when exposed to diverse and complex biological environments over extended time is vital for development more effective medical devices and diagnostics. here, we describe a micrometer-thick, porous nanocomposite coating with both exceptional antifouling and electroconducting properties that greatly enhance the sensitivity of electrochemical sensors. nozzle-assisted printing of oil-in-water emulsion is used to create a 1 micrometer thick coating composed of cross-linked albumin with interconnected pores, which also contains electroconducting gold nanowires. using this approach, the antifouling conductive coating can be deposited only on the surface of the working electrode, and not on the reference and counter electrodes, which greatly facilitates the fabrication and functionality of multiplexed electrochemical sensors. the layer effectively resists biofouling and maintains rapid electron transfer kinetics for over one month when exposed directly to complex biological fluids, including serum and nasopharyngeal secretions. compared to previously described thinner (nanometer thick) antifouling electroconductive coating made with drop casting or a spin coating of the same thickness, the nozzle-printed sensors coated with this thick porous nanocomposite exhibited sensitivities that were enhanced by 3.75- to 17-fold when three different target biomolecules were tested. as a result, emulsion-coated, multiplexed electrochemical sensors coated were able to carry out simultaneous detection of severe acute respiratory syndrome coronavirus 2 (sars-cov-2) nucleic acid, antigen, and host antibody in clinical specimens with high sensitivity and specificity. this thick porous emulsion coating technology may provide a way to address hurdles currently restricting the application of electrochemical sensors for point-of-care (poc) diagnostic applications, as well as their use in implantable devices and other healthcare monitoring systems."
https://doi.org/10.1101/2023.01.06.23284282,2024-01-31,omicrisp: a crispr sars-cov-2 test with omicron detection,"['Sharma, S.; Prakash, M. B.; Gupta, N.; Gupta, V.; Chandru, V.']","we have developed a crispr based assay that can detect the presence of sars-cov-2 in rna extracted from human samples and also predict if it is an omicron or non-omicron variant of the virus. this is a nucleic acid amplification-based test (naat). the amplification and detection are carried out in two independent steps in this assay. amplification is done using a standard one-step rt-pcr method. the detection is done using a method that utilizes the trans-cleavage activity of the cas12a enzyme. we have evaluated the performance of omicrisp in more than 80 clinical samples and observed an agreement of 100% with the sequencing results, in labeling sars-cov-2 positive samples as omicron or non-omicron. omicrisp -like platform can be developed quickly and can potentially complement sequencing for quick and rapid tracking of the transmission of new pathogen variants."
https://arxiv.org/abs/2403.08770,2024-03-13,FastMAC: Stochastic Spectral Sampling of Correspondence Graph,"['Yifei Zhang', 'Hao Zhao', 'Hongyang Li', 'Siheng Chen']","3D correspondence, i.e., a pair of 3D points, is a fundamental concept in computer vision. A set of 3D correspondences, when equipped with compatibility edges, forms a correspondence graph. This graph is a critical component in several state-of-the-art 3D point cloud registration approaches, e.g., the one based on maximal cliques (MAC). However, its properties have not been well understood. So we present the first study that introduces graph signal processing into the domain of correspondence graph. We exploit the generalized degree signal on correspondence graph and pursue sampling strategies that preserve high-frequency components of this signal. To address time-consuming singular value decomposition in deterministic sampling, we resort to a stochastic approximate sampling strategy. As such, the core of our method is the stochastic spectral sampling of correspondence graph. As an application, we build a complete 3D registration algorithm termed as FastMAC, that reaches real-time speed while leading to little to none performance drop. Through extensive experiments, we validate that FastMAC works for both indoor and outdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while maintaining high registration success rate on KITTI. Codes are publicly available at https://github.com/Forrest-110/FastMAC."
https://arxiv.org/abs/2403.08766,2024-03-13,MonoOcc: Digging into Monocular Semantic Occupancy Prediction,"['Yupeng Zheng', 'Xiang Li', 'Pengfei Li', 'Yuhang Zheng', 'Bu Jin', 'Chengliang Zhong', 'Xiaoxiao Long', 'Hao Zhao', 'Qichao Zhang']","Monocular Semantic Occupancy Prediction aims to infer the complete 3D geometry and semantic information of scenes from only 2D images. It has garnered significant attention, particularly due to its potential to enhance the 3D perception of autonomous vehicles. However, existing methods rely on a complex cascaded framework with relatively limited information to restore 3D scenes, including a dependency on supervision solely on the whole network's output, single-frame input, and the utilization of a small backbone. These challenges, in turn, hinder the optimization of the framework and yield inferior prediction results, particularly concerning smaller and long-tailed objects. To address these issues, we propose MonoOcc. In particular, we (i) improve the monocular occupancy prediction framework by proposing an auxiliary semantic loss as supervision to the shallow layers of the framework and an image-conditioned cross-attention module to refine voxel features with visual clues, and (ii) employ a distillation module that transfers temporal information and richer knowledge from a larger image backbone to the monocular semantic occupancy prediction framework with low cost of hardware. With these advantages, our method yields state-of-the-art performance on the camera-based SemanticKITTI Scene Completion benchmark. Codes and models can be accessed at https://github.com/ucaszyp/MonoOcc"
https://arxiv.org/abs/2403.08764,2024-03-13,VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis,"['Enric Corona', 'Andrei Zanfir', 'Eduard Gabriel Bazavan', 'Nikos Kolotouros', 'Thiemo Alldieck', 'Cristian Sminchisescu']","We propose VLOGGER, a method for audio-driven human video generation from a single input image of a person, which builds on the success of recent generative diffusion models. Our method consists of 1) a stochastic human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture that augments text-to-image models with both spatial and temporal controls. This supports the generation of high quality video of variable length, easily controllable through high-level representations of human faces and bodies. In contrast to previous work, our method does not require training for each person, does not rely on face detection and cropping, generates the complete image (not just the face or the lips), and considers a broad spectrum of scenarios (e.g. visible torso or diverse subject identities) that are critical to correctly synthesize humans who communicate. We also curate MENTOR, a new and diverse dataset with 3d pose and expression annotations, one order of magnitude larger than previous ones (800,000 identities) and with dynamic gestures, on which we train and ablate our main technical contributions."
https://arxiv.org/abs/2403.08755,2024-03-13,DAM: Dynamic Adapter Merging for Continual Video QA Learning,"['Feng Cheng', 'Ziyang Wang', 'Yi-Lin Sung', 'Yan-Bo Lin', 'Mohit Bansal', 'Gedas Bertasius']","We present a parameter-efficient method for continual video question-answering (VidQA) learning. Our method, named DAM, uses the proposed Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable efficient adaptation to continually arriving datasets, (iii) handle inputs from unknown datasets during inference, and (iv) enable knowledge sharing across similar dataset domains. Given a set of continually streaming VidQA datasets, we sequentially train dataset-specific adapters for each dataset while freezing the parameters of a large pretrained video-language backbone. During inference, given a video-question sample from an unknown domain, our method first uses the proposed non-parametric router function to compute a probability for each adapter, reflecting how relevant that adapter is to the current video-question input instance. Subsequently, the proposed dynamic adapter merging scheme aggregates all the adapter weights into a new adapter instance tailored for that particular test sample to compute the final VidQA prediction, mitigating the impact of inaccurate router predictions and facilitating knowledge sharing across domains. Our DAM model outperforms prior state-of-the-art continual learning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA datasets spanning various domains. We further extend DAM to continual image classification and image QA and outperform prior methods by a large margin. The code is publicly available at: https://github.com/klauscc/DAM"
https://arxiv.org/abs/2403.08737,2024-03-13,ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation,"['Sayar Ghosh Roy', 'Jiawei Han']","Existing Machine Learning approaches for local citation recommendation directly map or translate a query, which is typically a claim or an entity mention, to citation-worthy research papers. Within such a formulation, it is challenging to pinpoint why one should cite a specific research paper for a particular query, leading to limited recommendation interpretability. To alleviate this, we introduce the evidence-grounded local citation recommendation task, where the target latent space comprises evidence spans for recommending specific papers. Using a distantly-supervised evidence retrieval and multi-step re-ranking framework, our proposed system, ILCiteR, recommends papers to cite for a query grounded on similar evidence spans extracted from the existing research literature. Unlike past formulations that simply output recommendations, ILCiteR retrieves ranked lists of evidence span and recommended paper pairs. Secondly, previously proposed neural models for citation recommendation require expensive training on massive labeled data, ideally after every significant update to the pool of candidate papers. In contrast, ILCiteR relies solely on distant supervision from a dynamic evidence database and pre-trained Transformer-based Language Models without any model training. We contribute a novel dataset for the evidence-grounded local citation recommendation task and demonstrate the efficacy of our proposed conditional neural rank-ensembling approach for re-ranking evidence spans."
https://arxiv.org/abs/2403.08729,2024-03-13,Efficient and practical Hamiltonian simulation from time-dependent product formulas,"['Jan Lukas Bosse', 'Andrew M. Childs', 'Charles Derby', 'Filippo Maria Gambetta', 'Ashley Montanaro', 'Raul A. Santos']","In this work we propose an approach for implementing time-evolution of a quantum system using product formulas. The quantum algorithms we develop have provably better scaling (in terms of gate complexity and circuit depth) than a naive application of well-known Trotter formulas, for systems where the evolution is determined by a Hamiltonian with different energy scales (i.e., one part is ""large"" and another part is ""small""). Our algorithms generate a decomposition of the evolution operator into a product of simple unitaries that are directly implementable on a quantum computer. Although the theoretical scaling is suboptimal compared with state-of-the-art algorithms (e.g., quantum signal processing), the performance of the algorithms we propose is highly competitive in practice. We illustrate this via extensive numerical simulations for several models. For instance, in the strong-field regime of the 1D transverse-field Ising model, our algorithms achieve an improvement of one order of magnitude in both the system size and evolution time that can be simulated with a fixed budget of 1000 arbitrary 2-qubit gates, compared with standard Trotter formulas."
https://arxiv.org/abs/2403.08728,2024-03-13,Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models trained on Corrupted Data,"['Asad Aali', 'Giannis Daras', 'Brett Levac', 'Sidharth Kumar', 'Alexandros G. Dimakis', 'Jonathan I. Tamir']","We provide a framework for solving inverse problems with diffusion models learned from linearly corrupted data. Our method, Ambient Diffusion Posterior Sampling (A-DPS), leverages a generative model pre-trained on one type of corruption (e.g. image inpainting) to perform posterior sampling conditioned on measurements from a potentially different forward process (e.g. image blurring). We test the efficacy of our approach on standard natural image datasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes outperform models trained on clean data for several image restoration tasks in both speed and performance. We further extend the Ambient Diffusion framework to train MRI models with access only to Fourier subsampled multi-coil MRI measurements at various acceleration factors (R=2, 4, 6, 8). We again observe that models trained on highly subsampled data are better priors for solving inverse problems in the high acceleration regime than models trained on fully sampled data. We open-source our code and the trained Ambient Diffusion MRI models: https://github.com/utcsilab/ambient-diffusion-mri ."
https://arxiv.org/abs/2403.08721,2024-03-13,Historical Astronomical Diagrams Decomposition in Geometric Primitives,"['Syrine Kalleli', 'Scott Trigg', 'Ségolène Albouy', 'Mathieu Husson', 'Mathieu Aubry']","Automatically extracting the geometric content from the hundreds of thousands of diagrams drawn in historical manuscripts would enable historians to study the diffusion of astronomical knowledge on a global scale. However, state-of-the-art vectorization methods, often designed to tackle modern data, are not adapted to the complexity and diversity of historical astronomical diagrams. Our contribution is thus twofold. First, we introduce a unique dataset of 303 astronomical diagrams from diverse traditions, ranging from the XIIth to the XVIIIth century, annotated with more than 3000 line segments, circles and arcs. Second, we develop a model that builds on DINO-DETR to enable the prediction of multiple geometric primitives. We show that it can be trained solely on synthetic data and accurately predict primitives on our challenging dataset. Our approach widely improves over the LETR baseline, which is restricted to lines, by introducing a meaningful parametrization for multiple primitives, jointly training for detection and parameter refinement, using deformable attention and training on rich synthetic data. Our dataset and code are available on our webpage."
https://arxiv.org/abs/2403.08716,2024-03-13,DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation,"['Zilin Si', 'Gu Zhang', 'Qingwei Ben', 'Branden Romero', 'Zhou Xian', 'Chao Liu', 'Chuang Gan']","We introduce DIFFTACTILE, a physics-based differentiable tactile simulation system designed to enhance robotic manipulation with dense and physically accurate tactile feedback. In contrast to prior tactile simulators which primarily focus on manipulating rigid bodies and often rely on simplified approximations to model stress and deformations of materials in contact, DIFFTACTILE emphasizes physics-based contact modeling with high fidelity, supporting simulations of diverse contact modes and interactions with objects possessing a wide range of material properties. Our system incorporates several key components, including a Finite Element Method (FEM)-based soft body model for simulating the sensing elastomer, a multi-material simulator for modeling diverse object types (such as elastic, elastoplastic, cables) under manipulation, a penalty-based contact model for handling contact dynamics. The differentiable nature of our system facilitates gradient-based optimization for both 1) refining physical properties in simulation using real-world data, hence narrowing the sim-to-real gap and 2) efficient learning of tactile-assisted grasping and contact-rich manipulation skills. Additionally, we introduce a method to infer the optical response of our tactile sensor to contact using an efficient pixel-based neural module. We anticipate that DIFFTACTILE will serve as a useful platform for studying contact-rich manipulations, leveraging the benefits of dense tactile feedback and differentiable physics. Code and supplementary materials are available at the project website https://difftactile.github.io/."
https://arxiv.org/abs/2403.08714,2024-03-13,Dynamic computerized tomography using inexact models and motion estimation,"['Gesa Sarnighausen', 'Anne Wald', 'Alexander Meaney']","Reconstructing a dynamic object with affine motion in computerized tomography (CT) leads to motion artifacts if the motion is not taken into account. In most cases, the actual motion is neither known nor can be determined easily. As a consequence, the respective model that describes CT is incomplete. The iterative RESESOP-Kaczmarz method can - under certain conditions and by exploiting the modeling error - reconstruct dynamic objects at different time points even if the exact motion is unknown. However, the method is very time-consuming. To speed the reconstruction process up and obtain better results, we combine the following three steps: 1. RESESOP-Kacmarz with only a few iterations is implemented to reconstruct the object at different time points. 2. The motion is estimated via landmark detection, e.g. using deep learning. 3. The estimated motion is integrated into the reconstruction process, allowing the use of dynamic filtered backprojection. We give a short review of all methods involved and present numerical results as a proof of principle."
https://arxiv.org/abs/2403.08706,2024-03-13,Optimal adaptation of surface-code decoders to local noise,['Andrew S. Darmawan'],"Information obtained from noise characterization of a quantum device can be used in classical decoding algorithms to improve the performance of quantum error-correcting codes. Focusing on the surface code under local (i.e. single-qubit) noise, we present a simple method to determine the maximum extent to which adapting a surface-code decoder to a noise feature can lead to a performance improvement. Our method is based on a tensor-network decoding algorithm, which uses the syndrome information as well as a process matrix description of the noise to compute a near-optimal correction. By selectively mischaracterizing the noise model input to the decoder and measuring the resulting loss in fidelity of the logical qubit, we can determine the relative importance of individual noise parameters for decoding. We apply this method to several physically relevant uncorrelated noise models with features such as coherence, spatial inhomogeneity and bias. While noise generally requires many parameters to describe completely, we find that to achieve near optimal decoding it appears only necessary adapt the decoder to a small number of critical parameters."
https://arxiv.org/abs/2403.08702,2024-03-13,On the Stochasticity of Aerosol-Cloud Interactions within a Data-driven Framework,"['Xiang-Yu Li', 'Hailong Wang', 'TC Chakraborty', 'Armin Sorooshian', 'Luke D. Ziemba', 'Christiane Voigt', 'Kenneth Lee Thornhill']","Aerosol-cloud interactions (ACI) pose the largest uncertainty for climate projections. Among many challenges of understanding ACI, the question of whether ACI is deterministic or stochastic has not been explicitly formulated and asked. Here we attempt to answer this question by predicting cloud droplet number concentration Nc from aerosol number concentration Na and ambient conditions. We use aerosol properties, vertical velocity fluctuation w', and meteorological states (temperature T and water vapor mixing ratio q_v) from the ACTIVATE field observations (2020 to 2022) as predictor variables to estimate Nc. We show that the climatological Nc can be successfully predicted using a machine learning model despite the strongly nonlinear and multi-scale nature of ACI. However, the observation-trained machine learning model fails to predict Nc in individual cases while it successfully predicts Nc of randomly selected data points that cover a broad spatiotemporal scale, suggesting the stochastic nature of ACI at fine spatiotemporal scales."
https://arxiv.org/abs/2403.08674,2024-03-13,Quantum jump photodetector for narrowband photon counting with a single atom,"['Laura Zarraoa', 'Romain Veyron', 'Tomas Lamich', 'Morgan W. Mitchell']","Using a single neutral \textsuperscript{87}Rb atom held in an optical trap, and ""quantum jump"" detection of single-photon-initiated state changes, we demonstrate an intrinsically-narrowband single-photon detector, of interest for separating weak signals from strong optical background. Using novel statistical analysis, we measure quantum efficiency of \SI{2.9+-0.2e-3}{}, a record for single-pass quantum jump production, and dark counts of \SI{9+-20e-3}{counts\per\second} during passive accumulation plus \SI{1.8+-0.1e-2}{counts} per readout, orders of magnitude below those of traditional single-photon detectors. The \SI{6}{\mega\hertz} detection bandwidth is orders of magnitude narrower than existing atomic filters. Available methods can substantially improve \QJPDAcronym{} quantum efficiency, dark counts, bandwidth, and tunability."
https://arxiv.org/abs/2403.08662,2024-03-13,Self-Supervised Learning for Covariance Estimation,"['Tzvi Diskin', 'Ami Wiesel']","We consider the use of deep learning for covariance estimation. We propose to globally learn a neural network that will then be applied locally at inference time. Leveraging recent advancements in self-supervised foundational models, we train the network without any labeling by simply masking different samples and learning to predict their covariance given their surrounding neighbors. The architecture is based on the popular attention mechanism. Its main advantage over classical methods is the automatic exploitation of global characteristics without any distributional assumptions or regularization. It can be pre-trained as a foundation model and then be repurposed for various downstream tasks, e.g., adaptive target detection in radar or hyperspectral imagery."
https://arxiv.org/abs/2403.08657,2024-03-13,Predicting long timescale kinetics under variable experimental conditions with Kinetica.jl,"['Joe Gilkes', 'Mark Storr', 'Reinhard J. Maurer', 'Scott Habershon']","Predicting the degradation processes of molecules over long timescales is a key aspect of industrial materials design. However, it is made computationally challenging by the need to construct large networks of chemical reactions that are relevant to the experimental conditions that kinetic models must mirror, with every reaction requiring accurate kinetic data. Here we showcase Kinetica.jl, a new software package for constructing large-scale chemical reaction networks in a fully-automated fashion by exploring chemical reaction space with a kinetics-driven algorithm; coupled to efficient machine-learning models of activation energies for sampled elementary reactions, we show how this approach readily enables generation and kinetic characterization of networks containing $\sim10^{3}$ chemical species and $10^{4}$ - $10^{5}$ reactions. Symbolic-numeric modelling of the generated reaction networks is used to allow for flexible, efficient computation of kinetic profiles under experimentally-realizable conditions such as continuously-variable temperature regimes, enabling direct connection between bottom-up reaction networks and experimental observations. Highly efficient propagation of long-timescale kinetic profiles is required for automated reaction network refinement and is enabled here by a new discrete kinetic approximation. The resulting Kinetica.jl simulation package therefore enables automated generation, characterization, and long-timescale modelling of complex chemical reaction systems. We demonstrate this for hydrocarbon pyrolysis simulated over timescales of seconds, using transient temperature profiles representing those of tubular flow reactor experiments."
https://arxiv.org/abs/2403.08644,2024-03-13,Thermodynamic Integration for Dynamically Unstable Systems Using Interatomic Force Constants without Molecular Dynamics,"['Junsoo Park', 'Zhigang Wu', 'John W. Lawson']","We demonstrate an efficient and accurate, general-purpose first-principles blueprint for calculating anharmonic vibrational free energy and predicting structural phase transition temperatures of solids. Thermodynamic integration is performed without molecular dynamics using only interatomic force constants to model analogues of the true potential and generate their thermal ensembles. By replacing \textit{ab initio} molecular dynamics (AIMD) with statistical sampling of ensemble configurations and trading density-functional theory (DFT) energy calculations on each configuration for a set of matrix operations, our approach enables a faster thermodynamic integration by 4 orders of magnitude over the traditional route via AIMD. Experimental phase transition temperatures of a variety of strongly anharmonic materials with dynamical instabilities including shape-memory alloys are recovered to largely within 25% error. Such a combination of speed and accuracy enables the method to be deployed at a large-scale for predictive mapping of phase transition temperatures."
https://arxiv.org/abs/2403.08630,2024-03-13,Leveraging Non-Decimated Wavelet Packet Features and Transformer Models for Time Series Forecasting,"['Guy P Nason', 'James L. Wei']","This article combines wavelet analysis techniques with machine learning methods for univariate time series forecasting, focusing on three main contributions. Firstly, we consider the use of Daubechies wavelets with different numbers of vanishing moments as input features to both non-temporal and temporal forecasting methods, by selecting these numbers during the cross-validation phase. Secondly, we compare the use of both the non-decimated wavelet transform and the non-decimated wavelet packet transform for computing these features, the latter providing a much larger set of potentially useful coefficient vectors. The wavelet coefficients are computed using a shifted version of the typical pyramidal algorithm to ensure no leakage of future information into these inputs. Thirdly, we evaluate the use of these wavelet features on a significantly wider set of forecasting methods than previous studies, including both temporal and non-temporal models, and both statistical and deep learning-based methods. The latter include state-of-the-art transformer-based neural network architectures. Our experiments suggest significant benefit in replacing higher-order lagged features with wavelet features across all examined non-temporal methods for one-step-forward forecasting, and modest benefit when used as inputs for temporal deep learning-based models for long-horizon forecasting."
https://arxiv.org/abs/2403.08629,2024-03-13,Scaling Up Dynamic Human-Scene Interaction Modeling,"['Nan Jiang', 'Zhiyuan Zhang', 'Hongjie Li', 'Xiaoxuan Ma', 'Zan Wang', 'Yixin Chen', 'Tengyu Liu', 'Yixin Zhu', 'Siyuan Huang']","Confronting the challenges of data scarcity and advanced motion synthesis in human-scene interaction modeling, we introduce the TRUMANS dataset alongside a novel HSI motion synthesis method. TRUMANS stands as the most comprehensive motion-captured HSI dataset currently available, encompassing over 15 hours of human interactions across 100 indoor scenes. It intricately captures whole-body human motions and part-level object dynamics, focusing on the realism of contact. This dataset is further scaled up by transforming physical environments into exact virtual models and applying extensive augmentations to appearance and motion for both humans and objects while maintaining interaction fidelity. Utilizing TRUMANS, we devise a diffusion-based autoregressive model that efficiently generates HSI sequences of any length, taking into account both scene context and intended actions. In experiments, our approach shows remarkable zero-shot generalizability on a range of 3D scene datasets (e.g., PROX, Replica, ScanNet, ScanNet++), producing motions that closely mimic original motion-captured sequences, as confirmed by quantitative experiments and human studies."
https://arxiv.org/abs/2403.08627,2024-03-13,Multifidelity linear regression for scientific machine learning from scarce data,"['Elizabeth Qian', 'Anirban Chaudhuri', 'Dayoung Kang', 'Vignesh Sella']","Machine learning (ML) methods, which fit to data the parameters of a given parameterized model class, have garnered significant interest as potential methods for learning surrogate models for complex engineering systems for which traditional simulation is expensive. However, in many scientific and engineering settings, generating high-fidelity data on which to train ML models is expensive, and the available budget for generating training data is limited. ML models trained on the resulting scarce high-fidelity data have high variance and are sensitive to vagaries of the training data set. We propose a new multifidelity training approach for scientific machine learning that exploits the scientific context where data of varying fidelities and costs are available; for example high-fidelity data may be generated by an expensive fully resolved physics simulation whereas lower-fidelity data may arise from a cheaper model based on simplifying assumptions. We use the multifidelity data to define new multifidelity Monte Carlo estimators for the unknown parameters of linear regression models, and provide theoretical analyses that guarantee the approach's accuracy and improved robustness to small training budgets. Numerical results verify the theoretical analysis and demonstrate that multifidelity learned models trained on scarce high-fidelity data and additional low-fidelity data achieve order-of-magnitude lower model variance than standard models trained on only high-fidelity data of comparable cost. This illustrates that in the scarce data regime, our multifidelity training strategy yields models with lower expected error than standard training approaches."
https://arxiv.org/abs/2403.08625,2024-03-13,Variance Minimisation of the Lipkin-Meshkov-Glick Model on a Quantum Computer,"['Isaac Hobday', 'Paul Stevenson', 'James Benstead']","Quantum computing can potentially provide advantages for specific computational tasks. The simulation of fermionic systems is one such task that lends itself well to quantum computation, with applications in nuclear physics and electronic systems. Here we present work in which we use a variance minimisation method to find the full spectrum of energy eigenvalues of the Lipkin-Meshkov-Glick model; an exactly-solvable nuclear shell model-type system. We perform these calculations using both quantum simulators and real quantum hardware accessed via IBM cloud-based quantum computers. Using these IBM quantum computers we are able to obtain all eigenvalues for the cases of three and seven fermions (nucleons) in the Lipkin-Meshkov-Glick model."
https://arxiv.org/abs/2403.08618,2024-03-13,Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples,"['Sangamesh Kodge', 'Deepak Ravikumar', 'Gobinda Saha', 'Kaushik Roy']","Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models. This corruption often arises from non-expert labeling or adversarial attacks. Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive. To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining. We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update. Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data. We demonstrate Verifix's effectiveness on both synthetic and real-world label noise. Experiments on the CIFAR dataset with 25% synthetic corruption show 7.36% generalization improvements on average. Additionally, we observe generalization improvements of up to 2.63% on naturally corrupted datasets like WebVision1.0 and Clothing1M."
https://arxiv.org/abs/2403.08613,2024-03-13,Link Prediction for Social Networks using Representation Learning and Heuristic-based Features,"['Samarth Khanna', 'Sree Bhattacharyya', 'Sudipto Ghosh', 'Kushagra Agarwal', 'Asit Kumar Das']","The exponential growth in scale and relevance of social networks enable them to provide expansive insights. Predicting missing links in social networks efficiently can help in various modern-day business applications ranging from generating recommendations to influence analysis. Several categories of solutions exist for the same. Here, we explore various feature extraction techniques to generate representations of nodes and edges in a social network that allow us to predict missing links. We compare the results of using ten feature extraction techniques categorized across Structural embeddings, Neighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics, followed by modeling with ensemble classifiers and custom Neural Networks. Further, we propose combining heuristic-based features and learned representations that demonstrate improved performance for the link prediction task on social network datasets. Using this method to generate accurate recommendations for many applications is a matter of further study that appears very promising. The code for all the experiments has been made public."
https://arxiv.org/abs/2403.08604,2024-03-13,DevBench: A Comprehensive Benchmark for Software Development,"['Bowen Li', 'Wenhan Wu', 'Ziwei Tang', 'Lin Shi', 'John Yang', 'Jinyang Li', 'Shunyu Yao', 'Chen Qian', 'Binyuan Hui', 'Qicheng Zhang', 'Zhiyin Yu', 'He Du', 'Ping Yang', 'Dahua Lin', 'Chao Peng', 'Kai Chen']","Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. To this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing. DevBench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts. Our findings offer actionable insights for the future development of LLMs toward real-world programming applications. Our benchmark is available at https://github.com/open-compass/DevBench"
https://arxiv.org/abs/2403.08596,2024-03-13,Patching-based Deep Learning model for the Inpainting of Bragg Coherent Diffraction patterns affected by detectors' gaps,"['Matteo Masto', 'Vincent Favre-Nicolin', 'Steven Leake', 'Tobias Schülli', 'Marie-Ingrid Richard', 'Ewen Bellec']","We propose a deep learning algorithm for the inpainting of Bragg Coherent Diffraction Imaging (BCDI) patterns affected by detector gaps. These regions of missing intensity can compromise the accuracy of reconstruction algorithms, inducing artifacts in the final result. It is thus desirable to restore the intensity in these regions in order to ensure more reliable reconstructions. The key aspect of our method lies in the choice of training the neural network with cropped sections of both experimental diffraction data and simulated data and subsequently patching the predictions generated by the model along the gap, thus completing the full diffraction peak. This provides us with more experimental training data and allows for a faster model training due to the limited size, while the neural network can be applied to arbitrarily larger BCDI datasets. Moreover, our method not only broadens the scope of application but also ensures the preservation of data integrity and reliability in the face of challenging experimental conditions."
https://arxiv.org/abs/2403.08591,2024-03-13,ActionDiffusion: An Action-aware Diffusion Model for Procedure Planning in Instructional Videos,"['Lei Shi', 'Paul Bürkner', 'Andreas Bulling']","We present ActionDiffusion -- a novel diffusion model for procedure planning in instructional videos that is the first to take temporal inter-dependencies between actions into account in a diffusion model for procedure planning. This approach is in stark contrast to existing methods that fail to exploit the rich information content available in the particular order in which actions are performed. Our method unifies the learning of temporal dependencies between actions and denoising of the action plan in the diffusion process by projecting the action information into the noise space. This is achieved 1) by adding action embeddings in the noise masks in the noise-adding phase and 2) by introducing an attention mechanism in the noise prediction network to learn the correlations between different action steps. We report extensive experiments on three instructional video benchmark datasets (CrossTask, Coin, and NIV) and show that our method outperforms previous state-of-the-art methods on all metrics on CrossTask and NIV and all metrics except accuracy on Coin dataset. We show that by adding action embeddings into the noise mask the diffusion model can better learn action temporal dependencies and increase the performances on procedure planning."
https://arxiv.org/abs/2403.08589,2024-03-13,Can physical information aid the generalization ability of Neural Networks for hydraulic modeling?,"['Gianmarco Guglielmo', 'Andrea Montessori', 'Jean-Michel Tucny', 'Michele La Rocca', 'Pietro Prestininzi']","Application of Neural Networks to river hydraulics is fledgling, despite the field suffering from data scarcity, a challenge for machine learning techniques. Consequently, many purely data-driven Neural Networks proved to lack predictive capabilities. In this work, we propose to mitigate such problem by introducing physical information into the training phase. The idea is borrowed from Physics-Informed Neural Networks which have been recently proposed in other contexts. Physics-Informed Neural Networks embed physical information in the form of the residual of the Partial Differential Equations (PDEs) governing the phenomenon and, as such, are conceived as neural solvers, i.e. an alternative to traditional numerical solvers. Such approach is seldom suitable for environmental hydraulics, where epistemic uncertainties are large, and computing residuals of PDEs exhibits difficulties similar to those faced by classical numerical methods. Instead, we envisaged the employment of Neural Networks as neural operators, featuring physical constraints formulated without resorting to PDEs. The proposed novel methodology shares similarities with data augmentation and regularization. We show that incorporating such soft physical information can improve predictive capabilities."
https://arxiv.org/abs/2403.08586,2024-03-13,PRAGO: Differentiable Multi-View Pose Optimization From Objectness Detections,"['Matteo Taiana', 'Matteo Toso', 'Stuart James', 'Alessio Del Bue']","Robustly estimating camera poses from a set of images is a fundamental task which remains challenging for differentiable methods, especially in the case of small and sparse camera pose graphs. To overcome this challenge, we propose Pose-refined Rotation Averaging Graph Optimization (PRAGO). From a set of objectness detections on unordered images, our method reconstructs the rotational pose, and in turn, the absolute pose, in a differentiable manner benefiting from the optimization of a sequence of geometrical tasks. We show how our objectness pose-refinement module in PRAGO is able to refine the inherent ambiguities in pairwise relative pose estimation without removing edges and avoiding making early decisions on the viability of graph edges. PRAGO then refines the absolute rotations through iterative graph construction, reweighting the graph edges to compute the final rotational pose, which can be converted into absolute poses using translation averaging. We show that PRAGO is able to outperform non-differentiable solvers on small and sparse scenes extracted from 7-Scenes achieving a relative improvement of 21% for rotations while achieving similar translation estimates."
https://arxiv.org/abs/2403.08584,2024-03-13,Local Binary and Multiclass SVMs Trained on a Quantum Annealer,"['Enrico Zardini', 'Amer Delilbasic', 'Enrico Blanzieri', 'Gabriele Cavallaro', 'Davide Pastorello']","Support vector machines (SVMs) are widely used machine learning models (e.g., in remote sensing), with formulations for both classification and regression tasks. In the last years, with the advent of working quantum annealers, hybrid SVM models characterised by quantum training and classical execution have been introduced. These models have demonstrated comparable performance to their classical counterparts. However, they are limited in the training set size due to the restricted connectivity of the current quantum annealers. Hence, to take advantage of large datasets (like those related to Earth observation), a strategy is required. In the classical domain, local SVMs, namely, SVMs trained on the data samples selected by a k-nearest neighbors model, have already proven successful. Here, the local application of quantum-trained SVM models is proposed and empirically assessed. In particular, this approach allows overcoming the constraints on the training set size of the quantum-trained models while enhancing their performance. In practice, the FaLK-SVM method, designed for efficient local SVMs, has been combined with quantum-trained SVM models for binary and multiclass classification. In addition, for comparison, FaLK-SVM has been interfaced for the first time with a classical single-step multiclass SVM model (CS SVM). Concerning the empirical evaluation, D-Wave's quantum annealers and real-world datasets taken from the remote sensing domain have been employed. The results have shown the effectiveness and scalability of the proposed approach, but also its practical applicability in a real-world large-scale scenario."
https://arxiv.org/abs/2403.08579,2024-03-13,Machine Learning Optimized Orthogonal Basis Piecewise Polynomial Approximation,"['Hannes Waclawek', 'Stefan Huber']","Piecewise Polynomials (PPs) are utilized in several engineering disciplines, like trajectory planning, to approximate position profiles given in the form of a set of points. While the approximation target along with domain-specific requirements, like Ck -continuity, can be formulated as a system of equations and a result can be computed directly, such closed-form solutions posses limited flexibility with respect to polynomial degrees, polynomial bases or adding further domain-specific requirements. Sufficiently complex optimization goals soon call for the use of numerical methods, like gradient descent. Since gradient descent lies at the heart of training Artificial Neural Networks (ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set of gradient-based optimizers potentially suitable for a wide range of optimization problems beyond the training task for ANNs. Our approach is to utilize the versatility of PP models and combine it with the potential of modern ML optimizers for the use in function approximation in 1D trajectory planning in the context of electronic cam design. We utilize available optimizers of the ML framework TensorFlow directly, outside of the scope of ANNs, to optimize model parameters of our PP model. In this paper, we show how an orthogonal polynomial basis contributes to improving approximation and continuity optimization performance. Utilizing Chebyshev polynomials of the first kind, we develop a novel regularization approach enabling clearly improved convergence behavior. We show that, using this regularization approach, Chebyshev basis performs better than power basis for all relevant optimizers in the combined approximation and continuity optimization setting and demonstrate usability of the presented approach within the electronic cam domain."
https://arxiv.org/abs/2403.08565,2024-03-13,Deep Learning based Positioning with Multi-task Learning and Uncertainty-based Fusion,"['Anastasios Foliadis', 'Mario H. Castañeda', 'Richard A. Stirling-Gallacher', 'Reiner S. Thomä']","Deep learning (DL) methods have been shown to improve the performance of several use cases for the fifth-generation (5G) New radio (NR) air interface. In this paper we investigate user equipment (UE) positioning using the channel state information (CSI) fingerprints between a UE and multiple base stations (BSs). In such a setup, a single DL model can be trained for UE positioning using the CSI fingerprints of the multiple BSs as input. Alternatively, based on the CSI at each BS, a separate DL model can be trained at each BS and then the output of the different models are combined to determine the UE's position. In this work we compare these different fusion techniques and show that fusing the output of separate models achieves higher positioning accuracy, especially in a dynamic scenario. We also show that the fusion of multiple outputs further benefits from considering the uncertainty of the output of the DL model at each BS. For a more efficient training of the DL model across BSs, we additionally propose a multi-task learning (MTL) scheme by sharing some parameters across the models while jointly training all models. This method, not only improves the accuracy of the individual models, but also of the final combined estimate. Lastly, we evaluate the reliability of the uncertainty estimation to ascertain which of the fusion methods provides the highest quality of uncertainty estimates."
https://arxiv.org/abs/2403.08559,2024-03-13,End-to-End Amp Modeling: From Data to Controllable Guitar Amplifier Models,"['Lauri Juvela', 'Eero-Pekka Damskägg', 'Aleksi Peussa', 'Jaakko Mäkinen', 'Thomas Sherson', 'Stylianos I. Mimilakis', 'Athanasios Gotsopoulos']","This paper describes a data-driven approach to creating real-time neural network models of guitar amplifiers, recreating the amplifiers' sonic response to arbitrary inputs at the full range of controls present on the physical device. While the focus on the paper is on the data collection pipeline, we demonstrate the effectiveness of this conditioned black-box approach by training an LSTM model to the task, and comparing its performance to an offline white-box SPICE circuit simulation. Our listening test results demonstrate that the neural amplifier modeling approach can match the subjective performance of a high-quality SPICE model, all while using an automated, non-intrusive data collection process, and an end-to-end trainable, real-time feasible neural network model."
https://arxiv.org/abs/2403.08556,2024-03-13,SM4Depth: Seamless Monocular Metric Depth Estimation across Multiple Cameras and Scenes by One Model,"['Yihao Liu', 'Feng Xue', 'Anlong Ming']","The generalization of monocular metric depth estimation (MMDE) has been a longstanding challenge. Recent methods made progress by combining relative and metric depth or aligning input image focal length. However, they are still beset by challenges in camera, scene, and data levels: (1) Sensitivity to different cameras; (2) Inconsistent accuracy across scenes; (3) Reliance on massive training data. This paper proposes SM4Depth, a seamless MMDE method, to address all the issues above within a single network. First, we reveal that a consistent field of view (FOV) is the key to resolve ``metric ambiguity'' across cameras, which guides us to propose a more straightforward preprocessing unit. Second, to achieve consistently high accuracy across scenes, we explicitly model the metric scale determination as discretizing the depth interval into bins and propose variation-based unnormalized depth bins. This method bridges the depth gap of diverse scenes by reducing the ambiguity of the conventional metric bin. Third, to reduce the reliance on massive training data, we propose a ``divide and conquer"" solution. Instead of estimating directly from the vast solution space, the correct metric bins are estimated from multiple solution sub-spaces for complexity reduction. Finally, with just 150K RGB-D pairs and a consumer-grade GPU for training, SM4Depth achieves state-of-the-art performance on most previously unseen datasets, especially surpassing ZoeDepth and Metric3D on mRI$_θ$. The code can be found at https://github.com/1hao-Liu/SM4Depth."
https://arxiv.org/abs/2403.08554,2024-03-13,Federated Knowledge Graph Unlearning via Diffusion Model,"['Bingchen Liu', 'Yuanyuan Fang']","Federated learning (FL) promotes the development and application of artificial intelligence technologies by enabling model sharing and collaboration while safeguarding data privacy. Knowledge graph (KG) embedding representation provides a foundation for knowledge reasoning and applications by mapping entities and relations into vector space. Federated KG embedding enables the utilization of knowledge from diverse client sources while safeguarding the privacy of local data. However, due to demands such as privacy protection and the need to adapt to dynamic data changes, investigations into machine unlearning (MU) have been sparked. However, it is challenging to maintain the performance of KG embedding models while forgetting the influence of specific forgotten data on the model. In this paper, we propose FedDM, a novel framework tailored for machine unlearning in federated knowledge graphs. Leveraging diffusion models, we generate noisy data to sensibly mitigate the influence of specific knowledge on FL models while preserving the overall performance concerning the remaining data. We conduct experimental evaluations on benchmark datasets to assess the efficacy of the proposed model. Extensive experiments demonstrate that FedDM yields promising results in knowledge forgetting."
https://arxiv.org/abs/2403.08551,2024-03-13,GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting,"['Xinjie Zhang', 'Xingtong Ge', 'Tongda Xu', 'Dailan He', 'Yan Wang', 'Hongwei Qin', 'Guo Lu', 'Jing Geng', 'Jun Zhang']","Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. Furthermore, we integrate existing vector quantization technique to build an image codec. Experimental results demonstrate that our codec attains rate-distortion performance comparable to compression-based INRs such as COIN and COIN++, while facilitating decoding speeds of approximately 1000 FPS. Additionally, preliminary proof of concept shows that our codec surpasses COIN and COIN++ in performance when using partial bits-back coding."
https://arxiv.org/abs/2403.08546,2024-03-13,Semantic Segmentation of Solar Radio Spikes at Low Frequencies,"['Pearse C. Murphy', 'Stéphane Aicardi', 'Baptiste Cecconi', 'Carine Briand', 'Thibault Peccoux']","Solar radio spikes are short lived, narrow bandwidth features in low frequency solar radio observations. The timing of their occurrence and the number of spikes in a given observation is often unpredictable. The high temporal and frequency of resolution of modern radio telescopes such as NenuFAR mean that manually identifying radio spikes is an arduous task. Machine learning approaches to data exploration in solar radio data is on the rise. Here we describe a convolutional neural network to identify the per pixel location of radio spikes as well as determine some simple characteristics of duration, spectral width and drift rate. The model, which we call SpikeNet, was trained using an Nvidia Tesla T4 16GB GPU with ~100000 sample spikes in a total time of 2.2 hours. The segmentation performs well with an intersection over union in the test set of ~0.85. The root mean squared error for predicted spike properties is of the order of 23%. Applying the algorithm to unlabelled data successfully generates segmentation masks although the accuracy of the predicted properties is less reliable, particularly when more than one spike is present in the same 64 X 64 pixel time-frequency range. We have successfully demonstrated that our convolutional neural network can locate and characterise solar radio spikes in a number of seconds compared to the weeks it would take for manual identification."
https://arxiv.org/abs/2403.08525,2024-03-13,From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning,"['John Martinsson', 'Olof Mogren', 'Maria Sandsten', 'Tuomas Virtanen']","In this work we propose an audio recording segmentation method based on an adaptive change point detection (A-CPD) for machine guided weak label annotation of audio recording segments. The goal is to maximize the amount of information gained about the temporal activation's of the target sounds. For each unlabeled audio recording, we use a prediction model to derive a probability curve used to guide annotation. The prediction model is initially pre-trained on available annotated sound event data with classes that are disjoint from the classes in the unlabeled dataset. The prediction model then gradually adapts to the annotations provided by the annotator in an active learning loop. The queries used to guide the weak label annotator towards strong labels are derived using change point detection on these probabilities. We show that it is possible to derive strong labels of high quality even with a limited annotation budget, and show favorable results for A-CPD when compared to two baseline query strategies."
https://arxiv.org/abs/2403.08498,2024-03-13,Gaussian Splatting in Style,"['Abhishek Saroha', 'Mariia Gladkova', 'Cecilia Curreli', 'Tarun Yenamandra', 'Daniel Cremers']","Scene stylization extends the work of neural style transfer to three spatial dimensions. A vital challenge in this problem is to maintain the uniformity of the stylized appearance across a multi-view setting. A vast majority of the previous works achieve this by optimizing the scene with a specific style image. In contrast, we propose a novel architecture trained on a collection of style images, that at test time produces high quality stylized novel views. Our work builds up on the framework of 3D Gaussian splatting. For a given scene, we take the pretrained Gaussians and process them using a multi resolution hash grid and a tiny MLP to obtain the conditional stylised views. The explicit nature of 3D Gaussians give us inherent advantages over NeRF-based methods including geometric consistency, along with having a fast training and rendering regime. This enables our method to be useful for vast practical use cases such as in augmented or virtual reality applications. Through our experiments, we show our methods achieve state-of-the-art performance with superior visual quality on various indoor and outdoor real-world data."
https://arxiv.org/abs/2403.08492,2024-03-13,Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking,"['Ming Dong', 'Yujing Chen', 'Miao Zhang', 'Hao Sun', 'Tingting He']","Chinese Spell Checking (CSC) is a widely used technology, which plays a vital role in speech to text (STT) and optical character recognition (OCR). Most of the existing CSC approaches relying on BERT architecture achieve excellent performance. However, limited by the scale of the foundation model, BERT-based method does not work well in few-shot scenarios, showing certain limitations in practical applications. In this paper, we explore using an in-context learning method named RS-LLM (Rich Semantic based LLMs) to introduce large language models (LLMs) as the foundation model. Besides, we study the impact of introducing various Chinese rich semantic information in our framework. We found that by introducing a small number of specific Chinese rich semantic structures, LLMs achieve better performance than the BERT-based model on few-shot CSC task. Furthermore, we conduct experiments on multiple datasets, and the experimental results verified the superiority of our proposed framework."
https://arxiv.org/abs/2403.08484,2024-03-13,Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH Mask based Efficient Fine-tuning,"['Ming Dong', 'Kang Xue', 'Bolong Zheng', 'Tingting He']","In view of the huge number of parameters of Large language models (LLMs) , tuning all parameters is very costly, and accordingly fine-tuning specific parameters is more sensible. Most of parameter efficient fine-tuning (PEFT) concentrate on parameter selection strategies, such as additive method, selective method and reparametrization-based method. However, there are few methods that consider the impact of data samples on parameter selecting, such as Fish Mask based method. Fish Mask randomly choose a part of data samples and treat them equally during parameter selection, which is unable to dynamically select optimal parameters for inconstant data distributions. In this work, we adopt a data-oriented perspective, then proposing an IRD ($\mathrm{\underline I}$terative sample-parameter $\mathrm{\underline R}$ange $\mathrm{\underline D}$ecreasing) algorithm to search the best setting of sample-parameter pair for FISH Mask. In each iteration, by searching the set of samples and parameters with larger Fish information, IRD can find better sample-parameter pair in most scale. We demonstrate the effectiveness and rationality of proposed strategy by conducting experiments on GLUE benchmark. Experimental results show our strategy optimizes the parameter selection and achieves preferable performance."
https://arxiv.org/abs/2403.08481,2024-03-13,SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks,"['Guy Amit', 'Abigail Goldsteen', 'Ariel Farkash']","Natural language processing models have experienced a significant upsurge in recent years, with numerous applications being built upon them. Many of these applications require fine-tuning generic base models on customized, proprietary datasets. This fine-tuning data is especially likely to contain personal or sensitive information about individuals, resulting in increased privacy risk. Membership inference attacks are the most commonly employed attack to assess the privacy leakage of a machine learning model. However, limited research is available on the factors that affect the vulnerability of language models to this kind of attack, or on the applicability of different defense strategies in the language domain. We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies. We find that some training methods provide significantly reduced privacy risk, with the combination of differential privacy and low-rank adaptors achieving the best privacy protection against these attacks."
https://arxiv.org/abs/2403.08460,2024-03-13,Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal Diffusion Model,"['Ruibin Zhang', 'Donglai Xue', 'Yuhan Wang', 'Ruixu Geng', 'Fei Gao']","Millimeter wave (mmWave) radars have attracted significant attention from both academia and industry due to their capability to operate in extreme weather conditions. However, they face challenges in terms of sparsity and noise interference, which hinder their application in the field of micro aerial vehicle (MAV) autonomous navigation. To this end, this paper proposes a novel approach to dense and accurate mmWave radar point cloud construction via cross-modal learning. Specifically, we introduce diffusion models, which possess state-of-the-art performance in generative modeling, to predict LiDAR-like point clouds from paired raw radar data. We also incorporate the most recent diffusion model inference accelerating techniques to ensure that the proposed method can be implemented on MAVs with limited computing resources.We validate the proposed method through extensive benchmark comparisons and real-world experiments, demonstrating its superior performance and generalization ability. Code and pretrained models will be available at https://github.com/ZJU-FAST-Lab/Radar-Diffusion."
https://arxiv.org/abs/2403.08455,2024-03-13,IAMCV Multi-Scenario Vehicle Interaction Dataset,"['Novel Certad', 'Enrico del Re', 'Helena Korndörfer', 'Gregory Schröder', 'Walter Morales-Alvarez', 'Sebastian Tschernuth', 'Delgermaa Gankhuyag', 'Luigi del Re', 'Cristina Olaverri-Monreal']","The acquisition and analysis of high-quality sensor data constitute an essential requirement in shaping the development of fully autonomous driving systems. This process is indispensable for enhancing road safety and ensuring the effectiveness of the technological advancements in the automotive industry. This study introduces the Interaction of Autonomous and Manually-Controlled Vehicles (IAMCV) dataset, a novel and extensive dataset focused on inter-vehicle interactions. The dataset, enriched with a sophisticated array of sensors such as Light Detection and Ranging, cameras, Inertial Measurement Unit/Global Positioning System, and vehicle bus data acquisition, provides a comprehensive representation of real-world driving scenarios that include roundabouts, intersections, country roads, and highways, recorded across diverse locations in Germany. Furthermore, the study shows the versatility of the IAMCV dataset through several proof-of-concept use cases. Firstly, an unsupervised trajectory clustering algorithm illustrates the dataset's capability in categorizing vehicle movements without the need for labeled training data. Secondly, we compare an online camera calibration method with the Robot Operating System-based standard, using images captured in the dataset. Finally, a preliminary test employing the YOLOv8 object-detection model is conducted, augmented by reflections on the transferability of object detection across various LIDAR resolutions. These use cases underscore the practical utility of the collected dataset, emphasizing its potential to advance research and innovation in the area of intelligent vehicles."
https://arxiv.org/abs/2403.08444,2024-03-13,COSTREAM: Learned Cost Models for Operator Placement in Edge-Cloud Environments,"['Roman Heinrich', 'Carsten Binnig', 'Harald Kornmayer', 'Manisha Luthra']","In this work, we present COSTREAM, a novel learned cost model for Distributed Stream Processing Systems that provides accurate predictions of the execution costs of a streaming query in an edge-cloud environment. The cost model can be used to find an initial placement of operators across heterogeneous hardware, which is particularly important in these environments. In our evaluation, we demonstrate that COSTREAM can produce highly accurate cost estimates for the initial operator placement and even generalize to unseen placements, queries, and hardware. When using COSTREAM to optimize the placements of streaming operators, a median speed-up of around 21x can be achieved compared to baselines."
https://arxiv.org/abs/2403.08438,2024-03-13,Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research,"['Tobias Hille', 'Maximilian Stubbemann', 'Tom Hanika']","Difficulties in replication and reproducibility of empirical evidences in machine learning research have become a prominent topic in recent years. Ensuring that machine learning research results are sound and reliable requires reproducibility, which verifies the reliability of research findings using the same code and data. This promotes open and accessible research, robust experimental workflows, and the rapid integration of new findings. Evaluating the degree to which research publications support these different aspects of reproducibility is one goal of the present work. For this we introduce an ontology of reproducibility in machine learning and apply it to methods for graph neural networks. Building on these efforts we turn towards another critical challenge in machine learning, namely the curse of dimensionality, which poses challenges in data collection, representation, and analysis, making it harder to find representative data and impeding the training and inference processes. Using the closely linked concept of geometric intrinsic dimension we investigate to which extend the used machine learning models are influenced by the intrinsic dimension of the data sets they are trained on."
https://arxiv.org/abs/2403.08436,2024-03-13,PFStorer: Personalized Face Restoration and Super-Resolution,"['Tuomas Varanka', 'Tapani Toivonen', 'Soumya Tripathy', 'Guoying Zhao', 'Erman Acar']","Recent developments in face restoration have achieved remarkable results in producing high-quality and lifelike outputs. The stunning results however often fail to be faithful with respect to the identity of the person as the models lack necessary context. In this paper, we explore the potential of personalized face restoration with diffusion models. In our approach a restoration model is personalized using a few images of the identity, leading to tailored restoration with respect to the identity while retaining fine-grained details. By using independent trainable blocks for personalization, the rich prior of a base restoration model can be exploited to its fullest. To avoid the model relying on parts of identity left in the conditioning low-quality images, a generative regularizer is employed. With a learnable parameter, the model learns to balance between the details generated based on the input image and the degree of personalization. Moreover, we improve the training pipeline of face restoration models to enable an alignment-free approach. We showcase the robust capabilities of our approach in several real-world scenarios with multiple identities, demonstrating our method's ability to generate fine-grained details with faithful restoration. In the user study we evaluate the perceptual quality and faithfulness of the genereated details, with our method being voted best 61% of the time compared to the second best with 25% of the votes."
https://arxiv.org/abs/2403.08430,2024-03-13,Search-based Optimisation of LLM Learning Shots for Story Point Estimation,"['Vali Tawosi', 'Salwa Alamir', 'Xiaomo Liu']","One of the ways Large Language Models (LLMs) are used to perform machine learning tasks is to provide them with a few examples before asking them to produce a prediction. This is a meta-learning process known as few-shot learning. In this paper, we use available Search-Based methods to optimise the number and combination of examples that can improve an LLM's estimation performance, when it is used to estimate story points for new agile tasks. Our preliminary results show that our SBSE technique improves the estimation performance of the LLM by 59.34% on average (in terms of mean absolute error of the estimation) over three datasets against a zero-shot setting."
https://arxiv.org/abs/2403.08412,2024-03-13,Theoretical limits of magnetic detection of structural surface defects at the nanometer scale,"['Wolfgang Körner', 'Daniel F. Urban', 'Christian Elsässer']","We present a theoretical study on the magnetic signals of structural surface defects like cracks or indents combined with rough surfaces or subsurface inclusions of soft ferromagnetic metals like body-centered cubic Fe or amorphous CoFeB. We discuss limits of early detection of small surface defects on the basis of calculated magnetic stray fields few tens of nm above the surface. The considered surface imperfections have extensions of a few nm which correspond to low multiples of the magnetic exchange lengths of Fe or CoFeB. The detection of such small inhomogeneities requires that the sensor is about as close to the surface as the size of the inhomogeneity is. Furthermore, the step width of a scanning sensor must be of the same size as well. Both these requirements may be fulfilled for instance by scanning microscopy with diamond nitrogen-vacancy-center quantum sensors."
https://arxiv.org/abs/2403.08403,2024-03-13,FSDR: A Novel Deep Learning-based Feature Selection Algorithm for Pseudo Time-Series Data using Discrete Relaxation,"['Mohammad Rahman', 'Manzur Murshed', 'Shyh Wei Teng', 'Manoranjan Paul']","Conventional feature selection algorithms applied to Pseudo Time-Series (PTS) data, which consists of observations arranged in sequential order without adhering to a conventional temporal dimension, often exhibit impractical computational complexities with high dimensional data. To address this challenge, we introduce a Deep Learning (DL)-based feature selection algorithm: Feature Selection through Discrete Relaxation (FSDR), tailored for PTS data. Unlike the existing feature selection algorithms, FSDR learns the important features as model parameters using discrete relaxation, which refers to the process of approximating a discrete optimisation problem with a continuous one. FSDR is capable of accommodating a high number of feature dimensions, a capability beyond the reach of existing DL-based or traditional methods. Through testing on a hyperspectral dataset (i.e., a type of PTS data), our experimental results demonstrate that FSDR outperforms three commonly used feature selection algorithms, taking into account a balance among execution time, $R^2$, and $RMSE$."
https://arxiv.org/abs/2403.08388,2024-03-13,Feasibility of detecting shadows in disks induced by infall,"['A. Krieger', 'M. Kuffmeier', 'S. Reissl', 'C. P. Dullemond', 'C. Ginski', 'S. Wolf']","Observations performed with high-resolution imaging techniques revealed the existence of shadows in circumstellar disks that can be explained by the misalignment of an inner with respect to an outer disk. The cause of misalignment, however, is still debated. In this study, we investigate the feasibility of observing shadows induced by one prominent scenario that may lead to misalignment, which involves the late infall of material onto a protostellar system. In particular, we use previously performed hydrodynamical simulations of such events, and generate flux maps in the visible, near-infrared, submillimeter, and millimeter wavelength range using Monte Carlo radiative transfer. Based on that, we derive synthetic observations of these systems performed with the instruments SPHERE/VLT and ALMA, which we use as a basis for our subsequent analysis. We find that near-infrared observations with SPHERE are particularly well suited for detecting shadows via direct imaging alongside other features such as gaps, arcs, and streamers. On the contrary, performing a shadow detection based on reconstructed ALMA observations is very challenging due to the high sensitivity that is required for this task. Thus, in cases that allow for a detection, sophisticated analyses may be needed, for instance by the utilization of carefully constructed azimuthal profiles, aiding the search for potentially shallow shadows. Lastly, we conclude that late infall-induced disk misalignment offers a plausible explanation for the emergence of shadows that are observed in various systems."
https://arxiv.org/abs/2403.08383,2024-03-13,"RAF-GI: Towards Robust, Accurate and Fast-Convergent Gradient Inversion Attack in Federated Learning","['Can Liu', 'Jin Wang', 'Dongyang Yu']","Federated learning (FL) empowers privacy-preservation in model training by only exposing users' model gradients. Yet, FL users are susceptible to the gradient inversion (GI) attack which can reconstruct ground-truth training data such as images based on model gradients. However, reconstructing high-resolution images by existing GI attack works faces two challenges: inferior accuracy and slow-convergence, especially when the context is complicated, e.g., the training batch size is much greater than 1 on each FL user. To address these challenges, we present a Robust, Accurate and Fast-convergent GI attack algorithm, called RAF-GI, with two components: 1) Additional Convolution Block (ACB) which can restore labels with up to 20% improvement compared with existing works; 2) Total variance, three-channel mEan and cAnny edge detection regularization term (TEA), which is a white-box attack strategy to reconstruct images based on labels inferred by ACB. Moreover, RAF-GI is robust that can still accurately reconstruct ground-truth data when the users' training batch size is no more than 48. Our experimental results manifest that RAF-GI can diminish 94% time costs while achieving superb inversion quality in ImageNet dataset. Notably, with a batch size of 1, RAF-GI exhibits a 7.89 higher Peak Signal-to-Noise Ratio (PSNR) compared to the state-of-the-art baselines."
https://arxiv.org/abs/2403.08380,2024-03-13,Mitigate Target-level Insensitivity of Infrared Small Target Detection via Posterior Distribution Modeling,"['Haoqing Li', 'Jinfu Yang', 'Yifei Xu', 'Runshi Wang']","Infrared Small Target Detection (IRSTD) aims to segment small targets from infrared clutter background. Existing methods mainly focus on discriminative approaches, i.e., a pixel-level front-background binary segmentation. Since infrared small targets are small and low signal-to-clutter ratio, empirical risk has few disturbances when a certain false alarm and missed detection exist, which seriously affect the further improvement of such methods. Motivated by the dense prediction generative methods, in this paper, we propose a diffusion model framework for Infrared Small Target Detection which compensates pixel-level discriminant with mask posterior distribution modeling. Furthermore, we design a Low-frequency Isolation in the wavelet domain to suppress the interference of intrinsic infrared noise on the diffusion noise estimation. This transition from the discriminative paradigm to generative one enables us to bypass the target-level insensitivity. Experiments show that the proposed method achieves competitive performance gains over state-of-the-art methods on NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets. Code are available at https://github.com/Li-Haoqing/IRSTD-Diff."
https://arxiv.org/abs/2403.08362,2024-03-13,Mean-Field Microcanonical Gradient Descent,"['Marcus Häggbom', 'Morten Karlsmark', 'Joakim andén']","Microcanonical gradient descent is a sampling procedure for energy-based models allowing for efficient sampling of distributions in high dimension. It works by transporting samples from a high-entropy distribution, such as Gaussian white noise, to a low-energy region using gradient descent. We put this model in the framework of normalizing flows, showing how it can often overfit by losing an unnecessary amount of entropy in the descent. As a remedy, we propose a mean-field microcanonical gradient descent that samples several weakly coupled data points simultaneously, allowing for better control of the entropy loss while paying little in terms of likelihood fit. We study these models in the context of financial time series, illustrating the improvements on both synthetic and real data."
https://arxiv.org/abs/2403.08355,2024-03-13,NaturalVLM: Leveraging Fine-grained Natural Language for Affordance-Guided Visual Manipulation,"['Ran Xu', 'Yan Shen', 'Xiaoqi Li', 'Ruihai Wu', 'Hao Dong']","Enabling home-assistant robots to perceive and manipulate a diverse range of 3D objects based on human language instructions is a pivotal challenge. Prior research has predominantly focused on simplistic and task-oriented instructions, i.e., ""Slide the top drawer open"". However, many real-world tasks demand intricate multi-step reasoning, and without human instructions, these will become extremely difficult for robot manipulation. To address these challenges, we introduce a comprehensive benchmark, NrVLM, comprising 15 distinct manipulation tasks, containing over 4500 episodes meticulously annotated with fine-grained language instructions. We split the long-term task process into several steps, with each step having a natural language instruction. Moreover, we propose a novel learning framework that completes the manipulation task step-by-step according to the fine-grained instructions. Specifically, we first identify the instruction to execute, taking into account visual observations and the end-effector's current state. Subsequently, our approach facilitates explicit learning through action-prompts and perception-prompts to promote manipulation-aware cross-modality alignment. Leveraging both visual observations and linguistic guidance, our model outputs a sequence of actionable predictions for manipulation, including contact points and end-effector poses. We evaluate our method and baselines using the proposed benchmark NrVLM. The experimental results demonstrate the effectiveness of our approach. For additional details, please refer to https://sites.google.com/view/naturalvlm."
https://arxiv.org/abs/2403.08352,2024-03-13,Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods,"['Alhassan Mumuni', 'Fuseini Mumuni']","Data augmentation is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models. It primarily involves the application of appropriate data transformation operations to create new data samples with desired properties. Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually. Automated data augmentation methods aim to automate the process. State-of-the-art approaches typically rely on automated machine learning (AutoML) principles. This work presents a comprehensive survey of AutoML-based data augmentation techniques. We discuss various approaches for accomplishing data augmentation with AutoML, including data manipulation, data integration and data synthesis techniques. We present extensive discussion of techniques for realizing each of the major subtasks of the data augmentation process: search space design, hyperparameter optimization and model evaluation. Finally, we carried out an extensive comparison and analysis of the performance of automated data augmentation techniques and state-of-the-art methods based on classical augmentation approaches. The results show that AutoML methods for data augmentation currently outperform state-of-the-art techniques based on conventional approaches."
https://arxiv.org/abs/2403.08331,2024-03-13,Bayesian Optimization that Limits Search Region to Lower Dimensions Utilizing Local GPR,"['Yasunori Taguchi', 'Hiro Gangi']","Optimization of product and system characteristics is required in many fields, including design and control. Bayesian optimization (BO) is often used when there are high observing costs, because BO theoretically guarantees an upper bound on regret. However, computational costs increase exponentially with the number of parameters to be optimized, decreasing search efficiency. We propose a BO that limits the search region to lower dimensions and utilizes local Gaussian process regression (LGPR) to scale the BO to higher dimensions. LGPR treats the low-dimensional search region as ""local,"" improving prediction accuracies there. The LGPR model is trained on a local subset of data specific to that region. This improves prediction accuracy and search efficiency and reduces the time complexity of matrix inversion in the Gaussian process regression. In evaluations with 20D Ackley and Rosenbrock functions, search efficiencies are equal to or higher than those of the compared methods, improved by about 69% and 40% from the case without LGPR. We apply our method to an automatic design task for a power semiconductor device. We successfully reduce the specific on-resistance to 25% better than a conventional method and 3.4% better than without LGPR."
https://arxiv.org/abs/2403.08317,2024-03-13,From Channel Measurement to Training Data for PHY Layer AI Applications,"['Michael Zentarra', 'Julian Ahrens', 'Lia Ahrens']","Learning-based techniques such as artificial intelligence (AI) and machine learning (ML) play an increasingly important role in the development of future communication networks. The success of a learning algorithm depends on the quality and quantity of the available training data. In the physical layer (PHY), channel information data can be obtained either through measurement campaigns or through simulations based on predefined channel models. Performing measurements can be time consuming while only gaining information about one specific position or scenario. Simulated data, on the other hand, are more generalized and reflect in most cases not a real environment but instead, a statistical approximation based on a mathematical model. This paper presents a procedure for acquiring channel data by means of fast and flexible software defined radio (SDR) based channel measurements along with a method for a parameter extraction that provides configuration input to the simulator. The procedure from the measurement to the simulated channel data is demonstrated in two exemplary propagation scenarios. It is shown, that in both cases the simulated data is in good accordance to the measurements"
https://arxiv.org/abs/2403.08314,2024-03-13,Is Context Helpful for Chat Translation Evaluation?,"['Sweta Agrawal', 'Amin Farajian', 'Patrick Fernandes', 'Ricardo Rei', 'André F. T. Martins']","Despite the recent success of automatic metrics for assessing translation quality, their application in evaluating the quality of machine-translated chats has been limited. Unlike more structured texts like news, chat conversations are often unstructured, short, and heavily reliant on contextual information. This poses questions about the reliability of existing sentence-level metrics in this domain as well as the role of context in assessing the translation quality. Motivated by this, we conduct a meta-evaluation of existing sentence-level automatic metrics, primarily designed for structured domains such as news, to assess the quality of machine-translated chats. We find that reference-free metrics lag behind reference-based ones, especially when evaluating translation quality in out-of-English settings. We then investigate how incorporating conversational contextual information in these metrics affects their performance. Our findings show that augmenting neural learned metrics with contextual information helps improve correlation with human judgments in the reference-free scenario and when evaluating translations in out-of-English settings. Finally, we propose a new evaluation metric, Context-MQM, that utilizes bilingual context with a large language model (LLM) and further validate that adding context helps even for LLM-based evaluation metrics."
https://arxiv.org/abs/2403.08298,2024-03-13,Physics-Informed Deep Learning for Motion-Corrected Reconstruction of Quantitative Brain MRI,"['Hannah Eichhorn', 'Veronika Spieker', 'Kerstin Hammernik', 'Elisa Saks', 'Kilian Weiss', 'Christine Preibisch', 'Julia A. Schnabel']","We propose PHIMO, a physics-informed learning-based motion correction method tailored to quantitative MRI. PHIMO leverages information from the signal evolution to exclude motion-corrupted k-space lines from a data-consistent reconstruction. We demonstrate the potential of PHIMO for the application of T2* quantification from gradient echo MRI, which is particularly sensitive to motion due to its sensitivity to magnetic field inhomogeneities. A state-of-the-art technique for motion correction requires redundant acquisition of the k-space center, prolonging the acquisition. We show that PHIMO can detect and exclude intra-scan motion events and, thus, correct for severe motion artifacts. PHIMO approaches the performance of the state-of-the-art motion correction method, while substantially reducing the acquisition time by over 40%, facilitating clinical applicability. Our code is available at https://github.com/HannahEichhorn/PHIMO."
https://arxiv.org/abs/2403.08294,2024-03-13,Attack Deterministic Conditional Image Generative Models for Diverse and Controllable Generation,"['Tianyi Chu', 'Wei Xing', 'Jiafu Chen', 'Zhizhong Wang', 'Jiakai Sun', 'Lei Zhao', 'Haibo Chen', 'Huaizhong Lin']","Existing generative adversarial network (GAN) based conditional image generative models typically produce fixed output for the same conditional input, which is unreasonable for highly subjective tasks, such as large-mask image inpainting or style transfer. On the other hand, GAN-based diverse image generative methods require retraining/fine-tuning the network or designing complex noise injection functions, which is computationally expensive, task-specific, or struggle to generate high-quality results. Given that many deterministic conditional image generative models have been able to produce high-quality yet fixed results, we raise an intriguing question: is it possible for pre-trained deterministic conditional image generative models to generate diverse results without changing network structures or parameters? To answer this question, we re-examine the conditional image generation tasks from the perspective of adversarial attack and propose a simple and efficient plug-in projected gradient descent (PGD) like method for diverse and controllable image generation. The key idea is attacking the pre-trained deterministic generative models by adding a micro perturbation to the input condition. In this way, diverse results can be generated without any adjustment of network structures or fine-tuning of the pre-trained models. In addition, we can also control the diverse results to be generated by specifying the attack direction according to a reference text or image. Our work opens the door to applying adversarial attack to low-level vision tasks, and experiments on various conditional image generation tasks demonstrate the effectiveness and superiority of the proposed method."
https://arxiv.org/abs/2403.08277,2024-03-13,VIGFace: Virtual Identity Generation Model for Face Image Synthesis,"['Minsoo Kim', 'Min-Cheol Sagong', 'Gi Pyo Nam', 'Junghyun Cho', 'Ig-Jae Kim']","Deep learning-based face recognition continues to face challenges due to its reliance on huge datasets obtained from web crawling, which can be costly to gather and raise significant real-world privacy concerns. To address this issue, we propose VIGFace, a novel framework capable of generating synthetic facial images. Initially, we train the face recognition model using a real face dataset and create a feature space for both real and virtual IDs where virtual prototypes are orthogonal to other prototypes. Subsequently, we generate synthetic images by using the diffusion model based on the feature space. Our proposed framework provides two significant benefits. Firstly, it allows for creating virtual facial images without concerns about portrait rights, guaranteeing that the generated virtual face images are clearly differentiated from existing individuals. Secondly, it serves as an effective augmentation method by incorporating real existing images. Further experiments demonstrate the efficacy of our framework, achieving state-of-the-art results from both perspectives without any external data."
https://arxiv.org/abs/2403.08271,2024-03-13,Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification,"['Long Lan', 'Fengxiang Wang', 'Shuyan Li', 'Xiangtao Zheng', 'Zengmao Wang', 'Xinwang Liu']","Fine-grained ship classification in remote sensing (RS-FGSC) poses a significant challenge due to the high similarity between classes and the limited availability of labeled data, limiting the effectiveness of traditional supervised classification methods. Recent advancements in large pre-trained Vision-Language Models (VLMs) have demonstrated impressive capabilities in few-shot or zero-shot learning, particularly in understanding image content. This study delves into harnessing the potential of VLMs to enhance classification accuracy for unseen ship categories, which holds considerable significance in scenarios with restricted data due to cost or privacy constraints. Directly fine-tuning VLMs for RS-FGSC often encounters the challenge of overfitting the seen classes, resulting in suboptimal generalization to unseen classes, which highlights the difficulty in differentiating complex backgrounds and capturing distinct ship features. To address these issues, we introduce a novel prompt tuning technique that employs a hierarchical, multi-granularity prompt design. Our approach integrates remote sensing ship priors through bias terms, learned from a small trainable network. This strategy enhances the model's generalization capabilities while improving its ability to discern intricate backgrounds and learn discriminative ship features. Furthermore, we contribute to the field by introducing a comprehensive dataset, FGSCM-52, significantly expanding existing datasets with more extensive data and detailed annotations for less common ship classes. Extensive experimental evaluations demonstrate the superiority of our proposed method over current state-of-the-art techniques. The source code will be made publicly available."
https://arxiv.org/abs/2403.08270,2024-03-13,Identity-aware Dual-constraint Network for Cloth-Changing Person Re-identification,"['Peini Guo', 'Mengyuan Liu', 'Hong Liu', 'Ruijia Fan', 'Guoquan Wang', 'Bin He']","Cloth-Changing Person Re-Identification (CC-ReID) aims to accurately identify the target person in more realistic surveillance scenarios, where pedestrians usually change their clothing. Despite great progress, limited cloth-changing training samples in existing CC-ReID datasets still prevent the model from adequately learning cloth-irrelevant features. In addition, due to the absence of explicit supervision to keep the model constantly focused on cloth-irrelevant areas, existing methods are still hampered by the disruption of clothing variations. To solve the above issues, we propose an Identity-aware Dual-constraint Network (IDNet) for the CC-ReID task. Specifically, to help the model extract cloth-irrelevant clues, we propose a Clothes Diversity Augmentation (CDA), which generates more realistic cloth-changing samples by enriching the clothing color while preserving the texture. In addition, a Multi-scale Constraint Block (MCB) is designed, which extracts fine-grained identity-related features and effectively transfers cloth-irrelevant knowledge. Moreover, a Counterfactual-guided Attention Module (CAM) is presented, which learns cloth-irrelevant features from channel and space dimensions and utilizes the counterfactual intervention for supervising the attention map to highlight identity-related regions. Finally, a Semantic Alignment Constraint (SAC) is designed to facilitate high-level semantic feature interaction. Comprehensive experiments on four CC-ReID datasets indicate that our method outperforms prior state-of-the-art approaches."
https://arxiv.org/abs/2403.08266,2024-03-13,Sketch2Manga: Shaded Manga Screening from Sketch with Diffusion Models,"['Jian Lin', 'Xueting Liu', 'Chengze Li', 'Minshan Xie', 'Tien-Tsin Wong']","While manga is a popular entertainment form, creating manga is tedious, especially adding screentones to the created sketch, namely manga screening. Unfortunately, there is no existing method that tailors for automatic manga screening, probably due to the difficulty of generating high-quality shaded high-frequency screentones. The classic manga screening approaches generally require user input to provide screentone exemplars or a reference manga image. The recent deep learning models enables the automatic generation by learning from a large-scale dataset. However, the state-of-the-art models still fail to generate high-quality shaded screentones due to the lack of a tailored model and high-quality manga training data. In this paper, we propose a novel sketch-to-manga framework that first generates a color illustration from the sketch and then generates a screentoned manga based on the intensity guidance. Our method significantly outperforms existing methods in generating high-quality manga with shaded high-frequency screentones."
https://arxiv.org/abs/2403.08258,2024-03-13,Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition,"['Wenjing Zhu', 'Sining Sun', 'Changhao Shan', 'Peng Fan', 'Qing Yang']","Conformer-based attention models have become the de facto backbone model for Automatic Speech Recognition tasks. A blank symbol is usually introduced to align the input and output sequences for CTC or RNN-T models. Unfortunately, the long input length overloads computational budget and memory consumption quadratically by attention mechanism. In this work, we propose a ""Skip-and-Recover"" Conformer architecture, named Skipformer, to squeeze sequence input length dynamically and inhomogeneously. Skipformer uses an intermediate CTC output as criteria to split frames into three groups: crucial, skipping and ignoring. The crucial group feeds into next conformer blocks and its output joint with skipping group by original temporal order as the final encoder output. Experiments show that our model reduces the input sequence length by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile, the model can achieve better recognition accuracy and faster inference speed than recent baseline models. Our code is open-sourced and available online."
https://arxiv.org/abs/2403.08247,2024-03-13,A Dual-domain Regularization Method for Ring Artifact Removal of X-ray CT,"['Hongyang Zhu', 'Xin Lu', 'Yanwei Qin', 'Xinran Yu', 'Tianjiao Sun', 'Yunsong Zhao']","Ring artifacts in computed tomography images, arising from the undesirable responses of detector units, significantly degrade image quality and diagnostic reliability. To address this challenge, we propose a dual-domain regularization model to effectively remove ring artifacts, while maintaining the integrity of the original CT image. The proposed model corrects the vertical stripe artifacts on the sinogram by innovatively updating the response inconsistency compensation coefficients of detector units, which is achieved by employing the group sparse constraint and the projection-view direction sparse constraint on the stripe artifacts. Simultaneously, we apply the sparse constraint on the reconstructed image to further rectified ring artifacts in the image domain. The key advantage of the proposed method lies in considering the relationship between the response inconsistency compensation coefficients of the detector units and the projection views, which enables a more accurate correction of the response of the detector units. An alternating minimization method is designed to solve the model. Comparative experiments on real photon counting detector data demonstrate that the proposed method not only surpasses existing methods in removing ring artifacts but also excels in preserving structural details and image fidelity."
https://arxiv.org/abs/2403.08244,2024-03-13,Evaluating the Efficiency and Cost-effectiveness of RPB-based CO2 Capture: A Comprehensive Approach to Simultaneous Design and Operating Condition Optimization,"['Howoun Jung', 'Nohjin Park', 'Jay H. Lee']","Despite ongoing global initiatives to reduce CO2 emissions, implementing large-scale CO2 capture using amine solvents is fraught with economic uncertainties and technical hurdles. The Rotating Packed Bed (RPB) presents a promising alternative to traditional packed towers, offering compact design and adaptability. Nonetheless, scaling RPB processes to an industrial level is challenging due to the nascent nature of its application. The complexity of designing RPB units, setting operating conditions, and evaluating process performance adds layers of difficulty to the adoption of RPB-based systems in industries. This study introduces an optimization-driven design and evaluation for CO2 capture processes utilizing RPB columns. By employing detailed process simulation, we aim to concurrently optimize unit design and operating parameters, underscoring its advantage over conventional sequential approaches. Our process design method integrates heuristic design recommendations as constraints, resulting in 9.4% to 12.7% cost savings compared to conventional sequential design methods. Furthermore, our comprehensive process-level analysis reveals that using concentrated MEA solvent can yield total cost savings of 13.4% to 25.0% compared to the standard 30wt% MEA solvent. Additionally, the RPB unit can deliver an 8.5 to 23.6 times reduction in packing volume. While the commercial-scale feasibility of RPB technology has been established, the advancement of this field hinges on acquiring a broader and more robust dataset from commercial-scale implementations. Employing strategic methods like modularization could significantly reduce the entry barriers for CO2 capture projects, facilitating their broader adoption and implementation."
https://arxiv.org/abs/2403.08240,2024-03-13,Capturing electronic correlations in electron-phonon interactions in molecular systems with the GW approximation,"['Antonios M. Alvertis', 'David B. Williams-Young', 'Fabien Bruneval', 'Jeffrey B. Neaton']","Electron-phonon interactions are of great importance to a variety of physical phenomena, and their accurate description is an important goal for first-principles calculations. Isolated examples of materials and molecular systems have emerged where electron-phonon coupling is enhanced over density functional theory (DFT) when using the Green's-function-based ab initio GW method, which provides a more accurate description of electronic correlations. It is however unclear how general this enhancement is, and how employing high-end quantum chemistry methods, which further improve the description of electronic correlations, might further alter electron-phonon interactions over GW or DFT. Here, we address these questions by computing the renormalization of the highest occupied molecular orbital energies of Thiel's set of organic molecules by harmonic vibrations using DFT, GW and equation-of-motion coupled-cluster calculations. We find that GW can increase the magnitude of the electron-phonon coupling across this set of molecules by an average factor of 1.1-1.8 compared to DFT, while equation-of-motion coupled-cluster leads to an increase of 1.4-2. The electron-phonon coupling predicted with the ab initio GW method is generally in much closer agreement to coupled cluster values compared to DFT, establishing GW as an accurate way of computing electron-phonon phenomena in molecules and beyond at a much lower computational cost than higher-end quantum chemistry techniques."
https://arxiv.org/abs/2403.08239,2024-03-13,Continuous Object State Recognition for Cooking Robots Using Pre-Trained Vision-Language Models and Black-box Optimization,"['Kento Kawaharazuka', 'Naoaki Kanazawa', 'Yoshiki Obinata', 'Kei Okada', 'Masayuki Inaba']","The state recognition of the environment and objects by robots is generally based on the judgement of the current state as a classification problem. On the other hand, state changes of food in cooking happen continuously and need to be captured not only at a certain time point but also continuously over time. In addition, the state changes of food are complex and cannot be easily described by manual programming. Therefore, we propose a method to recognize the continuous state changes of food for cooking robots through the spoken language using pre-trained large-scale vision-language models. By using models that can compute the similarity between images and texts continuously over time, we can capture the state changes of food while cooking. We also show that by adjusting the weighting of each text prompt based on fitting the similarity changes to a sigmoid function and then performing black-box optimization, more accurate and robust continuous state recognition can be achieved. We demonstrate the effectiveness and limitations of this method by performing the recognition of water boiling, butter melting, egg cooking, and onion stir-frying."
https://arxiv.org/abs/2403.08233,2024-03-13,A Parallel Beam Splitting Based on Gradient Metasurface: Preparation and Fusion of Quantum Entanglement,"['Qi Liu', 'Xuan Liu', 'Yu Tian', 'Zhaohua Tian', 'Guixin Li', 'Xi-Feng Ren', 'Qihuang Gong', 'Ying Gu']","Gradient metasurface, formed by a set of subwavelength unit cells with different phase modulation, is widely used in polarized beam splitting (BS) in the classical and quantum optics. Specifically, its phase gradient allows the path and polarization of multiple output lights to be locked by corresponding inputs.Using this unique path-polarization locked property, we demonstrate that the single metasurface can function as sequentially linked beamsplitters, enabling the parallelization of a series of BS processes. Such a parallel BS metasurface provides a multi-beam interference capability for both classical and quantum light manipulation. Taking this advantage, we first prepare path and polarization hybrid entangled states of two, three, and multi photons from unentangled photon sources. Then, the ability of parallel BS-facilitated entanglement is applied to demonstrate entanglement fusion among entangled photon pairs, which can greatly enlarge the entanglement dimension. The principle of parallel BS through the metasurface opens up a versatile way to manipulate the quantum state at the micro/nano scale, which will have potential applications in on-chip quantum optics and quantum information processing."
https://arxiv.org/abs/2403.08229,2024-03-13,Boosting Disfluency Detection with Large Language Model as Disfluency Generator,"['Zhenrong Cheng', 'Jiayan Guo', 'Hao Sun', 'Yan Zhang']","Current disfluency detection methods heavily rely on costly and scarce human-annotated data. To tackle this issue, some approaches employ heuristic or statistical features to generate disfluent sentences, partially improving detection performance. However, these sentences often deviate from real-life scenarios, constraining overall model enhancement. In this study, we propose a lightweight data augmentation approach for disfluency detection, utilizing the superior generative and semantic understanding capabilities of large language model (LLM) to generate disfluent sentences as augmentation data. We leverage LLM to generate diverse and more realistic sentences guided by specific prompts, without the need for fine-tuning the LLM. Subsequently, we apply an uncertainty-aware data filtering approach to improve the quality of the generated sentences, utilized in training a small detection model for improved performance. Experiments using enhanced data yielded state-of-the-art results. The results showed that using a small amount of LLM-generated enhanced data can significantly improve performance, thereby further enhancing cost-effectiveness."
https://arxiv.org/abs/2403.08227,2024-03-13,Matching Non-Identical Objects,"['Yusuke Marumo', 'Kazuhiko Kawamoto', 'Hiroshi Kera']","Not identical but similar objects are everywhere in the world. Examples include four-legged animals such as dogs and cats, cars of different models, akin flowers in various colors, and countless others. In this study, we address a novel task of matching such non-identical objects. We propose a simple weighting scheme of descriptors that enhance various sparse image matching methods, which are originally designed for matching identical objects captured from different perspectives, and achieve semantically robust matching. The experiments show successful matching between non-identical objects in various cases including domain shift. Further, we present a first evaluation of the robustness of the image matching methods under common corruptions, which is a sort of domain shift, and the proposed method improves the matching in this case as well."
https://arxiv.org/abs/2403.08220,2024-03-12,Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators,"['Lianghao Cao', ""Thomas O'Leary-Roseberry"", 'Omar Ghattas']","We propose an operator learning approach to accelerate geometric Markov chain Monte Carlo (MCMC) for solving infinite-dimensional nonlinear Bayesian inverse problems. While geometric MCMC employs high-quality proposals that adapt to posterior local geometry, it requires computing local gradient and Hessian information of the log-likelihood, incurring a high cost when the parameter-to-observable (PtO) map is defined through expensive model simulations. We consider a delayed-acceptance geometric MCMC method driven by a neural operator surrogate of the PtO map, where the proposal is designed to exploit fast surrogate approximations of the log-likelihood and, simultaneously, its gradient and Hessian. To achieve a substantial speedup, the surrogate needs to be accurate in predicting both the observable and its parametric derivative (the derivative of the observable with respect to the parameter). Training such a surrogate via conventional operator learning using input--output samples often demands a prohibitively large number of model simulations. In this work, we present an extension of derivative-informed operator learning [O'Leary-Roseberry et al., J. Comput. Phys., 496 (2024)] using input--output--derivative training samples. Such a learning method leads to derivative-informed neural operator (DINO) surrogates that accurately predict the observable and its parametric derivative at a significantly lower training cost than the conventional method. Cost and error analysis for reduced basis DINO surrogates are provided. Numerical studies on PDE-constrained Bayesian inversion demonstrate that DINO-driven MCMC generates effective posterior samples 3--9 times faster than geometric MCMC and 60--97 times faster than prior geometry-based MCMC. Furthermore, the training cost of DINO surrogates breaks even after collecting merely 10--25 effective posterior samples compared to geometric MCMC."
https://arxiv.org/abs/2403.08216,2024-03-12,PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise,"['Qinglong Meng', 'Chongkun Xia', 'Xueqian Wang']","Normalizing flow is a generative modeling approach with efficient sampling. However, Flow-based models suffer two issues, which are manifold and discrete data. If the target distribution is a manifold, which means the dimension of the latent target distribution and the dimension of the data distribution are unmatched, flow-based models might perform badly. Discrete data makes flow-based models collapse into a degenerate mixture of point masses. In this paper, to sidestep such two issues we propose PaddingFlow, a novel dequantization method, which improves normalizing flows with padding-dimensional noise. PaddingFlow is easy to implement, computationally cheap, widely suitable for various tasks, and generates samples that are unbiased estimations of the data. Especially, our method can overcome the limitation of existing dequantization methods that have to change the data distribution, which might degrade performance. We validate our method on the main benchmarks of unconditional density estimation, including five tabular datasets and four image datasets for VAE models, and the IK experiments which are conditional density estimation. The results show that PaddingFlow can provide improvement on all tasks in this paper."
https://arxiv.org/abs/2403.08208,2024-03-12,Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks,"['Khondoker Murad Hossain', 'Tim Oates']","In the rapidly evolving landscape of communication and network security, the increasing reliance on deep neural networks (DNNs) and cloud services for data processing presents a significant vulnerability: the potential for backdoors that can be exploited by malicious actors. Our approach leverages advanced tensor decomposition algorithms Independent Vector Analysis (IVA), Multiset Canonical Correlation Analysis (MCCA), and Parallel Factor Analysis (PARAFAC2) to meticulously analyze the weights of pre-trained DNNs and distinguish between backdoored and clean models effectively. The key strengths of our method lie in its domain independence, adaptability to various network architectures, and ability to operate without access to the training data of the scrutinized models. This not only ensures versatility across different application scenarios but also addresses the challenge of identifying backdoors without prior knowledge of the specific triggers employed to alter network behavior. We have applied our detection pipeline to three distinct computer vision datasets, encompassing both image classification and object detection tasks. The results demonstrate a marked improvement in both accuracy and efficiency over existing backdoor detection methods. This advancement enhances the security of deep learning and AI in networked systems, providing essential cybersecurity against evolving threats in emerging technologies."
https://arxiv.org/abs/2403.08207,2024-03-12,BG-HGNN: Toward Scalable and Efficient Heterogeneous Graph Neural Network,"['Junwei Su', 'Lingjun Mao', 'Chuan Wu']","Many computer vision and machine learning problems are modelled as learning tasks on heterogeneous graphs, featuring a wide array of relations from diverse types of nodes and edges. Heterogeneous graph neural networks (HGNNs) stand out as a promising neural model class designed for heterogeneous graphs. Built on traditional GNNs, existing HGNNs employ different parameter spaces to model the varied relationships. However, the practical effectiveness of existing HGNNs is often limited to simple heterogeneous graphs with few relation types. This paper first highlights and demonstrates that the standard approach employed by existing HGNNs inevitably leads to parameter explosion and relation collapse, making HGNNs less effective or impractical for complex heterogeneous graphs with numerous relation types. To overcome this issue, we introduce a novel framework, Blend&Grind-HGNN (BG-HGNN), which effectively tackles the challenges by carefully integrating different relations into a unified feature space manageable by a single set of parameters. This results in a refined HGNN method that is more efficient and effective in learning from heterogeneous graphs, especially when the number of relations grows. Our empirical studies illustrate that BG-HGNN significantly surpasses existing HGNNs in terms of parameter efficiency (up to 28.96 $\times$), training throughput (up to 8.12 $\times$), and accuracy (up to 1.07 $\times$)."
https://arxiv.org/abs/2403.08204,2024-03-12,AutoDFP: Automatic Data-Free Pruning via Channel Similarity Reconstruction,"['Siqi Li', 'Jun Chen', 'Jingyang Xiang', 'Chengrui Zhu', 'Yong Liu']","Structured pruning methods are developed to bridge the gap between the massive scale of neural networks and the limited hardware resources. Most current structured pruning methods rely on training datasets to fine-tune the compressed model, resulting in high computational burdens and being inapplicable for scenarios with stringent requirements on privacy and security. As an alternative, some data-free methods have been proposed, however, these methods often require handcraft parameter tuning and can only achieve inflexible reconstruction. In this paper, we propose the Automatic Data-Free Pruning (AutoDFP) method that achieves automatic pruning and reconstruction without fine-tuning. Our approach is based on the assumption that the loss of information can be partially compensated by retaining focused information from similar channels. Specifically, We formulate data-free pruning as an optimization problem, which can be effectively addressed through reinforcement learning. AutoDFP assesses the similarity of channels for each layer and provides this information to the reinforcement learning agent, guiding the pruning and reconstruction process of the network. We evaluate AutoDFP with multiple networks on multiple datasets, achieving impressive compression results. For instance, on the CIFAR-10 dataset, AutoDFP demonstrates a 2.87\% reduction in accuracy loss compared to the recently proposed data-free pruning method DFPC with fewer FLOPs on VGG-16. Furthermore, on the ImageNet dataset, AutoDFP achieves 43.17\% higher accuracy than the SOTA method with the same 80\% preserved ratio on MobileNet-V1."
https://arxiv.org/abs/2403.08198,2024-03-12,Validating and Exploring Large Geographic Corpora,['Jonathan Dunn'],"This paper investigates the impact of corpus creation decisions on large multi-lingual geographic web corpora. Beginning with a 427 billion word corpus derived from the Common Crawl, three methods are used to improve the quality of sub-corpora representing specific language-country pairs like New Zealand English: (i) the agreement of independent language identification systems, (ii) hash-based deduplication, and (iii) location-specific outlier detection. The impact of each of these steps is then evaluated at the language level and the country level by using corpus similarity measures to compare each resulting corpus with baseline data sets. The goal is to understand the impact of upstream data cleaning decisions on downstream corpora with a specific focus on under-represented languages and populations. The evaluation shows that the validity of sub-corpora is improved with each stage of cleaning but that this improvement is unevenly distributed across languages and populations. This result shows how standard corpus creation techniques can accidentally exclude under-represented populations."
https://arxiv.org/abs/2403.08193,2024-03-12,Learning-driven Physically-aware Large-scale Circuit Gate Sizing,"['Yuyang Ye', 'Peng Xu', 'Lizheng Ren', 'Tinghuan Chen', 'Hao Yan', 'Bei Yu', 'Longxing Shi']","Gate sizing plays an important role in timing optimization after physical design. Existing machine learning-based gate sizing works cannot optimize timing on multiple timing paths simultaneously and neglect the physical constraint on layouts. They cause sub-optimal sizing solutions and low-efficiency issues when compared with commercial gate sizing tools. In this work, we propose a learning-driven physically-aware gate sizing framework to optimize timing performance on large-scale circuits efficiently. In our gradient descent optimization-based work, for obtaining accurate gradients, a multi-modal gate sizing-aware timing model is achieved via learning timing information on multiple timing paths and physical information on multiple-scaled layouts jointly. Then, gradient generation based on the sizing-oriented estimator and adaptive back-propagation are developed to update gate sizes. Our results demonstrate that our work achieves higher timing performance improvements in a faster way compared with the commercial gate sizing tool."
https://arxiv.org/abs/2403.08191,2024-03-12,Synchronized Dual-arm Rearrangement via Cooperative mTSP,"['Wenhao Li', 'Shishun Zhang', 'Sisi Dai', 'Hui Huang', 'Ruizhen Hu', 'Xiaohong Chen', 'Kai Xu']","Synchronized dual-arm rearrangement is widely studied as a common scenario in industrial applications. It often faces scalability challenges due to the computational complexity of robotic arm rearrangement and the high-dimensional nature of dual-arm planning. To address these challenges, we formulated the problem as cooperative mTSP, a variant of mTSP where agents share cooperative costs, and utilized reinforcement learning for its solution. Our approach involved representing rearrangement tasks using a task state graph that captured spatial relationships and a cooperative cost matrix that provided details about action costs. Taking these representations as observations, we designed an attention-based network to effectively combine them and provide rational task scheduling. Furthermore, a cost predictor is also introduced to directly evaluate actions during both training and planning, significantly expediting the planning process. Our experimental results demonstrate that our approach outperforms existing methods in terms of both performance and planning efficiency."
https://arxiv.org/abs/2403.08184,2024-03-12,Quantum skyrmion dynamics studied by neural network quantum states,"['Ashish Joshi', 'Robert Peters', 'Thore Posske']","We study the dynamics of quantum skyrmions under a magnetic field gradient using neural network quantum states. First, we obtain a quantum skyrmion lattice ground state using variational Monte Carlo with a restricted Boltzmann machine as the variational ansatz for a quantum Heisenberg model with Dzyaloshinskii-Moriya interaction. Then, using the time-dependent variational principle, we study the real-time evolution of quantum skyrmions after a Hamiltonian quench with an inhomogeneous external magnetic field. We show that field gradients are an effective way of manipulating and moving quantum skyrmions. Furthermore, we demonstrate that quantum skyrmions can decay when interacting with each other. This work shows that neural network quantum states offer a promising way of studying the real-time evolution of quantum magnetic systems that are outside the realm of exact diagonalization."
https://arxiv.org/abs/2403.08175,2024-03-12,A forward-modelling approach to overcome PSF smearing and fit flexible models to the chemical structure of galaxies,"['Benjamin Metha', 'Simon Birrer', 'Tommaso Treu', 'Michele Trenti', 'Xuheng Ding', 'Xin Wang']","Historically, metallicity profiles of galaxies have been modelled using a radially symmetric, two-parameter linear model, which reveals that most galaxies are more metal-rich in their central regions than their outskirts. However, this model is known to yield inaccurate results when the point-spread function (PSF) of a telescope is large. Furthermore, a radially symmetric model cannot capture asymmetric structures within a galaxy. In this work, we present an extension of the popular forward-modelling python package LENSTRONOMY, which allows the user to overcome both of these obstacles. We demonstrate the new features of this code base through two illustrative examples on simulated data. First, we show that through forward modelling, LENSTRONOMY is able to recover accurately the metallicity gradients of galaxies, even when the PSF is comparable to the size of a galaxy, as long as the data is observed with a sufficient number of pixels. Additionally, we demonstrate how LENSTRONOMY is able to fit irregular metallicity profiles to galaxies that are not well-described by a simple surface brightness profile. This opens up pathways for detailed investigations into the connections between morphology and chemical structure for galaxies at cosmological distances using the transformative capabilities of JWST. Our code is publicly available and open source, and can also be used to model spatial distributions of other galaxy properties that are traced by its surface brightness profile."
https://arxiv.org/abs/2403.08173,2024-03-12,"A bargain for mergesorts (functional pearl) -- How to prove your mergesort correct and stable, almost for free","['Cyril Cohen', 'Kazuhiko Sakaguchi']","We present a novel characterization of stable mergesort functions using relational parametricity, and show that it implies the correctness of mergesort. As a result, one can prove the correctness of several variations of mergesort (e.g., top-down, bottom-up, tail-recursive, non-tail-recursive, smooth, and non-smooth mergesorts) by proving the characterization property for each variation. To further motivate this work, we show a performance trade-off between tail-recursive and non-tail-recursive mergesorts that (1) the former in call-by-value evaluation avoids using up stack space and is efficient and (2) the latter in call-by-need evaluation is an optimal incremental sort, meaning that it performs only $\mathcal{O}(n + k \log k)$ comparisons to compute the least (or greatest) $k$ items of a list of length $n$. Thanks to our characterization and the parametricity translation, we deduced the correctness results, including stability, of various implementations of mergesort for lists, including highly optimized ones, in the Coq proof assistant."
https://arxiv.org/abs/2403.08164,2024-03-12,EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech,"['Ziqi Liang', 'Haoxiang Shi', 'Jiawei Wang', 'Keda Lu']","Recently, deep learning-based Text-to-Speech (TTS) systems have achieved high-quality speech synthesis results. Recurrent neural networks have become a standard modeling technique for sequential data in TTS systems and are widely used. However, training a TTS model which includes RNN components requires powerful GPU performance and takes a long time. In contrast, CNN-based sequence synthesis techniques can significantly reduce the parameters and training time of a TTS model while guaranteeing a certain performance due to their high parallelism, which alleviate these economic costs of training. In this paper, we propose a lightweight TTS system based on deep convolutional neural networks, which is a two-stage training end-to-end TTS model and does not employ any recurrent units. Our model consists of two stages: Text2Spectrum and SSRN. The former is used to encode phonemes into a coarse mel spectrogram and the latter is used to synthesize the complete spectrum from the coarse mel spectrogram. Meanwhile, we improve the robustness of our model by a series of data augmentations, such as noise suppression, time warping, frequency masking and time masking, for solving the low resource mongolian problem. Experiments show that our model can reduce the training time and parameters while ensuring the quality and naturalness of the synthesized speech compared to using mainstream TTS models. Our method uses NCMMSC2022-MTTSC Challenge dataset for validation, which significantly reduces training time while maintaining a certain accuracy."
https://arxiv.org/abs/2403.08160,2024-03-12,Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime,"['Hong Hu', 'Yue M. Lu', 'Theodor Misiakiewicz']","Recent advances in machine learning have been achieved by using overparametrized models trained until near interpolation of the training data. It was shown, e.g., through the double descent phenomenon, that the number of parameters is a poor proxy for the model complexity and generalization capabilities. This leaves open the question of understanding the impact of parametrization on the performance of these models. How does model complexity and generalization depend on the number of parameters $p$? How should we choose $p$ relative to the sample size $n$ to achieve optimal test error?"
https://arxiv.org/abs/2403.08156,2024-03-12,NeRF-Supervised Feature Point Detection and Description,"['Ali Youssef', 'Francisco Vasconcelos']","Feature point detection and description is the backbone for various computer vision applications, such as Structure-from-Motion, visual SLAM, and visual place recognition. While learning-based methods have surpassed traditional handcrafted techniques, their training often relies on simplistic homography-based simulations of multi-view perspectives, limiting model generalisability. This paper introduces a novel approach leveraging neural radiance fields (NeRFs) for realistic multi-view training data generation. We create a diverse multi-view dataset using NeRFs, consisting of indoor and outdoor scenes. Our proposed methodology adapts state-of-the-art feature detectors and descriptors to train on NeRF-synthesised views supervised by perspective projective geometry. Our experiments demonstrate that the proposed methods achieve competitive or superior performance on standard benchmarks for relative pose estimation, point cloud registration, and homography estimation while requiring significantly less training data compared to existing approaches."
https://arxiv.org/abs/2403.08147,2024-03-12,Representing Molecules as Random Walks Over Interpretable Grammars,"['Michael Sun', 'Minghao Guo', 'Weize Yuan', 'Veronika Thost', 'Crystal Elaine Owens', 'Aristotle Franklin Grosz', 'Sharvaa Selvan', 'Katelyn Zhou', 'Hassan Mohiuddin', 'Benjamin J Pedretti', 'Zachary P Smith', 'Jie Chen', 'Wojciech Matusik']","Recent research in molecular discovery has primarily been devoted to small, drug-like molecules, leaving many similarly important applications in material design without adequate technology. These applications often rely on more complex molecular structures with fewer examples that are carefully designed using known substructures. We propose a data-efficient and interpretable model for representing and reasoning over such molecules in terms of graph grammars that explicitly describe the hierarchical design space featuring motifs to be the design basis. We present a novel representation in the form of random walks over the design space, which facilitates both molecule generation and property prediction. We demonstrate clear advantages over existing methods in terms of performance, efficiency, and synthesizability of predicted molecules, and we provide detailed insights into the method's chemical interpretability."
https://arxiv.org/abs/2403.08136,2024-03-12,RoboCertProb: Property Specification for Probabilistic RoboChart Models,"['Kangfeng Ye', 'Jim Woodcock']","RoboChart is a core notation in the RoboStar framework which brings modern modelling and formal verification technologies into software engineering for robotics. It is a timed and probabilistic domain-specific language for robotics and provides a UML-like architectural and state machine modelling. This work presents RoboCertProb for specifying quantitative properties of probabilistic robotic systems modelled in RoboChart. RoboCertProb's semantics is based on PCTL*. To interpret RoboCertProb over RoboChart models, we give a Markov semantics (DTMCs and MDPs) to RoboChart, derived from its existing transformation semantics to the PRISM language. In addition to property specification, RoboCertProb also entitles us to configure loose constants and unspecified functions and operations in RoboChart models. It allows us to set up environmental inputs to verify reactive probabilistic systems not directly supported in probabilistic model checkers like PRISM because they employ a closed-world assumption. We implement RoboCertProb in an accompanying tool of RoboChart, RoboTool, for specifying properties and automatically generating PRISM properties from them to formally verify RoboChart models using PRISM. We have used it to analyse the behaviour of software controllers for two real robots: an industrial painting robot and an agricultural robot for treating plants with UV lights."
https://arxiv.org/abs/2403.08125,2024-03-12,Q-SLAM: Quadric Representations for Monocular SLAM,"['Chensheng Peng', 'Chenfeng Xu', 'Yue Wang', 'Mingyu Ding', 'Heng Yang', 'Masayoshi Tomizuka', 'Kurt Keutzer', 'Marco Pavone', 'Wei Zhan']","Monocular SLAM has long grappled with the challenge of accurately modeling 3D geometries. Recent advances in Neural Radiance Fields (NeRF)-based monocular SLAM have shown promise, yet these methods typically focus on novel view synthesis rather than precise 3D geometry modeling. This focus results in a significant disconnect between NeRF applications, i.e., novel-view synthesis and the requirements of SLAM. We identify that the gap results from the volumetric representations used in NeRF, which are often dense and noisy. In this study, we propose a novel approach that reimagines volumetric representations through the lens of quadric forms. We posit that most scene components can be effectively represented as quadric planes. Leveraging this assumption, we reshape the volumetric representations with million of cubes by several quadric planes, which leads to more accurate and efficient modeling of 3D scenes in SLAM contexts. Our method involves two key steps: First, we use the quadric assumption to enhance coarse depth estimations obtained from tracking modules, e.g., Droid-SLAM. This step alone significantly improves depth estimation accuracy. Second, in the subsequent mapping phase, we diverge from previous NeRF-based SLAM methods that distribute sampling points across the entire volume space. Instead, we concentrate sampling points around quadric planes and aggregate them using a novel quadric-decomposed Transformer. Additionally, we introduce an end-to-end joint optimization strategy that synchronizes pose estimation with 3D reconstruction."
https://arxiv.org/abs/2403.08124,2024-03-12,Towards Independence Criterion in Machine Unlearning of Features and Labels,"['Ling Han', 'Nanqing Luo', 'Hao Huang', 'Jing Chen', 'Mary-Anne Hartley']","This work delves into the complexities of machine unlearning in the face of distributional shifts, particularly focusing on the challenges posed by non-uniform feature and label removal. With the advent of regulations like the GDPR emphasizing data privacy and the right to be forgotten, machine learning models face the daunting task of unlearning sensitive information without compromising their integrity or performance. Our research introduces a novel approach that leverages influence functions and principles of distributional independence to address these challenges. By proposing a comprehensive framework for machine unlearning, we aim to ensure privacy protection while maintaining model performance and adaptability across varying distributions. Our method not only facilitates efficient data removal but also dynamically adjusts the model to preserve its generalization capabilities. Through extensive experimentation, we demonstrate the efficacy of our approach in scenarios characterized by significant distributional shifts, making substantial contributions to the field of machine unlearning. This research paves the way for developing more resilient and adaptable unlearning techniques, ensuring models remain robust and accurate in the dynamic landscape of data privacy and machine learning."
https://arxiv.org/abs/2403.08108,2024-03-12,TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection,"['Hanning Chen', 'Wenjun Huang', 'Yang Ni', 'Sanggeon Yun', 'Fei Wen', 'Hugo Latapie', 'Mohsen Imani']","Task-oriented object detection aims to find objects suitable for accomplishing specific tasks. As a challenging task, it requires simultaneous visual data processing and reasoning under ambiguous semantics. Recent solutions are mainly all-in-one models. However, the object detection backbones are pre-trained without text supervision. Thus, to incorporate task requirements, their intricate models undergo extensive learning on a highly imbalanced and scarce dataset, resulting in capped performance, laborious training, and poor generalizability. In contrast, we propose TaskCLIP, a more natural two-stage design composed of general object detection and task-guided object selection. Particularly for the latter, we resort to the recently successful large Vision-Language Models (VLMs) as our backbone, which provides rich semantic knowledge and a uniform embedding space for images and texts. Nevertheless, the naive application of VLMs leads to sub-optimal quality, due to the misalignment between embeddings of object images and their visual attributes, which are mainly adjective phrases. To this end, we design a transformer-based aligner after the pre-trained VLMs to re-calibrate both embeddings. Finally, we employ a trainable score function to post-process the VLM matching results for object selection. Experimental results demonstrate that our TaskCLIP outperforms the state-of-the-art DETR-based model TOIST by 3.5% and only requires a single NVIDIA RTX 4090 for both training and inference."
https://arxiv.org/abs/2403.08100,2024-03-12,Efficient Language Model Architectures for Differentially Private Federated Learning,"['Jae Hun Ro', 'Srinadh Bhojanapalli', 'Zheng Xu', 'Yanxiang Zhang', 'Ananda Theertha Suresh']","Cross-device federated learning (FL) is a technique that trains a model on data distributed across typically millions of edge devices without data leaving the devices. SGD is the standard client optimizer for on device training in cross-device FL, favored for its memory and computational efficiency. However, in centralized training of neural language models, adaptive optimizers are preferred as they offer improved stability and performance. In light of this, we ask if language models can be modified such that they can be efficiently trained with SGD client optimizers and answer this affirmatively."
https://arxiv.org/abs/2403.08092,2024-03-12,Mitigating the Impact of Attribute Editing on Face Recognition,"['Sudipta Banerjee', 'Sai Pranaswi Mullangi', 'Shruti Wagle', 'Chinmay Hegde', 'Nasir Memon']","Facial attribute editing using generative models can impair automated face recognition. This degradation persists even with recent identity-preserving models such as InstantID. To mitigate this issue, we propose two techniques that perform local and global attribute editing. Local editing operates on the finer details via a regularization-free method based on ControlNet conditioned on depth maps and auxiliary semantic segmentation masks. Global editing operates on coarser details via a regularization-based method guided by custom loss and regularization set. In this work, we empirically ablate twenty-six facial semantic, demographic and expression-based attributes altered using state-of-the-art generative models and evaluate them using ArcFace and AdaFace matchers on CelebA, CelebAMaskHQ and LFW datasets. Finally, we use LLaVA, a vision-language framework for attribute prediction to validate our editing techniques. Our methods outperform SoTA (BLIP, InstantID) at facial editing while retaining identity."
https://arxiv.org/abs/2403.08086,2024-03-12,Flow-Based Visual Stream Compression for Event Cameras,"['Daniel C. Stumpp', 'Himanshu Akolkar', 'Alan D. George', 'Ryad Benosman']","As the use of neuromorphic, event-based vision sensors expands, the need for compression of their output streams has increased. While their operational principle ensures event streams are spatially sparse, the high temporal resolution of the sensors can result in high data rates from the sensor depending on scene dynamics. For systems operating in communication-bandwidth-constrained and power-constrained environments, it is essential to compress these streams before transmitting them to a remote receiver. Therefore, we introduce a flow-based method for the real-time asynchronous compression of event streams as they are generated. This method leverages real-time optical flow estimates to predict future events without needing to transmit them, therefore, drastically reducing the amount of data transmitted. The flow-based compression introduced is evaluated using a variety of methods including spatiotemporal distance between event streams. The introduced method itself is shown to achieve an average compression ratio of 2.81 on a variety of event-camera datasets with the evaluation configuration used. That compression is achieved with a median temporal error of 0.48 ms and an average spatiotemporal event-stream distance of 3.07. When combined with LZMA compression for non-real-time applications, our method can achieve state-of-the-art average compression ratios ranging from 10.45 to 17.24. Additionally, we demonstrate that the proposed prediction algorithm is capable of performing real-time, low-latency event prediction."
https://arxiv.org/abs/2403.08063,2024-03-12,Towards Code Generation for Octree-Based Multigrid Solvers,"['Richard Angersbach', 'Sebastian Kuckuck', 'Harald Köstler']","This paper presents a novel method designed to generate multigrid solvers optimized for octree-based software frameworks. Our approach focuses on accurately capturing local features within a domain while leveraging the efficiency inherent in multigrid techniques. We outline the essential steps involved in generating specialized kernels for local refinement and communication routines, integrating on-the-fly interpolations to seamlessly transfer information between refinement levels. For this purpose, we established a software coupling via an automatic fusion of generated multigrid solvers and communication kernels with manual implementations of complex octree data structures and algorithms often found in established software frameworks. We demonstrate the effectiveness of our method through numerical experiments with different interpolation orders. Large-scale benchmarks conducted on the SuperMUC-NG CPU cluster underscore the advantages of our approach, offering a comparison against a reference implementation to highlight the benefits of our method and code generation in general."
https://arxiv.org/abs/2403.08059,2024-03-12,FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation,"['Benjamin D. Killeen', 'Liam J. Wang', 'Han Zhang', 'Mehran Armand', 'Russell H. Taylor', 'Greg Osgood', 'Mathias Unberath']","Automated X-ray image segmentation would accelerate research and development in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving specific image analysis problems, but the utility of these models is restricted to their particular task domain, and expanding to broader use requires additional data, labels, and retraining efforts. Recently, foundation models (FMs) -- machine learning models trained on large amounts of highly variable data thus enabling broad applicability -- have emerged as promising tools for automated image analysis. Existing FMs for medical image analysis focus on scenarios and modalities where objects are clearly defined by visually apparent boundaries, such as surgical tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally offer such clearly delineated boundaries or structure priors. During X-ray image formation, complex 3D structures are projected in transmission onto the imaging plane, resulting in overlapping features of varying opacity and shape. To pave the way toward an FM for comprehensive and automated analysis of arbitrary medical X-ray images, we develop FluoroSAM, a language-aligned variant of the Segment-Anything Model, trained from scratch on 1.6M synthetic X-ray images. FluoroSAM is trained on data including masks for 128 organ types and 464 non-anatomical objects, such as tools and implants. In real X-ray images of cadaveric specimens, FluoroSAM is able to segment bony anatomical structures based on text-only prompting with 0.51 and 0.79 DICE with point-based refinement, outperforming competing SAM variants for all structures. FluoroSAM is also capable of zero-shot generalization to segmenting classes beyond the training set thanks to its language alignment, which we demonstrate for full lung segmentation on real chest X-rays."
https://arxiv.org/abs/2403.08058,2024-03-12,CHAI: Clustered Head Attention for Efficient LLM Inference,"['Saurabh Agarwal', 'Bilge Acun', 'Basil Homer', 'Mostafa Elhoushi', 'Yejin Lee', 'Shivaram Venkataraman', 'Dimitris Papailiopoulos', 'Carole-Jean Wu']","Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is one of the key components of LLMs, which can account for over 50% of LLMs memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73x without any fine-tuning required. CHAI achieves this with a maximum 3.2% deviation in accuracy across 3 different models (i.e. OPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets."
https://arxiv.org/abs/2403.08055,2024-03-12,DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design and Graph-Based Drag Prediction,"['Mohamed Elrefaie', 'Angela Dai', 'Faez Ahmed']","This study introduces DrivAerNet, a large-scale high-fidelity CFD dataset of 3D industry-standard car shapes, and RegDGCNN, a dynamic graph convolutional neural network model, both aimed at aerodynamic car design through machine learning. DrivAerNet, with its 4000 detailed 3D car meshes using 0.5 million surface mesh faces and comprehensive aerodynamic performance data comprising of full 3D pressure, velocity fields, and wall-shear stresses, addresses the critical need for extensive datasets to train deep learning models in engineering applications. It is 60\% larger than the previously available largest public dataset of cars, and is the only open-source dataset that also models wheels and underbody. RegDGCNN leverages this large-scale dataset to provide high-precision drag estimates directly from 3D meshes, bypassing traditional limitations such as the need for 2D image rendering or Signed Distance Fields (SDF). By enabling fast drag estimation in seconds, RegDGCNN facilitates rapid aerodynamic assessments, offering a substantial leap towards integrating data-driven methods in automotive design. Together, DrivAerNet and RegDGCNN promise to accelerate the car design process and contribute to the development of more efficient vehicles. To lay the groundwork for future innovations in the field, the dataset and code used in our study are publicly accessible at \url{https://github.com/Mohamedelrefaie/DrivAerNet}"
https://arxiv.org/abs/2403.08052,2024-03-12,A Computational Method for $H_2$-optimal Estimator and State Feedback Controller Synthesis for PDEs,"['Sachin Shivakumar', 'Matthew Peet']","In this paper, we present solvable, convex formulations of $H_2$-optimal state estimation and state-feedback control problems for a general class of linear Partial Differential Equations (PDEs) with one spatial dimension. These convex formulations are derived by using an analysis and control framework called the `Partial Integral Equation' (PIE) framework, which utilizes the PIE representation of infinite-dimensional systems. Since PIEs are parameterized by Partial Integral (PI) operators that form an algebra, $H_2$-optimal estimation and control problems for PIEs can be formulated as Linear PI Inequalities (LPIs). Furthermore, if a PDE admits a PIE representation, then the stability and $H_2$ performance of the PIE system implies that of the PDE system. Consequently, the optimal estimator and controller obtained for a PIE using LPIs provide the same stability and performance when applied to the corresponding PDE. These LPI optimization problems can be solved computationally using semi-definite programming solvers because such problems can be formulated using Linear Matrix Inequalities by using positive matrices to parameterize a cone of positive PI operators. We illustrate the application of these methods by constructing observers and controllers for some standard PDE examples."
https://arxiv.org/abs/2403.08040,2024-03-12,MicroT: Low-Energy and Adaptive Models for MCUs,"['Yushan Huang', 'Ranya Aloufi', 'Xavier Cadet', 'Yuchen Zhao', 'Payam Barnaghi', 'Hamed Haddadi']","We propose MicroT, a low-energy, multi-task adaptive model framework for resource-constrained MCUs. We divide the original model into a feature extractor and a classifier. The feature extractor is obtained through self-supervised knowledge distillation and further optimized into part and full models through model splitting and joint training. These models are then deployed on MCUs, with classifiers added and trained on local tasks, ultimately performing stage-decision for joint inference. In this process, the part model initially processes the sample, and if the confidence score falls below the set threshold, the full model will resume and continue the inference. We evaluate MicroT on two models, three datasets, and two MCU boards. Our experimental evaluation shows that MicroT effectively improves model performance and reduces energy consumption when dealing with multiple local tasks. Compared to the unoptimized feature extractor, MicroT can improve accuracy by up to 9.87%. On MCUs, compared to the standard full model inference, MicroT can save up to about 29.13% in energy consumption. MicroT also allows users to adaptively adjust the stage-decision ratio as needed, better balancing model performance and energy consumption. Under the standard stage-decision ratio configuration, MicroT can increase accuracy by 5.91% and save about 14.47% of energy consumption."
https://arxiv.org/abs/2403.08032,2024-03-12,LG-Traj: LLM Guided Pedestrian Trajectory Prediction,"['Pranav Singh Chib', 'Pravendra Singh']","Accurate pedestrian trajectory prediction is crucial for various applications, and it requires a deep understanding of pedestrian motion patterns in dynamic environments. However, existing pedestrian trajectory prediction methods still need more exploration to fully leverage these motion patterns. This paper investigates the possibilities of using Large Language Models (LLMs) to improve pedestrian trajectory prediction tasks by inducing motion cues. We introduce LG-Traj, a novel approach incorporating LLMs to generate motion cues present in pedestrian past/observed trajectories. Our approach also incorporates motion cues present in pedestrian future trajectories by clustering future trajectories of training data using a mixture of Gaussians. These motion cues, along with pedestrian coordinates, facilitate a better understanding of the underlying representation. Furthermore, we utilize singular value decomposition to augment the observed trajectories, incorporating them into the model learning process to further enhance representation learning. Our method employs a transformer-based architecture comprising a motion encoder to model motion patterns and a social decoder to capture social interactions among pedestrians. We demonstrate the effectiveness of our approach on popular pedestrian trajectory prediction benchmarks, namely ETH-UCY and SDD, and present various ablation experiments to validate our approach."
https://arxiv.org/abs/2403.08029,2024-03-12,HOLISMOKES -- XII. Time-delay Measurements of Strongly Lensed Type Ia Supernovae using a Long Short-Term Memory Network,"['S. Huber', 'S. H. Suyu']","Strongly lensed Type Ia supernovae (LSNe Ia) are a promising probe to measure the Hubble constant ($H_0$) directly. To use LSNe Ia for cosmography, a time-delay measurement between the multiple images, a lens-mass model, and a mass reconstruction along the line of sight are required. In this work, we present the machine learning network LSTM-FCNN which is a combination of a Long Short-Term Memory Network (LSTM) and a fully-connected neural network (FCNN). The LSTM-FCNN is designed to measure time delays on a sample of LSNe Ia spanning a broad range of properties, which we expect to find with the upcoming Rubin Observatory Legacy Survey of Space and Time (LSST) and for which follow-up observations are planned. With follow-up observations in $i$ band (cadence of one to three days with a single-epoch $5σ$ depth of 24.5 mag), we reach a bias-free delay measurement with a precision around 0.7 days over a large sample of LSNe Ia. The LSTM-FCNN is far more general than previous machine learning approaches such as the Random Forest (RF), where a RF has to be trained for each observational pattern separately, and yet the LSTM-FCNN outperforms the RF by a factor of roughly three. Therefore, the LSTM-FCNN is a very promising approach to achieve robust time delays in LSNe Ia, which is important for a precise and accurate constraint on $H_0$"
https://arxiv.org/abs/2403.08019,2024-03-12,MRC-Net: 6-DoF Pose Estimation with MultiScale Residual Correlation,"['Yuelong Li', 'Yafei Mao', 'Raja Bala', 'Sunil Hadap']","We propose a single-shot approach to determining 6-DoF pose of an object with available 3D computer-aided design (CAD) model from a single RGB image. Our method, dubbed MRC-Net, comprises two stages. The first performs pose classification and renders the 3D object in the classified pose. The second stage performs regression to predict fine-grained residual pose within class. Connecting the two stages is a novel multi-scale residual correlation (MRC) layer that captures high-and-low level correspondences between the input image and rendering from first stage. MRC-Net employs a Siamese network with shared weights between both stages to learn embeddings for input and rendered images. To mitigate ambiguity when predicting discrete pose class labels on symmetric objects, we use soft probabilistic labels to define pose class in the first stage. We demonstrate state-of-the-art accuracy, outperforming all competing RGB-based methods on four challenging BOP benchmark datasets: T-LESS, LM-O, YCB-V, and ITODD. Our method is non-iterative and requires no complex post-processing."
https://arxiv.org/abs/2403.08017,2024-03-12,Red Teaming Models for Hyperspectral Image Analysis Using Explainable AI,"['Vladimir Zaigrajew', 'Hubert Baniecki', 'Lukasz Tulczyjew', 'Agata M. Wijata', 'Jakub Nalepa', 'Nicolas Longépé', 'Przemyslaw Biecek']","Remote sensing (RS) applications in the space domain demand machine learning (ML) models that are reliable, robust, and quality-assured, making red teaming a vital approach for identifying and exposing potential flaws and biases. Since both fields advance independently, there is a notable gap in integrating red teaming strategies into RS. This paper introduces a methodology for examining ML models operating on hyperspectral images within the HYPERVIEW challenge, focusing on soil parameters' estimation. We use post-hoc explanation methods from the Explainable AI (XAI) domain to critically assess the best performing model that won the HYPERVIEW challenge and served as an inspiration for the model deployed on board the INTUITION-1 hyperspectral mission. Our approach effectively red teams the model by pinpointing and validating key shortcomings, constructing a model that achieves comparable performance using just 1% of the input features and a mere up to 5% performance loss. Additionally, we propose a novel way of visualizing explanations that integrate domain-specific information about hyperspectral bands (wavelengths) and data transformations to better suit interpreting models for hyperspectral image analysis."
https://arxiv.org/abs/2403.08013,2024-03-12,Supervised Time Series Classification for Anomaly Detection in Subsea Engineering,"['Ergys Çokaj', 'Halvor Snersrud Gustad', 'Andrea Leone', 'Per Thomas Moe', 'Lasse Moldestad']","Time series classification is of significant importance in monitoring structural systems. In this work, we investigate the use of supervised machine learning classification algorithms on simulated data based on a physical system with two states: Intact and Broken. We provide a comprehensive discussion of the preprocessing of temporal data, using measures of statistical dispersion and dimension reduction techniques. We present an intuitive baseline method and discuss its efficiency. We conclude with a comparison of the various methods based on different performance metrics, showing the advantage of using machine learning techniques as a tool in decision making."
https://arxiv.org/abs/2403.08003,2024-03-12,Real-time Surgical Instrument Segmentation in Video Using Point Tracking and Segment Anything,"['Zijian Wu', 'Adam Schmidt', 'Peter Kazanzides', 'Septimiu E. Salcudean']","The Segment Anything Model (SAM) is a powerful vision foundation model that is revolutionizing the traditional paradigm of segmentation. Despite this, a reliance on prompting each frame and large computational cost limit its usage in robotically assisted surgery. Applications, such as augmented reality guidance, require little user intervention along with efficient inference to be usable clinically. In this study, we address these limitations by adopting lightweight SAM variants to meet the speed requirement and employing fine-tuning techniques to enhance their generalization in surgical scenes. Recent advancements in Tracking Any Point (TAP) have shown promising results in both accuracy and efficiency, particularly when points are occluded or leave the field of view. Inspired by this progress, we present a novel framework that combines an online point tracker with a lightweight SAM model that is fine-tuned for surgical instrument segmentation. Sparse points within the region of interest are tracked and used to prompt SAM throughout the video sequence, providing temporal consistency. The quantitative results surpass the state-of-the-art semi-supervised video object segmentation method on the EndoVis 2015 dataset, with an over 25 FPS inference speed running on a single GeForce RTX 4060 GPU."
https://arxiv.org/abs/2403.08002,2024-03-12,Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging,"['Juan Manuel Zambrano Chaves', 'Shih-Cheng Huang', 'Yanbo Xu', 'Hanwen Xu', 'Naoto Usuyama', 'Sheng Zhang', 'Fei Wang', 'Yujia Xie', 'Mahmoud Khademi', 'Ziyi Yang', 'Hany Awadalla', 'Julia Gong', 'Houdong Hu', 'Jianwei Yang', 'Chunyuan Li', 'Jianfeng Gao', 'Yu Gu', 'Cliff Wong', 'Mu Wei', 'Tristan Naumann', 'Muhao Chen', 'Matthew P. Lungren', 'Serena Yeung-Levy', 'Curtis P. Langlotz', 'Sheng Wang']","The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such large models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world applications. Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data. In this paper, we explore training open-source small multimodal models (SMMs) to bridge biomedical competency gaps for unmet clinical needs. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on training a lightweight adapter to ground each modality to the text embedding space. We conduct a comprehensive study of this approach on radiology imaging. For training, we assemble a large dataset with over 1 million image-text pairs. For evaluation, we propose a clinically driven novel approach using GPT-4 and demonstrate its parity with expert evaluation. We also study grounding qualitatively using attention. For best practice, we conduct a systematic ablation study on various choices in data engineering and multimodal training. The resulting LLaVA-Rad (7B) model attains state-of-the-art results on radiology tasks such as report generation and cross-modal retrieval, even outperforming much larger models such as GPT-4V and Med-PaLM M (84B). LLaVA-Rad is fast and can be run on a single V100 GPU in private settings, offering a promising state-of-the-art tool for real-world clinical applications."
https://arxiv.org/abs/2403.07975,2024-03-12,Superphot+: Realtime Fitting and Classification of Supernova Light Curves,"['Kaylee M. de Soto', 'Ashley Villar', 'Edo Berger', 'Sebastian Gomez', 'Griffin Hosseinzadeh', 'Doug Branton', 'Sandro Campos', 'Melissa DeLucchi', 'Jeremy Kubica', 'Olivia Lynn', 'Konstantin Malanchev', 'Alex I. Malz']","Photometric classifications of supernova (SN) light curves have become necessary to utilize the full potential of large samples of observations obtained from wide-field photometric surveys, such as the Zwicky Transient Facility (ZTF) and the Vera C. Rubin Observatory. Here, we present a photometric classifier for SN light curves that does not rely on redshift information and still maintains comparable accuracy to redshift-dependent classifiers. Our new package, Superphot+, uses a parametric model to extract meaningful features from multiband SN light curves. We train a gradient-boosted machine with fit parameters from 6,061 ZTF SNe that pass data quality cuts and are spectroscopically classified as one of five classes: SN Ia, SN II, SN Ib/c, SN IIn, and SLSN-I. Without redshift information, our classifier yields a class-averaged F1-score of 0.61 +/- 0.02 and a total accuracy of 0.83 +/- 0.01. Including redshift information improves these metrics to 0.71 +/- 0.02 and 0.88 +/- 0.01, respectively. We assign new class probabilities to 3,558 ZTF transients that show SN-like characteristics (based on the ALeRCE Broker light curve and stamp classifiers), but lack spectroscopic classifications. Finally, we compare our predicted SN labels with those generated by the ALeRCE light curve classifier, finding that the two classifiers agree on photometric labels for 82 +/- 2% of light curves with spectroscopic labels and 72% of light curves without spectroscopic labels. Superphot+ is currently classifying ZTF SNe in real time via the ANTARES Broker, and is designed for simple adaptation to six-band Rubin light curves in the future."
https://arxiv.org/abs/2403.07973,2024-03-12,Flexible Non-intrusive Dynamic Instrumentation for WebAssembly,"['Ben L. Titzer', 'Elizabeth Gilbert', 'Bradley Wei Jie Teo', 'Yash Anand', 'Kazuyuki Takayama', 'Heather Miller']","A key strength of managed runtimes over hardware is the ability to gain detailed insight into the dynamic execution of programs with instrumentation. Analyses such as code coverage, execution frequency, tracing, and debugging, are all made easier in a virtual setting. As a portable, low-level bytecode, WebAssembly offers inexpensive in-process sandboxing with high performance. Yet to date, Wasm engines have not offered much insight into executing programs, supporting at best bytecode-level stepping and basic source maps, but no instrumentation capabilities. In this paper, we show the first non-intrusive dynamic instrumentation system for WebAssembly in the open-source Wizard Research Engine. Our innovative design offers a flexible, complete hierarchy of instrumentation primitives that support building high-level, complex analyses in terms of low-level, programmable probes. In contrast to emulation or machine code instrumentation, injecting probes at the bytecode level increases expressiveness and vastly simplifies the implementation by reusing the engine's JIT compiler, interpreter, and deoptimization mechanism rather than building new ones. Wizard supports both dynamic instrumentation insertion and removal while providing consistency guarantees, which is key to composing multiple analyses without interference. We detail a fully-featured implementation in a high-performance multi-tier Wasm engine, show novel optimizations specifically designed to minimize instrumentation overhead, and evaluate performance characteristics under load from various analyses. This design is well-suited for production engine adoption as probes can be implemented to have no impact on production performance when not in use."
https://arxiv.org/abs/2403.07951,2024-03-11,SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic Microscopy Segmentation,"['Yiran Wang', 'Li Xiao']","It has been shown that traditional deep learning methods for electronic microscopy segmentation usually suffer from low transferability when samples and annotations are limited, while large-scale vision foundation models are more robust when transferring between different domains but facing sub-optimal improvement under fine-tuning. In this work, we present a new few-shot domain adaptation framework SAMDA, which combines the Segment Anything Model(SAM) with nnUNet in the embedding space to achieve high transferability and accuracy. Specifically, we choose the Unet-based network as the ""expert"" component to learn segmentation features efficiently and design a SAM-based adaptation module as the ""generic"" component for domain transfer. By amalgamating the ""generic"" and ""expert"" components, we mitigate the modality imbalance in the complex pre-training knowledge inherent to large-scale Vision Foundation models and the challenge of transferability inherent to traditional neural networks. The effectiveness of our model is evaluated on two electron microscopic image datasets with different modalities for mitochondria segmentation, which improves the dice coefficient on the target domain by 6.7%. Also, the SAM-based adaptor performs significantly better with only a single annotated image than the 10-shot domain adaptation on nnUNet. We further verify our model on four MRI datasets from different sources to prove its generalization ability."
https://arxiv.org/abs/2403.07940,2024-03-08,Hair and scalp disease detection using deep learning,"['Kavita Sultanpure', 'Bhairavi Shirsath', 'Bhakti Bhande', 'Harshada Sawai', 'Srushti Gawade', 'Suraj Samgir']","In recent years, there has been a notable advancement in the integration of healthcare and technology, particularly evident in the field of medical image analysis. This paper introduces a pioneering approach in dermatology, presenting a robust method for the detection of hair and scalp diseases using state-of-the-art deep learning techniques. Our methodology relies on Convolutional Neural Networks (CNNs), well-known for their efficacy in image recognition, to meticulously analyze images for various dermatological conditions affecting the hair and scalp. Our proposed system represents a significant advancement in dermatological diagnostics, offering a non-invasive and highly efficient means of early detection and diagnosis. By leveraging the capabilities of CNNs, our model holds the potential to revolutionize dermatology, providing accessible and timely healthcare solutions. Furthermore, the seamless integration of our trained model into a web-based platform developed with the Django framework ensures broad accessibility and usability, democratizing advanced medical diagnostics. The integration of machine learning algorithms into web applications marks a pivotal moment in healthcare delivery, promising empowerment for both healthcare providers and patients. Through the synergy between technology and healthcare, our paper outlines the meticulous methodology, technical intricacies, and promising future prospects of our system. With a steadfast commitment to advancing healthcare frontiers, our goal is to significantly contribute to leveraging technology for improved healthcare outcomes globally. This endeavor underscores the profound impact of technological innovation in shaping the future of healthcare delivery and patient care, highlighting the transformative potential of our approach."
https://arxiv.org/abs/2403.07936,2024-03-07,Accelerating multigrid solver with generative super-resolution,"['Francisco Holguin', 'GS Sidharth', 'Gavin Portwood']","The geometric multigrid algorithm is an efficient numerical method for solving a variety of elliptic partial differential equations (PDEs). The method damps errors at progressively finer grid scales, resulting in faster convergence compared to iterative methods such as Gauss-Seidel. The prolongation or coarse-to-fine interpolation operator within the multigrid algorithm, lends itself to a data-driven treatment with deep learning super-resolution, commonly used to increase the resolution of images. We (i) propose the integration of a super-resolution generative adversarial network (GAN) model with the multigrid algorithm as the prolongation operator and (ii) show that the GAN-interpolation can improve the convergence properties of multigrid in comparison to cubic spline interpolation on a class of multiscale PDEs typically solved in fluid mechanics and engineering simulations. We also highlight the importance of characterizing hybrid (machine learning/traditional) algorithm parameters."
https://arxiv.org/abs/2403.07857,2024-03-12,Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias,"['Sierra Wyllie', 'Ilia Shumailov', 'Nicolas Papernot']","Model-induced distribution shifts (MIDS) occur as previous model outputs pollute new model training sets over generations of models. This is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for supervised models. When a model induces a distribution shift, it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data ecosystem. We introduce a framework that allows us to track multiple MIDS over many generations, finding that they can lead to loss in performance, fairness, and minoritized group representation, even in initially unbiased datasets. Despite these negative consequences, we identify how models might be used for positive, intentional, interventions in their data ecosystems, providing redress for historical discrimination through a framework called algorithmic reparation (AR). We simulate AR interventions by curating representative training batches for stochastic gradient descent to demonstrate how AR can improve upon the unfairnesses of models and data ecosystems subject to other MIDS. Our work takes an important step towards identifying, mitigating, and taking accountability for the unfair feedback loops enabled by the idea that ML systems are inherently neutral and objective."
https://arxiv.org/abs/2403.07856,2024-03-12,Quantum Support Vector Machine for Prostate Cancer Detection: A Performance Analysis,"['Walid El Maouaki', 'Taoufik Said', 'Mohamed Bennai']","This study addresses the urgent need for improved prostate cancer detection methods by harnessing the power of advanced technological solutions. We introduce the application of Quantum Support Vector Machine (QSVM) to this critical healthcare challenge, showcasing an enhancement in diagnostic performance over the classical Support Vector Machine (SVM) approach. Our study not only outlines the remarkable improvements in diagnostic performance made by QSVM over the classic SVM technique, but it delves into the advancements brought about by the quantum feature map architecture, which has been carefully identified and evaluated, ensuring it aligns seamlessly with the unique characteristics of our prostate cancer dataset. This architecture succeded in creating a distinct feature space, enabling the detection of complex, non-linear patterns in the data. The findings reveal not only a comparable accuracy with classical SVM ($92\%$) but also a $7.14\%$ increase in sensitivity and a notably high F1-Score ($93.33\%$). This study's important combination of quantum computing in medical diagnostics marks a pivotal step forward in cancer detection, offering promising implications for the future of healthcare technology."
https://arxiv.org/abs/2403.07854,2024-03-12,Distilling the Knowledge in Data Pruning,"['Emanuel Ben-Baruch', 'Adam Botach', 'Igor Kviatkovsky', 'Manoj Aggarwal', 'Gérard Medioni']","With the increasing size of datasets used for training neural networks, data pruning becomes an attractive field of research. However, most current data pruning algorithms are limited in their ability to preserve accuracy compared to models trained on the full data, especially in high pruning regimes. In this paper we explore the application of data pruning while incorporating knowledge distillation (KD) when training on a pruned subset. That is, rather than relying solely on ground-truth labels, we also use the soft predictions from a teacher network pre-trained on the complete data. By integrating KD into training, we demonstrate significant improvement across datasets, pruning methods, and on all pruning fractions. We first establish a theoretical motivation for employing self-distillation to improve training on pruned data. Then, we empirically make a compelling and highly practical observation: using KD, simple random pruning is comparable or superior to sophisticated pruning methods across all pruning regimes. On ImageNet for example, we achieve superior accuracy despite training on a random subset of only 50% of the data. Additionally, we demonstrate a crucial connection between the pruning factor and the optimal knowledge distillation weight. This helps mitigate the impact of samples with noisy labels and low-quality images retained by typical pruning algorithms. Finally, we make an intriguing observation: when using lower pruning fractions, larger teachers lead to accuracy degradation, while surprisingly, employing teachers with a smaller capacity than the student's may improve results. Our code will be made available."
https://arxiv.org/abs/2403.07853,2024-03-12,Improving Fairness in Photovoltaic Curtailments via Daily Topology Reconfiguration for Voltage Control in Power Distribution Networks,"['Rahul K. Gupta', 'Daniel K. Molzahn']","In PV-rich power distribution systems, over-voltage issues are often addressed by curtailing excess generation from PV plants (in addition to reactive power control), raising fairness concerns. Existing fairness-aware control schemes tackle this problem by incorporating fairness objectives into the cost function. However, such schemes result in increased overall curtailments. This paper proposes a solution through daily topology reconfiguration, ensuring that different PV plants face varying grid conditions each day, leading to different curtailment levels and enhancing fairness. We illustrate that implementing this approach enhances overall fairness without significantly increasing overall curtailments. The optimization problem involves two stages. The day-ahead stage optimizes the network topology using day-ahead forecasts of PV generation and demand, minimizing net curtailment and accounting for fairness based on curtailments from prior days. The real-time stage implements the optimized topology and computes active and reactive power setpoints for the PV plants. Day-ahead grid constraints are modeled using LinDistFlow, and real-time control employs a linearized model with a first-order Taylor approximation. The proposed scheme is numerically validated on several benchmark test cases. Results are compared using the Jain Fairness Index, considering fairness and reconfiguration scenarios."
https://arxiv.org/abs/2403.07850,2024-03-12,Laser-written waveguide-integrated coherent spins in diamond,"['Yanzhao Guo', 'John P. Hadden', 'Federico Gorrini', 'Giulio Coccia', 'Vibhav Bharadwaj', 'Vinaya Kumar Kavatamane', 'Mohammad Sahnawaz Alam', 'Roberta Ramponi', 'Paul E. Barclay', 'Andrea Chiappini', 'Maurizio Ferrari', 'Alexander Kubanek', 'Angelo Bifone', 'Shane M. Eaton', 'Anthony J. Bennett']","Quantum emitters, such as the negatively charged nitrogen-vacancy center in diamond, are attractive for quantum technologies such as nano-sensing, quantum information processing, and as a non-classical light source. However, it is still challenging to position individual emitters in photonic structures whilst preserving the spin coherence properties of the defect. In this paper, we investigate single and ensemble waveguide-integrated nitrogen-vacancy centers in diamond fabricated by femtosecond laser writing followed by thermal annealing. Their spin coherence properties are systematically investigated and are shown to be comparable to native nitrogen-vacancy centers in diamond. This method paves the way for the fabrication of coherent spins integrated within photonic devices."
https://arxiv.org/abs/2403.07849,2024-03-12,Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations,"['Harish G. Naik', 'Jan Polster', 'Raj Shekhar', 'Tamás Horváth', 'György Turán']","We formulate an XAI-based model improvement approach for Graph Neural Networks (GNNs) for node classification, called Explanation Enhanced Graph Learning (EEGL). The goal is to improve predictive performance of GNN using explanations. EEGL is an iterative self-improving algorithm, which starts with a learned ""vanilla"" GNN, and repeatedly uses frequent subgraph mining to find relevant patterns in explanation subgraphs. These patterns are then filtered further to obtain application-dependent features corresponding to the presence of certain subgraphs in the node neighborhoods. Giving an application-dependent algorithm for such a subgraph-based extension of the Weisfeiler-Leman (1-WL) algorithm has previously been posed as an open problem. We present experimental evidence, with synthetic and real-world data, which show that EEGL outperforms related approaches in predictive performance and that it has a node-distinguishing power beyond that of vanilla GNNs. We also analyze EEGL's training dynamics."
https://arxiv.org/abs/2403.07845,2024-03-12,Hyper-density functional theory of soft matter,"['Florian Sammüller', 'Silas Robitschko', 'Sophie Hermann', 'Matthias Schmidt']","We present a scheme for investigating arbitrary thermal observables in spatially inhomogeneous many-body systems. Extending the equilibrium ensemble yields any given observable as an explicit hyper-density functional. Associated local fluctuation profiles follow from an exact hyper-Ornstein-Zernike equation. Simulation-based supervised machine learning trains neural networks that act as hyper-direct correlation functionals which facilitate efficient and accurate predictions. We exemplify the approach for the cluster statistics of hard rods and square well particles. The theory provides access to complex order parameters, as is impossible in standard density functional theory."
https://arxiv.org/abs/2403.07835,2024-03-12,On Modeling Adequacy and Stability Analysis of IBR-related Subsynchronous Oscillations in Multimachine Systems,"['Lilan Karunaratne', 'Nilanjan Ray Chaudhuri', 'Amirthagunaraj Yogarathnam', 'Meng Yue']","Time-varying phasor-based analysis of subsynchronous oscillations (SSOs) involving grid-following converters (GFLCs) and its benchmarking with electromagnetic transient (EMT) models have so far been restricted to highly simplified grid models with constant voltage sources behind series R-L circuits. In this paper, modeling adequacy of bulk power systems with synchronous generators (SGs), transmission systems, loads, and GFLCs are considered. To this end, we revisit the notions of time-varying phasor calculus, highlighting the distinction between space-phasor-calculus (SPC) and two often interchangeably used frameworks namely baseband-abc and generalized averaging. We present the models of grids in SPC framework that include transmission line dynamics, load dynamics, and SG stator transients. Next, we propose a generic approach to study modeling adequacy in small-signal sense by (a) identifying critical modes through eigenvalue and singular value analysis followed by (b) using weighted maximum singular value error magnitudes as metrics, and (c) further cross-validation. Using a modified 4-machine IEEE benchmark model with up to 3 GFLCs we show that SPC framework can be used for analysis of SSOs. Further, we consider the quasistationary phasor calculus (QPC) framework that neglects transmission line, load, and SG stator dynamics to show its adequacy in SSO modeling and analysis. Time-domain and frequency-domain results with EMT models are also presented."
https://arxiv.org/abs/2403.07825,2024-03-12,The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing,"['Jianchen Wang', 'Zhouhong Gu', 'Zhuozhi Xiong', 'Hongwei Feng', 'Yanghua Xiao']","Large Language Models have revolutionized numerous tasks with their remarkable efficacy.However, the editing of these models, crucial for rectifying outdated or erroneous information, often leads to a complex issue known as the ripple effect in the hidden space. This effect, while difficult to detect, can significantly impede the efficacy of model editing tasks and deteriorate model performance.This paper addresses this scientific challenge by proposing a novel evaluation methodology, Graphical Outlier Relation based Assessment(GORA), which quantitatively evaluates the adaptations of the model and the subsequent impact of editing. Furthermore, we introduce the Selective Outlier Re-Editing Approach(SORA), a model editing method designed to mitigate this ripple effect. Our comprehensive evaluations reveal that the ripple effect in the hidden space is a significant issue in all current model editing methods. However, our proposed methods, GORA and SORA, effectively identify and alleviate this issue, respectively, contributing to the advancement of LLM editing techniques."
https://arxiv.org/abs/2403.07822,2024-03-12,Fusing Climate Data Products using a Spatially Varying Autoencoder,"['Jacob A. Johnson', 'Matthew J. Heaton', 'William F. Christensen', 'Lynsie R. Warr', 'Summer B. Rupper']","Autoencoders are powerful machine learning models used to compress information from multiple data sources. However, autoencoders, like all artificial neural networks, are often unidentifiable and uninterpretable. This research focuses on creating an identifiable and interpretable autoencoder that can be used to meld and combine climate data products. The proposed autoencoder utilizes a Bayesian statistical framework, allowing for probabilistic interpretations while also varying spatially to capture useful spatial patterns across the various data products. Constraints are placed on the autoencoder as it learns patterns in the data, creating an interpretable consensus that includes the important features from each input. We demonstrate the utility of the autoencoder by combining information from multiple precipitation products in High Mountain Asia."
https://arxiv.org/abs/2403.07816,2024-03-12,Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM,"['Sainbayar Sukhbaatar', 'Olga Golovneva', 'Vasu Sharma', 'Hu Xu', 'Xi Victoria Lin', 'Baptiste Rozière', 'Jacob Kahn', 'Daniel Li', 'Wen-tau Yih', 'Jason Weston', 'Xian Li']","We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff."
https://arxiv.org/abs/2403.07811,2024-03-12,Mesh Refinement with Early Termination for Dynamic Feasibility Problems,"['Eduardo M. G. Vila', 'Eric C. Kerrigan', 'Paul Bruce']","We propose a novel early-terminating mesh refinement strategy using an integrated residual method to solve dynamic feasibility problems. As a generalization of direct collocation, the integrated residual method is used to approximate an infinite-dimensional problem into a sequence of finite-dimensional optimization subproblems. Each subproblem in the sequence is a finer approximation of the previous. It is shown that these subproblems need not be solved to a high precision; instead, an early termination procedure can determine when mesh refinement should be performed. The new refinement strategy, applied to an inverted pendulum swing-up problem, outperforms a conventional refinement method by up to a factor of three in function evaluations."
https://arxiv.org/abs/2403.07795,2024-03-12,Fine-tuning Neural Network Quantum States,"['Riccardo Rende', 'Sebastian Goldt', 'Federico Becca', 'Luciano Loris Viteritti']","Recent progress in the design and optimization of Neural Network Quantum States (NNQS) have made them an effective method to investigate ground-state properties of quantum many-body systems. In contrast to the standard approach of training a separate NNQS from scratch at every point of the phase diagram, we demonstrate that the optimization at a highly expressive point of the phase diagram (i.e., close to a phase transition) yields interpretable features that can be reused to accurately describe a wide region across the transition. We demonstrate the feasibility of our approach on different systems in one and two dimensions by initially pretraining a NNQS at a given point of the phase diagram, followed by fine-tuning only the output layer for all other points. Notably, the computational cost of the fine-tuning step is very low compared to the pretraining stage. We argue that the reduced cost of this paradigm has significant potential to advance the exploration of condensed matter systems using NNQS, mirroring the success of fine-tuning in machine learning and natural language processing."
https://arxiv.org/abs/2403.07792,2024-03-12,Search for new bosons with ytterbium isotope shifts,"['Menno Door', 'Chih-Han Yeh', 'Matthias Heinz', 'Fiona Kirk', 'Chunhai Lyu', 'Takayuki Miyagi', 'Julian C. Berengut', 'Jacek Bieroń', 'Klaus Blaum', 'Laura S. Dreissen', 'Sergey Eliseev', 'Pavel Filianin', 'Melina Filzinger', 'Elina Fuchs', 'Henning A. Fürst', 'Gediminas Gaigalas', 'Zoltán Harman', 'Jost Herkenhoff', 'Nils Huntemann', 'Christoph H. Keitel', 'Kathrin Kromer', 'Daniel Lange', 'Alexander Rischka', 'Christoph Schweiger', 'Achim Schwenk']","The Standard Model of particle physics describes the properties of elementary particles and their interactions remarkably well, but in particular does not account for dark matter. Isotope-shift spectroscopy is a sensitive probe of fifth forces and new particles that illuminate the dark matter sector. This method sets bounds on new bosons that couple neutrons and electrons with masses in the keV/c2 to MeV/c2 range. With increasing spectroscopic precision, such searches are limited by uncertainties of isotope masses and the understanding of nuclear structure. Here, we report on high-precision mass-ratio and isotope-shift measurements of the ytterbium isotopes $^{168,170,172,174,176}$Yb that exceed previous measurements by up to two orders of magnitude. From these measurements, we extract higher-order changes in the nuclear charge distribution along the Yb isotope chain and use these to benchmark novel ab initio calculations. Our measurements set new bounds on the existence of the proposed boson."
https://arxiv.org/abs/2403.07787,2024-03-12,Transparent boundary condition and its effectively local approximation for the Schrödinger equation on a rectangular computational domain,"['Samardhi Yadav', 'Vishal Vaibhav']","The transparent boundary condition for the free Schrödinger equation on a rectangular computational domain requires implementation of an operator of the form $\sqrt{\partial_t-i\triangle_Γ}$ where $\triangle_Γ$ is the Laplace-Beltrami operator. It is known that this operator is nonlocal in time as well as space which poses a significant challenge in developing an efficient numerical method of solution. The computational complexity of the existing methods scale with the number of time-steps which can be attributed to the nonlocal nature of the boundary operator. In this work, we report an effectively local approximation for the boundary operator such that the resulting complexity remains independent of number of time-steps. At the heart of this algorithm is a Padé approximant based rational approximation of certain fractional operators that handles corners of the domain adequately. For the spatial discretization, we use a Legendre-Galerkin spectral method with a new boundary adapted basis which ensures that the resulting linear system is banded. A compatible boundary-lifting procedure is also presented which accommodates the segments as well as the corners on the boundary. The proposed novel scheme can be implemented within the framework of any one-step time marching schemes. In particular, we demonstrate these ideas for two one-step methods, namely, the backward-differentiation formula of order 1 (BDF1) and the trapezoidal rule (TR). For the sake of comparison, we also present a convolution quadrature based scheme conforming to the one-step methods which is computationally expensive but serves as a golden standard. Finally, several numerical tests are presented to demonstrate the effectiveness of our novel method as well as to verify the order of convergence empirically."
https://arxiv.org/abs/2403.07786,2024-03-12,Generative deep learning-enabled ultra-large field-of-view lens-free imaging,"['Ronald B. Liu', 'Zhe Liu', 'Max G. A. Wolf', 'Krishna P. Purohit', 'Gregor Fritz', 'Yi Feng', 'Carsten G. Hansen', 'Pierre O. Bagnaninchi', 'Xavier Casadevall i Solvas', 'Yunjie Yang']","Advancements in high-throughput biomedical applications necessitate real-time, large field-of-view (FOV) imaging capabilities. Conventional lens-free imaging (LFI) systems, while addressing the limitations of physical lenses, have been constrained by dynamic, hard-to-model optical fields, resulting in a limited one-shot FOV of approximately 20 $mm^2$. This restriction has been a major bottleneck in applications like live-cell imaging and automation of microfluidic systems for biomedical research. Here, we present a deep-learning(DL)-based imaging framework -- GenLFI -- leveraging generative artificial intelligence (AI) for holographic image reconstruction. We demonstrate that GenLFI can achieve a real-time FOV over 550 $mm^2$, surpassing the current LFI system by more than 20-fold, and even larger than the world's largest confocal microscope by 1.76 times. The resolution is at the sub-pixel level of 5.52 $μm$, without the need for a shifting light source. The unsupervised learning-based reconstruction does not require optical field modeling, making imaging dynamic 3D samples (e.g., droplet-based microfluidics and 3D cell models) in complex optical fields possible. This GenLFI framework unlocks the potential of LFI systems, offering a robust tool to tackle new frontiers in high-throughput biomedical applications such as drug discovery."
https://arxiv.org/abs/2403.07780,2024-03-12,FairRR: Pre-Processing for Group Fairness through Randomized Response,"['Xianli Zeng', 'Joshua Ward', 'Guang Cheng']","The increasing usage of machine learning models in consequential decision-making processes has spurred research into the fairness of these systems. While significant work has been done to study group fairness in the in-processing and post-processing setting, there has been little that theoretically connects these results to the pre-processing domain. This paper proposes that achieving group fairness in downstream models can be formulated as finding the optimal design matrix in which to modify a response variable in a Randomized Response framework. We show that measures of group fairness can be directly controlled for with optimal model utility, proposing a pre-processing algorithm called FairRR that yields excellent downstream model utility and fairness."
https://arxiv.org/abs/2403.07764,2024-03-12,Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model,"['Yuxuan Zhang', 'Lifu Wei', 'Qing Zhang', 'Yiren Song', 'Jiaming Liu', 'Huaxia Li', 'Xu Tang', 'Yao Hu', 'Haibo Zhao']","Current makeup transfer methods are limited to simple makeup styles, making them difficult to apply in real-world scenarios. In this paper, we introduce Stable-Makeup, a novel diffusion-based makeup transfer method capable of robustly transferring a wide range of real-world makeup, onto user-provided faces. Stable-Makeup is based on a pre-trained diffusion model and utilizes a Detail-Preserving (D-P) makeup encoder to encode makeup details. It also employs content and structural control modules to preserve the content and structural information of the source image. With the aid of our newly added makeup cross-attention layers in U-Net, we can accurately transfer the detailed makeup to the corresponding position in the source image. After content-structure decoupling training, Stable-Makeup can maintain content and the facial structure of the source image. Moreover, our method has demonstrated strong robustness and generalizability, making it applicable to varioustasks such as cross-domain makeup transfer, makeup-guided text-to-image generation and so on. Extensive experiments have demonstrated that our approach delivers state-of-the-art (SOTA) results among existing makeup transfer methods and exhibits a highly promising with broad potential applications in various related fields."
https://arxiv.org/abs/2403.07753,2024-03-12,A robust SVM-based approach with feature selection and outliers detection for classification problems,"['Marta Baldomero-Naranjo', 'Luisa I. Martínez-Merino', 'Antonio M. Rodríguez-Chía']","This paper proposes a robust classification model, based on support vector machine (SVM), which simultaneously deals with outliers detection and feature selection. The classifier is built considering the ramp loss margin error and it includes a budget constraint to limit the number of selected features. The search of this classifier is modeled using a mixed-integer formulation with big M parameters. Two different approaches (exact and heuristic) are proposed to solve the model. The heuristic approach is validated by comparing the quality of the solutions provided by this approach with the exact approach. In addition, the classifiers obtained with the heuristic method are tested and compared with existing SVM-based models to demonstrate their efficiency."
https://arxiv.org/abs/2403.07750,2024-03-12,Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings,"['Sahand Sharifzadeh', 'Christos Kaplanis', 'Shreya Pathak', 'Dharshan Kumaran', 'Anastasija Ilic', 'Jovana Mitrovic', 'Charles Blundell', 'Andrea Banino']","The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training. Our method employs pretraining a text-to-image model to synthesize image embeddings starting from captions generated by an LLM. These synthetic pairs are then used to train a VLM. Extensive experiments demonstrate that the VLM trained with synthetic data exhibits comparable performance on image captioning, while requiring a fraction of the data used by models trained solely on human-annotated data. In particular, we outperform the baseline by 17% through augmentation with a synthetic dataset. Furthermore, we show that synthesizing in the image embedding space is 25% faster than in the pixel space. This research introduces a promising technique for generating large-scale, customizable image datasets, leading to enhanced VLM performance and wider applicability across various domains, all with improved data efficiency and resource utilization."
https://arxiv.org/abs/2403.07743,2024-03-13,Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs,"['Neel Kanwal', 'Farbod Khoraminia', 'Umay Kiraz', 'Andres Mosquera-Zamudio', 'Carlos Monteagudo', 'Emiel A. M. Janssen', 'Tahlita C. M. Zuiverloon', 'Chunmig Rong', 'Kjersti Engan']","Histopathology is a gold standard for cancer diagnosis under a microscopic examination. However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs). Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions. Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis. In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs. First, we train independent binary DL models as experts to capture particular artifact morphology. Then, we ensemble their predictions using a fusion mechanism. We apply probabilistic thresholding over the final probability distribution to improve the sensitivity of the MoE. We developed DL pipelines using two MoEs and two multiclass models of state-of-the-art deep convolutional neural networks (DCNNs) and vision transformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed simpler multiclass models and were tested on datasets from different hospitals and cancer types, where MoE using DCNNs yielded the best results. The proposed MoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining less computational cost for inference than MoE using ViTs. This best performance of MoEs comes with relatively higher computational trade-offs than multiclass models. The proposed artifact detection pipeline will not only ensure reliable CPATH predictions but may also provide quality control."
https://arxiv.org/abs/2403.07733,2024-03-12,DSEG-LIME - Improving Image Explanation by Hierarchical Data-Driven Segmentation,"['Patrick Knab', 'Sascha Marton', 'Christian Bartelt']","Explainable Artificial Intelligence is critical in unraveling decision-making processes in complex machine learning models. LIME (Local Interpretable Model-agnostic Explanations) is a well-known XAI framework for image analysis. It utilizes image segmentation to create features to identify relevant areas for classification. Consequently, poor segmentation can compromise the consistency of the explanation and undermine the importance of the segments, affecting the overall interpretability. Addressing these challenges, we introduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a data-driven segmentation for human-recognized feature generation, and ii) a hierarchical segmentation procedure through composition. We benchmark DSEG-LIME on pre-trained models with images from the ImageNet dataset - scenarios without domain-specific knowledge. The analysis includes a quantitative evaluation using established XAI metrics, complemented by a qualitative assessment through a user study. Our findings demonstrate that DSEG outperforms in most of the XAI metrics and enhances the alignment of explanations with human-recognized concepts, significantly improving interpretability. The code is available under: https://github. com/patrick-knab/DSEG-LIME"
https://arxiv.org/abs/2403.07721,2024-03-13,Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion,"['Dongyang Li', 'Chen Wei', 'Shiying Li', 'Jiachen Zou', 'Quanying Liu']","How to decode human vision through neural signals has attracted a long-standing interest in neuroscience and machine learning. Modern contrastive learning and generative models improved the performance of fMRI-based visual decoding and reconstruction. However, the high cost and low temporal resolution of fMRI limit their applications in brain-computer interfaces (BCIs), prompting a high need for EEG-based visual reconstruction. In this study, we present an EEG-based visual reconstruction framework. It consists of a plug-and-play EEG encoder called the Adaptive Thinking Mapper (ATM), which is aligned with image embeddings, and a two-stage EEG guidance image generator that first transforms EEG features into image priors and then reconstructs the visual stimuli with a pre-trained image generator. Our approach allows EEG embeddings to achieve superior performance in image classification and retrieval tasks. Our two-stage image generation strategy vividly reconstructs images seen by humans. Furthermore, we analyzed the impact of signals from different time windows and brain regions on decoding and reconstruction. The versatility of our framework is demonstrated in the magnetoencephalogram (MEG) data modality. We report that EEG-based visual decoding achieves SOTA performance, highlighting the portability, low cost, and high temporal resolution of EEG, enabling a wide range of BCI applications. The code of ATM is available at https://github.com/dongyangli-del/EEG_Image_decode."
https://arxiv.org/abs/2403.07711,2024-03-12,SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces,"['Yuta Oshima', 'Shohei Taniguchi', 'Masahiro Suzuki', 'Yutaka Matsuo']","Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, we perform an experiment using the MineRL Navigate dataset, varying the number of frames to 64 and 150. In these settings, our SSM-based model can considerably save memory consumption for longer sequences, while maintaining competitive FVD scores to the attention-based models. Our codes are available at https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models."
https://arxiv.org/abs/2403.07708,2024-03-12,Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards,"['Wei Shen', 'Xiaoying Zhang', 'Yuanshun Yao', 'Rui Zheng', 'Hongyi Guo', 'Yang Liu']","Reinforcement learning from human feedback (RLHF) is the mainstream paradigm used to align large language models (LLMs) with human preferences. Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile. In this work, we improve the effectiveness of the reward model by introducing a penalty term on the reward, named as \textit{contrastive rewards}. %Contrastive rewards Our approach involves two steps: (1) an offline sampling step to obtain responses to prompts that serve as baseline calculation and (2) a contrastive reward calculated using the baseline responses and used in the Proximal Policy Optimization (PPO) step. We show that contrastive rewards enable the LLM to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and reduce variance in PPO. We show empirically contrastive rewards can improve RLHF substantially, evaluated by both GPTs and humans, and our method consistently outperforms strong baselines."
https://arxiv.org/abs/2403.07706,2024-03-12,Fast and Simple Explainability for Point Cloud Networks,"['Meir Yossef Levi', 'Guy Gilboa']","We propose a fast and simple explainable AI (XAI) method for point cloud data. It computes pointwise importance with respect to a trained network downstream task. This allows better understanding of the network properties, which is imperative for safety-critical applications. In addition to debugging and visualization, our low computational complexity facilitates online feedback to the network at inference. This can be used to reduce uncertainty and to increase robustness. In this work, we introduce \emph{Feature Based Interpretability} (FBI), where we compute the features' norm, per point, before the bottleneck. We analyze the use of gradients and post- and pre-bottleneck strategies, showing pre-bottleneck is preferred, in terms of smoothness and ranking. We obtain at least three orders of magnitude speedup, compared to current XAI methods, thus, scalable for big point clouds or large-scale architectures. Our approach achieves SOTA results, in terms of classification explainability. We demonstrate how the proposed measure is helpful in analyzing and characterizing various aspects of 3D learning, such as rotation invariance, robustness to out-of-distribution (OOD) outliers or domain shift and dataset bias."
https://arxiv.org/abs/2403.07704,2024-03-12,Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning,"['Motoki Omura', 'Takayuki Osa', 'Yusuke Mukuta', 'Tatsuya Harada']","In deep reinforcement learning, estimating the value function to evaluate the quality of states and actions is essential. The value function is often trained using the least squares method, which implicitly assumes a Gaussian error distribution. However, a recent study suggested that the error distribution for training the value function is often skewed because of the properties of the Bellman operator, and violates the implicit assumption of normal error distribution in the least squares method. To address this, we proposed a method called Symmetric Q-learning, in which the synthetic noise generated from a zero-mean distribution is added to the target values to generate a Gaussian error distribution. We evaluated the proposed method on continuous control benchmark tasks in MuJoCo. It improved the sample efficiency of a state-of-the-art reinforcement learning method by reducing the skewness of the error distribution."
https://arxiv.org/abs/2403.07700,2024-03-12,CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive Self-Supervised Transformers,"['Shahaf Arica', 'Or Rubin', 'Sapir Gershov', 'Shlomi Laufer']","In this paper, we introduce VoteCut, an innovative method for unsupervised object discovery that leverages feature representations from multiple self-supervised models. VoteCut employs normalized-cut based graph partitioning, clustering and a pixel voting approach. Additionally, We present CuVLER (Cut-Vote-and-LEaRn), a zero-shot model, trained using pseudo-labels, generated by VoteCut, and a novel soft target loss to refine segmentation accuracy. Through rigorous evaluations across multiple datasets and several unsupervised setups, our methods demonstrate significant improvements in comparison to previous state-of-the-art models. Our ablation studies further highlight the contributions of each component, revealing the robustness and efficacy of our approach. Collectively, VoteCut and CuVLER pave the way for future advancements in image segmentation."
https://arxiv.org/abs/2403.07693,2024-03-12,"Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization","['Yanyue Zhang', 'Pengfei Li', 'Yilong Lai', 'Deyu Zhou']","As more than 70$\%$ of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are reluctant to generate negative summaries given the input of negative texts. To address such sentiment bias, a direct approach without the over-reliance on a specific framework is to generate additional data based on large language models to balance the emotional distribution of the dataset. However, data augmentation based on large language models faces two disadvantages: 1) the potential issues or toxicity in the augmented data; 2) the expensive costs. Therefore, in this paper, we propose a novel data augmentation framework based on both large and small language models for debiasing opinion summarization. In specific, a small size of synthesized negative reviews is obtained by rewriting the positive text via a large language model. Then, a disentangle reconstruction model is trained based on the generated data. After training, a large amount of synthetic data can be obtained by decoding the new representation obtained from the combination of different sample representations and filtering based on confusion degree and sentiment classification. Experiments have proved that our framework can effectively alleviate emotional bias same as using only large models, but more economically."
https://arxiv.org/abs/2403.07692,2024-03-12,Masked AutoDecoder is Effective Multi-Task Vision Generalist,"['Han Qiu', 'Jiaxing Huang', 'Peng Gao', 'Lewei Lu', 'Xiaoqin Zhang', 'Shijian Lu']","Inspired by the success of general-purpose models in NLP, recent studies attempt to unify different vision tasks in the same sequence format and employ autoregressive Transformers for sequence prediction. They apply uni-directional attention to capture sequential dependencies and generate task sequences recursively. However, such autoregressive Transformers may not fit vision tasks well, as vision task sequences usually lack the sequential dependencies typically observed in natural languages. In this work, we design Masked AutoDecoder~(MAD), an effective multi-task vision generalist. MAD consists of two core designs. First, we develop a parallel decoding framework that introduces bi-directional attention to capture contextual dependencies comprehensively and decode vision task sequences in parallel. Second, we design a masked sequence modeling approach that learns rich task contexts by masking and reconstructing task sequences. In this way, MAD handles all the tasks by a single network branch and a simple cross-entropy loss with minimal task-specific designs. Extensive experiments demonstrate the great potential of MAD as a new paradigm for unifying various vision tasks. MAD achieves superior performance and inference efficiency compared to autoregressive counterparts while obtaining competitive accuracy with task-specific models. Code will be released."
https://arxiv.org/abs/2403.07690,2024-03-12,SATDAUG -- A Balanced and Augmented Dataset for Detecting Self-Admitted Technical Debt,"['Edi Sutoyo', 'Andrea Capiluppi']","Self-admitted technical debt (SATD) refers to a form of technical debt in which developers explicitly acknowledge and document the existence of technical shortcuts, workarounds, or temporary solutions within the codebase. Over recent years, researchers have manually labeled datasets derived from various software development artifacts: source code comments, messages from the issue tracker and pull request sections, and commit messages. These datasets are designed for training, evaluation, performance validation, and improvement of machine learning and deep learning models to accurately identify SATD instances. However, class imbalance poses a serious challenge across all the existing datasets, particularly when researchers are interested in categorizing the specific types of SATD. In order to address the scarcity of labeled data for SATD \textit{identification} (i.e., whether an instance is SATD or not) and \textit{categorization} (i.e., which type of SATD is being classified) in existing datasets, we share the \textit{SATDAUG} dataset, an augmented version of existing SATD datasets, including source code comments, issue tracker, pull requests, and commit messages. These augmented datasets have been balanced in relation to the available artifacts and provide a much richer source of labeled data for training machine learning or deep learning models."
https://arxiv.org/abs/2403.07688,2024-03-12,Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons,"['Simon Dufort-Labbé', ""Pierluca D'Oro"", 'Evgenii Nikishin', 'Razvan Pascanu', 'Pierre-Luc Bacon', 'Aristide Baratin']","When training deep neural networks, the phenomenon of $\textit{dying neurons}$ $\unicode{x2013}$units that become inactive or saturated, output zero during training$\unicode{x2013}$ has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss in continual learning scenarios. In this paper, we reassess this phenomenon, focusing on sparsity and pruning. By systematically exploring the impact of various hyperparameter configurations on dying neurons, we unveil their potential to facilitate simple yet effective structured pruning algorithms. We introduce $\textit{Demon Pruning}$ (DemP), a method that controls the proliferation of dead neurons, dynamically leading to network sparsity. Achieved through a combination of noise injection on active units and a one-cycled schedule regularization strategy, DemP stands out for its simplicity and broad applicability. Experiments on CIFAR10 and ImageNet datasets demonstrate that DemP surpasses existing structured pruning techniques, showcasing superior accuracy-sparsity tradeoffs and training speedups. These findings suggest a novel perspective on dying neurons as a valuable resource for efficient model compression and optimization."
https://arxiv.org/abs/2403.07678,2024-03-12,MoralBERT: Detecting Moral Values in Social Discourse,"['Vjosa Preniqi', 'Iacopo Ghinassi', 'Kyriaki Kalimeri', 'Charalampos Saitis']","Morality plays a fundamental role in how we perceive information while greatly influencing our decisions and judgements. Controversial topics, including vaccination, abortion, racism, and sexuality, often elicit opinions and attitudes that are not solely based on evidence but rather reflect moral worldviews. Recent advances in natural language processing have demonstrated that moral values can be gauged in human-generated textual content. Here, we design a range of language representation models fine-tuned to capture exactly the moral nuances in text, called MoralBERT. We leverage annotated moral data from three distinct sources: Twitter, Reddit, and Facebook user-generated content covering various socially relevant topics. This approach broadens linguistic diversity and potentially enhances the models' ability to comprehend morality in various contexts. We also explore a domain adaptation technique and compare it to the standard fine-tuned BERT model, using two different frameworks for moral prediction: single-label and multi-label. We compare in-domain approaches with conventional models relying on lexicon-based techniques, as well as a Machine Learning classifier with Word2Vec representation. Our results showed that in-domain prediction models significantly outperformed traditional models. While the single-label setting reaches a higher accuracy than previously achieved for the task when using BERT pretrained models. Experiments in an out-of-domain setting, instead, suggest that further work is needed for existing domain adaptation techniques to generalise between different social media platforms, especially for the multi-label task. The investigations and outcomes from this study pave the way for further exploration, enabling a more profound comprehension of moral narratives about controversial social issues."
https://arxiv.org/abs/2403.07669,2024-03-12,Machine Learning for Soccer Match Result Prediction,"['Rory Bunker', 'Calvin Yeung', 'Keisuke Fujii']","Machine learning has become a common approach to predicting the outcomes of soccer matches, and the body of literature in this domain has grown substantially in the past decade and a half. This chapter discusses available datasets, the types of models and features, and ways of evaluating model performance in this application domain. The aim of this chapter is to give a broad overview of the current state and potential future developments in machine learning for soccer match results prediction, as a resource for those interested in conducting future studies in the area. Our main findings are that while gradient-boosted tree models such as CatBoost, applied to soccer-specific ratings such as pi-ratings, are currently the best-performing models on datasets containing only goals as the match features, there needs to be a more thorough comparison of the performance of deep learning models and Random Forest on a range of datasets with different types of features. Furthermore, new rating systems using both player- and team-level information and incorporating additional information from, e.g., spatiotemporal tracking and event data, could be investigated further. Finally, the interpretability of match result prediction models needs to be enhanced for them to be more useful for team management."
https://arxiv.org/abs/2403.07657,2024-03-12,Scalable Spatiotemporal Prediction with Bayesian Neural Fields,"['Feras Saad', 'Jacob Burnim', 'Colin Carroll', 'Brian Patton', 'Urs Köster', 'Rif A. Saurous', 'Matthew Hoffman']","Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequence of smooth differentiable transforms, posterior inference is conducted on large-scale data using variationally learned surrogates trained via stochastic gradient descent. We evaluate BayesNF against prominent statistical and machine-learning baselines, showing considerable improvements on diverse prediction problems from climate and public health datasets that contain tens to hundreds of thousands of measurements. The paper is accompanied with an open-source software package (https://github.com/google/bayesnf) that is easy-to-use and compatible with modern GPU and TPU accelerators on the JAX machine learning platform."
https://arxiv.org/abs/2403.07652,2024-03-12,Harder Tasks Need More Experts: Dynamic Routing in MoE Models,"['Quzhe Huang', 'Zhenwei An', 'Nan Zhuang', 'Mingxu Tao', 'Chen Zhang', 'Yang Jin', 'Kun Xu', 'Kun Xu', 'Liwei Chen', 'Songfang Huang', 'Yansong Feng']","In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing, which activates a predetermined number of experts regardless of the input's complexity, our method dynamically selects experts based on the confidence level in expert selection for each input. This allows for a more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced reasoning and fewer for simpler tasks. Through extensive evaluations, our dynamic routing method demonstrates substantial improvements over conventional Top-2 routing across various benchmarks, achieving an average improvement of 0.7% with less than 90% activated parameters. Further analysis shows our model dispatches more experts to tasks requiring complex reasoning skills, like BBH, confirming its ability to dynamically allocate computational resources in alignment with the input's complexity. Our findings also highlight a variation in the number of experts needed across different layers of the transformer model, offering insights into the potential for designing heterogeneous MoE frameworks. The code and models are available at https://github.com/ZhenweiAn/Dynamic_MoE."
https://arxiv.org/abs/2403.07632,2024-03-12,CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs for Reduced hERG Liability,"['Gregory W. Kyro', 'Matthew T. Martin', 'Eric D. Watt', 'Victor S. Batista']","Drug-induced cardiotoxicity is a major health concern which can lead to serious adverse effects including life-threatening cardiac arrhythmias via the blockade of the voltage-gated hERG potassium ion channel. It is therefore of tremendous interest to develop advanced methods to identify hERG-active compounds in early stages of drug development, as well as to optimize commercially available drugs for reduced hERG activity. In this work, we present CardioGenAI, a machine learning-based framework for re-engineering both developmental and marketed drugs for reduced hERG activity while preserving their pharmacological activity. The framework incorporates novel state-of-the-art discriminative models for predicting hERG channel activity, as well as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to their potential implications in modulating the arrhythmogenic potential induced by hERG channel blockade. These models can also serve independently as effective components of a virtual screening pipeline. We applied the complete framework to pimozide, an FDA-approved antipsychotic agent that demonstrates high affinity to the hERG channel, and generated 100 refined candidates. Remarkably, among the candidates is fluspirilene, a compound which is of the same class of drugs (diphenylmethanes) as pimozide and therefore has similar pharmacological activity, yet exhibits over 700-fold weaker binding to hERG. We have made all of our software open-source to facilitate integration of the CardioGenAI framework for molecular hypothesis generation into drug discovery workflows."
https://arxiv.org/abs/2403.07611,2024-03-12,Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning,"['Vinay Chakravarthi Gogineni', 'Esmaeil S. Nadimi']","Machine unlearning has garnered significant attention due to its ability to selectively erase knowledge obtained from specific training data samples in an already trained machine learning model. This capability enables data holders to adhere strictly to data protection regulations. However, existing unlearning techniques face practical constraints, often causing performance degradation, demanding brief fine-tuning post unlearning, and requiring significant storage. In response, this paper introduces a novel class of machine unlearning algorithms. First method is partial amnesiac unlearning, integration of layer-wise pruning with amnesiac unlearning. In this method, updates made to the model during training are pruned and stored, subsequently used to forget specific data from trained model. The second method assimilates layer-wise partial-updates into label-flipping and optimization-based unlearning to mitigate the adverse effects of data deletion on model efficacy. Through a detailed experimental evaluation, we showcase the effectiveness of proposed unlearning methods. Experimental results highlight that the partial amnesiac unlearning not only preserves model efficacy but also eliminates the necessity for brief post fine-tuning, unlike conventional amnesiac unlearning. Moreover, employing layer-wise partial updates in label-flipping and optimization-based unlearning techniques demonstrates superiority in preserving model efficacy compared to their naive counterparts."
https://arxiv.org/abs/2403.07608,2024-03-12,Couler: Unified Machine Learning Workflow Optimization in Cloud,"['Xiaoda Wang', 'Yuan Tang', 'Tengda Guo', 'Bo Sang', 'Jingji Wu', 'Jian Sha', 'Ke Zhang', 'Jiang Qian', 'Mingjie Tang']","Machine Learning (ML) has become ubiquitous, fueling data-driven applications across various organizations. Contrary to the traditional perception of ML in research, ML workflows can be complex, resource-intensive, and time-consuming. Expanding an ML workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs. Currently, numerous workflow engines are available (with over ten being widely recognized). This variety poses a challenge for end-users in terms of mastering different engine APIs. While efforts have primarily focused on optimizing ML Operations (MLOps) for a specific workflow engine, current methods largely overlook workflow optimization across different engines."
https://arxiv.org/abs/2403.07598,2024-03-12,Mondrian: On-Device High-Performance Video Analytics with Compressive Packed Inference,"['Changmin Jeon', 'Seonjun Kim', 'Juheon Yi', 'Youngki Lee']","In this paper, we present Mondrian, an edge system that enables high-performance object detection on high-resolution video streams. Many lightweight models and system optimization techniques have been proposed for resource-constrained devices, but they do not fully utilize the potential of the accelerators over dynamic, high-resolution videos. To enable such capability, we devise a novel Compressive Packed Inference to minimize per-pixel processing costs by selectively determining the necessary pixels to process and combining them to maximize processing parallelism. In particular, our system quickly extracts ROIs and dynamically shrinks them, reflecting the effect of the fast-changing characteristics of objects and scenes. It then intelligently combines such scaled ROIs into large canvases to maximize the utilization of inference accelerators such as GPU. Evaluation across various datasets, models, and devices shows Mondrian outperforms state-of-the-art baselines (e.g., input rescaling, ROI extractions, ROI extractions+batching) by 15.0-19.7% higher accuracy, leading to $\times$6.65 higher throughput than frame-wise inference for processing various 1080p video streams. We will release the code after the paper review."
https://arxiv.org/abs/2403.07594,2024-03-12,Stability of Stationary Solutions to the Nonisentropic Euler-Poisson System in a Perturbed Half Space,"['Mingjie Li', 'Masahiro Suzuki']","The main concern of this paper is to mathematically investigate the formation of a plasma sheath near the surface of nonplanar walls. We study the existence and asymptotic stability of stationary solutions for the nonisentropic Euler-Poisson equations in a domain of which boundary is drawn by a graph, by employing a space weighted energy method. Moreover, the convergence rate of the solution toward the stationary solution is obtained, provided that the initial perturbation belongs to the weighted Sobolev space. Because the domain is the perturbed half space, we first show the time-global solvability of the nonisentropic Euler-Poisson equations, then construct stationary solutions by using the time-global solutions."
https://arxiv.org/abs/2403.07588,2024-03-12,Visual Privacy Auditing with Diffusion Models,"['Kristian Schwethelm', 'Johannes Kaiser', 'Moritz Knolle', 'Daniel Rueckert', 'Georgios Kaissis', 'Alexander Ziller']","Image reconstruction attacks on machine learning models pose a significant risk to privacy by potentially leaking sensitive information. Although defending against such attacks using differential privacy (DP) has proven effective, determining appropriate DP parameters remains challenging. Current formal guarantees on data reconstruction success suffer from overly theoretical assumptions regarding adversary knowledge about the target data, particularly in the image domain. In this work, we empirically investigate this discrepancy and find that the practicality of these assumptions strongly depends on the domain shift between the data prior and the reconstruction target. We propose a reconstruction attack based on diffusion models (DMs) that assumes adversary access to real-world image priors and assess its implications on privacy leakage under DP-SGD. We show that (1) real-world data priors significantly influence reconstruction success, (2) current reconstruction bounds do not model the risk posed by data priors well, and (3) DMs can serve as effective auditing tools for visualizing privacy leakage."
https://arxiv.org/abs/2403.07586,2024-03-12,Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments,"['Saksham Checker', 'Nikhil Churamani', 'Hatice Gunes']","As social robots become increasingly integrated into daily life, ensuring their behaviours align with social norms is crucial. For their widespread open-world application, it is important to explore Federated Learning (FL) settings where individual robots can learn about their unique environments while also learning from each others' experiences. In this paper, we present a novel FL benchmark that evaluates different strategies, using multi-label regression objectives, where each client individually learns to predict the social appropriateness of different robot actions while also sharing their learning with others. Furthermore, splitting the training data by different contexts such that each client incrementally learns across contexts, we present a novel Federated Continual Learning (FCL) benchmark that adapts FL-based methods to use state-of-the-art Continual Learning (CL) methods to continually learn socially appropriate agent behaviours under different contextual settings. Federated Averaging (FedAvg) of weights emerges as a robust FL strategy while rehearsal-based FCL enables incrementally learning the social appropriateness of robot actions, across contextual splits."
https://arxiv.org/abs/2403.07584,2024-03-12,Molecularity: a fast and efficient criterion for probing superconductivity,"['Matías E. di Mauro', 'Benoît Braïda', 'Ion Errea', 'Trinidad Novoa', 'Julia Contreras-García']","We present an efficient criterion for probing the critical temperature of hydrogen based superconductors. We start by expanding the applicability of 3D descriptors of electron localization to superconducting states within the framework of superconducting DFT. We first apply this descriptor to a model system, the hydrogen chain, which allows to prove two main concepts: i) that the electron localization changes very little when the transition from the normal to the superconducting state takes place, i.e. that it can be described at the DFT level from the normal state; and ii) that the formation of molecules can be characterized within this theoretical framework, enabling to filter out systems with marked molecular character and hence with low potential to be good superconductors. These two ideas, are then exploited in real binary and ternary systems, showing i) that the bonding type can be characterized automatically; and ii) that this provides a new index which enables to feed machine learning algorithms for a better prediction of critical temperatures. Overall, this sets a grounded theoretical scenario for an automatic and efficient high-throughput of potential hydrogen based superconductors."
https://arxiv.org/abs/2403.07581,2024-03-12,LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model,"['Linmei Hu', 'Hongyu He', 'Duokang Wang', 'Ziwang Zhao', 'Yingxia Shao', 'Liqiang Nie']","Personality detection aims to detect one's personality traits underlying in social media posts. One challenge of this task is the scarcity of ground-truth personality traits which are collected from self-report questionnaires. Most existing methods learn post features directly by fine-tuning the pre-trained language models under the supervision of limited personality labels. This leads to inferior quality of post features and consequently affects the performance. In addition, they treat personality traits as one-hot classification labels, overlooking the semantic information within them. In this paper, we propose a large language model (LLM) based text augmentation enhanced personality detection model, which distills the LLM's knowledge to enhance the small model for personality detection, even when the LLM fails in this task. Specifically, we enable LLM to generate post analyses (augmentations) from the aspects of semantic, sentiment, and linguistic, which are critical for personality detection. By using contrastive learning to pull them together in the embedding space, the post encoder can better capture the psycho-linguistic information within the post representations, thus improving personality detection. Furthermore, we utilize the LLM to enrich the information of personality labels for enhancing the detection performance. Experimental results on the benchmark datasets demonstrate that our model outperforms the state-of-the-art methods on personality detection."
https://arxiv.org/abs/2403.07573,2024-03-12,Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC),"['Masoud Shokrnezhad', 'Hao Yu', 'Tarik Taleb', 'Richard Li', 'Kyunghan Lee', 'Jaeseung Song', 'Cedric Westphal']","In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites. The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration. While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources. Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent requirements. ACNC encompasses two primary functionalities: state recognition and context detection. Given the intricate nature of the user-service-computing-network space, the paper employs dimension reduction to generate live, holistic, abstract system states in a hierarchical structure. To address the challenges posed by dynamic changes, Continual Learning (CL) is employed, classifying the system state into contexts controlled by dedicated ML agents, enabling them to operate efficiently. These two functionalities are intricately linked within a closed loop overseen by the End-to-End (E2E) orchestrator to allocate resources. The paper introduces the components of ACNC, proposes a Metaverse scenario to exemplify ACNC's role in resource provisioning with Segment Routing v6 (SRv6), outlines ACNC's workflow, details a numerical analysis for efficiency assessment, and concludes with discussions on relevant challenges and potential avenues for future research."
https://arxiv.org/abs/2403.07567,2024-03-12,Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource Agglutinative Data-to-Text Generation,"['Francois Meyer', 'Jan Buys']","Most data-to-text datasets are for English, so the difficulties of modelling data-to-text for low-resource languages are largely unexplored. In this paper we tackle data-to-text for isiXhosa, which is low-resource and agglutinative. We introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of WebNLG, which presents a new linguistic context that shifts modelling demands to subword-driven techniques. We also develop an evaluation framework for T2X that measures how accurately generated text describes the data. This enables future users of T2X to go beyond surface-level metrics in evaluation. On the modelling side we explore two classes of methods - dedicated data-to-text models trained from scratch and pretrained language models (PLMs). We propose a new dedicated architecture aimed at agglutinative data-to-text, the Subword Segmental Pointer Generator (SSPG). It jointly learns to segment words and copy entities, and outperforms existing dedicated models for 2 agglutinative languages (isiXhosa and Finnish). We investigate pretrained solutions for T2X, which reveals that standard PLMs come up short. Fine-tuning machine translation models emerges as the best method overall. These findings underscore the distinct challenge presented by T2X: neither well-established data-to-text architectures nor customary pretrained methodologies prove optimal. We conclude with a qualitative analysis of generation errors and an ablation study."
https://arxiv.org/abs/2403.07554,2024-03-12,An Adaptive Learning Approach to Multivariate Time Forecasting in Industrial Processes,"['Fernando Miguelez', 'Josu Doncel', 'Maria Dolores Ugarte']","Industrial processes generate a massive amount of monitoring data that can be exploited to uncover hidden time losses in the system, leading to enhanced accuracy of maintenance policies and, consequently, increasing the effectiveness of the equipment. In this work, we propose a method for one-step probabilistic multivariate forecasting of time variables based on a Hidden Markov Model with covariates (IO-HMM). These covariates account for the correlation of the predicted variables with their past values and additional process measurements by means of a discrete model and a continuous model. The probabilities of the former are updated using Bayesian principles, while the parameter estimates for the latter are recursively computed through an adaptive algorithm that also admits a Bayesian interpretation. This approach permits the integration of new samples into the estimation of unknown parameters, computationally improving the efficiency of the process. We evaluate the performance of the method using a real data set obtained from a company of a particular sector; however, it is a versatile technique applicable to any other data set. The results show a consistent improvement over a persistence model, which assumes that future values are the same as current values, and more importantly, over univariate versions of our model."
https://arxiv.org/abs/2403.07544,2024-03-12,MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki,"['Timothee Mickus', 'Stig-Arne Grönroos', 'Joseph Attieh', 'Michele Boggia', 'Ona De Gibert', 'Shaoxiong Ji', 'Niki Andreas Lopi', 'Alessandro Raganato', 'Raúl Vázquez', 'Jörg Tiedemann']","NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled. The trend goes to modularization, a necessary step into the direction of designing smaller sub-networks and components with specialized functionality. In this paper, we present the MAMMOTH toolkit: a framework designed for training massively multilingual modular machine translation systems at scale, initially derived from OpenNMT-py and then adapted to ensure efficient training across computation clusters. We showcase its efficiency across clusters of A100 and V100 NVIDIA GPUs, and discuss our design philosophy and plans for future information. The toolkit is publicly available online."
https://arxiv.org/abs/2403.07540,2024-03-12,WannaLaugh: A Configurable Ransomware Emulator -- Learning to Mimic Malicious Storage Traces,"['Dionysios Diamantopolous', 'Roman Pletka', 'Slavisa Sarafijanovic', 'A. L. Narasimha Reddy', 'Haris Pozidis']","Ransomware, a fearsome and rapidly evolving cybersecurity threat, continues to inflict severe consequences on individuals and organizations worldwide. Traditional detection methods, reliant on static signatures and application behavioral patterns, are challenged by the dynamic nature of these threats. This paper introduces three primary contributions to address this challenge. First, we introduce a ransomware emulator. This tool is designed to safely mimic ransomware attacks without causing actual harm or spreading malware, making it a unique solution for studying ransomware behavior. Second, we demonstrate how we use this emulator to create storage I/O traces. These traces are then utilized to train machine-learning models. Our results show that these models are effective in detecting ransomware, highlighting the practical application of our emulator in developing responsible cybersecurity tools. Third, we show how our emulator can be used to mimic the I/O behavior of existing ransomware thereby enabling safe trace collection. Both the emulator and its application represent significant steps forward in ransomware detection in the era of machine-learning-driven cybersecurity."
https://arxiv.org/abs/2403.07536,2024-03-12,LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes,"['Julian Suk', 'Baris Imre', 'Jelmer M. Wolterink']","Many anatomical structures can be described by surface or volume meshes. Machine learning is a promising tool to extract information from these 3D models. However, high-fidelity meshes often contain hundreds of thousands of vertices, which creates unique challenges in building deep neural network architectures. Furthermore, patient-specific meshes may not be canonically aligned which limits the generalisation of machine learning algorithms. We propose LaB-GATr, a transfomer neural network with geometric tokenisation that can effectively learn with large-scale (bio-)medical surface and volume meshes through sequence compression and interpolation. Our method extends the recently proposed geometric algebra transformer (GATr) and thus respects all Euclidean symmetries, i.e. rotation, translation and reflection, effectively mitigating the problem of canonical alignment between patients. LaB-GATr achieves state-of-the-art results on three tasks in cardiovascular hemodynamics modelling and neurodevelopmental phenotype prediction, featuring meshes of up to 200,000 vertices. Our results demonstrate that LaB-GATr is a powerful architecture for learning with high-fidelity meshes which has the potential to enable interesting downstream applications. Our implementation is publicly available."
https://arxiv.org/abs/2403.07507,2024-03-12,Reconstructions of Jupiter's magnetic field using physics informed neural networks,"['Philip W. Livermore', 'Leyuan Wu', 'Longwei Chen', 'Sjoerd A. L. de Ridder']","Magnetic sounding using data collected from the Juno mission can be used to provide constraints on Jupiter's interior. However, inwards continuation of reconstructions assuming zero electrical conductivity and a representation in spherical harmonics are limited by the enhancement of noise at small scales. In this paper we describe new reconstructions of Jupiter's internal magnetic field based on physics-informed neural networks and either the first 33 (PINN33) or the first 50 (PINN50) of Juno's orbits. The method can resolve local structures, and allows for weak ambient electrical currents. Compared with other methods, our reconstructions of Jupiter's magnetic field both on and above the surface are similar, and we achieve a similar fit to the Juno data. However, our models are not hampered by noise at depth, and so offer a much clearer picture of the interior structure. We estimate that the dynamo boundary is at a fractional radius of 0.8. At this depth, the magnetic field is arranged into longitudinal bands, and the great blue spot appears to be rooted in neighbouring structures of oppositely signed flux."
https://arxiv.org/abs/2403.07501,2024-03-12,Detecting Security-Relevant Methods using Multi-label Machine Learning,"['Oshando Johnson', 'Goran Piskachev', 'Ranjith Krishnamurthy', 'Eric Bodden']","To detect security vulnerabilities, static analysis tools need to be configured with security-relevant methods. Current approaches can automatically identify such methods using binary relevance machine learning approaches. However, they ignore dependencies among security-relevant methods, over-generalize and perform poorly in practice. Additionally, users have to nevertheless manually configure static analysis tools using the detected methods. Based on feedback from users and our observations, the excessive manual steps can often be tedious, error-prone and counter-intuitive."
https://arxiv.org/abs/2403.07483,2024-03-12,A Deep Learning Approach to Diabetes Diagnosis,"['Zeyu Zhang', 'Khandaker Asif Ahmed', 'Md Rakibul Hasan', 'Tom Gedeon', 'Md Zakir Hossain']","Diabetes, resulting from inadequate insulin production or utilization, causes extensive harm to the body. Existing diagnostic methods are often invasive and come with drawbacks, such as cost constraints. Although there are machine learning models like Classwise k Nearest Neighbor (CkNN) and General Regression Neural Network (GRNN), they struggle with imbalanced data and result in under-performance. Leveraging advancements in sensor technology and machine learning, we propose a non-invasive diabetes diagnosis using a Back Propagation Neural Network (BPNN) with batch normalization, incorporating data re-sampling and normalization for class balancing. Our method addresses existing challenges such as limited performance associated with traditional machine learning. Experimental results on three datasets show significant improvements in overall accuracy, sensitivity, and specificity compared to traditional methods. Notably, we achieve accuracies of 89.81% in Pima diabetes dataset, 75.49% in CDC BRFSS2015 dataset, and 95.28% in Mesra Diabetes dataset. This underscores the potential of deep learning models for robust diabetes diagnosis. See project website https://steve-zeyu-zhang.github.io/DiabetesDiagnosis/"
https://arxiv.org/abs/2403.07460,2024-03-12,Experimental Comparison of Ensemble Methods and Time-to-Event Analysis Models Through Integrated Brier Score and Concordance Index,"['Camila Fernandez', 'Chung Shue Chen', 'Chen Pierre Gaillard', 'Alonso Silva']","Time-to-event analysis is a branch of statistics that has increased in popularity during the last decades due to its many application fields, such as predictive maintenance, customer churn prediction and population lifetime estimation. In this paper, we review and compare the performance of several prediction models for time-to-event analysis. These consist of semi-parametric and parametric statistical models, in addition to machine learning approaches. Our study is carried out on three datasets and evaluated in two different scores (the integrated Brier score and concordance index). Moreover, we show how ensemble methods, which surprisingly have not yet been much studied in time-to-event analysis, can improve the prediction accuracy and enhance the robustness of the prediction performance. We conclude the analysis with a simulation experiment in which we evaluate the factors influencing the performance ranking of the methods using both scores."
https://arxiv.org/abs/2403.07454,2024-03-12,"Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings","['Henrik Häggström', 'Pedro L. C. Rodrigues', 'Geoffroy Oudoumanessah', 'Florence Forbes', 'Umberto Picchini']","Bayesian inference for complex models with an intractable likelihood can be tackled using algorithms performing many calls to computer simulators. These approaches are collectively known as ""simulation-based inference"" (SBI). Recent SBI methods have made use of neural networks (NN) to provide approximate, yet expressive constructs for the unavailable likelihood function and the posterior distribution. However, they do not generally achieve an optimal trade-off between accuracy and computational demand. In this work, we propose an alternative that provides both approximations to the likelihood and the posterior distribution, using structured mixtures of probability distributions. Our approach produces accurate posterior inference when compared to state-of-the-art NN-based SBI methods, while exhibiting a much smaller computational footprint. We illustrate our results on several benchmark models from the SBI literature."
https://arxiv.org/abs/2403.07444,2024-03-12,A Survey on Federated Learning in Intelligent Transportation Systems,"['Rongqing Zhang', 'Hanqiu Wang', 'Bing Li', 'Xiang Cheng', 'Liuqing Yang']","The development of Intelligent Transportation System (ITS) has brought about comprehensive urban traffic information that not only provides convenience to urban residents in their daily lives but also enhances the efficiency of urban road usage, leading to a more harmonious and sustainable urban life. Typical scenarios in ITS mainly include traffic flow prediction, traffic target recognition, and vehicular edge computing. However, most current ITS applications rely on a centralized training approach where users upload source data to a cloud server with high computing power for management and centralized training. This approach has limitations such as poor real-time performance, data silos, and difficulty in guaranteeing data privacy. To address these limitations, federated learning (FL) has been proposed as a promising solution. In this paper, we present a comprehensive review of the application of FL in ITS, with a particular focus on three key scenarios: traffic flow prediction, traffic target recognition, and vehicular edge computing. For each scenario, we provide an in-depth analysis of its key characteristics, current challenges, and specific manners in which FL is leveraged. Moreover, we discuss the benefits that FL can offer as a potential solution to the limitations of the centralized training approach currently used in ITS applications."
https://arxiv.org/abs/2403.07436,2024-03-12,JSTR: Joint Spatio-Temporal Reasoning for Event-based Moving Object Detection,"['Hanyu Zhou', 'Zhiwei Shi', 'Hao Dong', 'Shihan Peng', 'Yi Chang', 'Luxin Yan']","Event-based moving object detection is a challenging task, where static background and moving object are mixed together. Typically, existing methods mainly align the background events to the same spatial coordinate system via motion compensation to distinguish the moving object. However, they neglect the potential spatial tailing effect of moving object events caused by excessive motion, which may affect the structure integrity of the extracted moving object. We discover that the moving object has a complete columnar structure in the point cloud composed of motion-compensated events along the timestamp. Motivated by this, we propose a novel joint spatio-temporal reasoning method for event-based moving object detection. Specifically, we first compensate the motion of background events using inertial measurement unit. In spatial reasoning stage, we project the compensated events into the same image coordinate, discretize the timestamp of events to obtain a time image that can reflect the motion confidence, and further segment the moving object through adaptive threshold on the time image. In temporal reasoning stage, we construct the events into a point cloud along timestamp, and use RANSAC algorithm to extract the columnar shape in the cloud for peeling off the background. Finally, we fuse the results from the two reasoning stages to extract the final moving object region. This joint spatio-temporal reasoning framework can effectively detect the moving object from motion confidence and geometric structure. Moreover, we conduct extensive experiments on various datasets to verify that the proposed method can improve the moving object detection accuracy by 13\%."
https://arxiv.org/abs/2403.07434,2024-03-12,DALSA: Domain Adaptation for Supervised Learning From Sparsely Annotated MR Images,"['Michael Götz', 'Christian Weber', 'Franciszek Binczyk', 'Joanna Polanska', 'Rafal Tarnawski', 'Barbara Bobek-Billewicz', 'Ullrich Köthe', 'Jens Kleesiek', 'Bram Stieltjes', 'Klaus H. Maier-Hein']","We propose a new method that employs transfer learning techniques to effectively correct sampling selection errors introduced by sparse annotations during supervised learning for automated tumor segmentation. The practicality of current learning-based automated tissue classification approaches is severely impeded by their dependency on manually segmented training databases that need to be recreated for each scenario of application, site, or acquisition setup. The comprehensive annotation of reference datasets can be highly labor-intensive, complex, and error-prone. The proposed method derives high-quality classifiers for the different tissue classes from sparse and unambiguous annotations and employs domain adaptation techniques for effectively correcting sampling selection errors introduced by the sparse sampling. The new approach is validated on labeled, multi-modal MR images of 19 patients with malignant gliomas and by comparative analysis on the BraTS 2013 challenge data sets. Compared to training on fully labeled data, we reduced the time for labeling and training by a factor greater than 70 and 180 respectively without sacrificing accuracy. This dramatically eases the establishment and constant extension of large annotated databases in various scenarios and imaging setups and thus represents an important step towards practical applicability of learning-based approaches in tissue classification."
https://arxiv.org/abs/2403.07428,2024-03-12,Input Data Adaptive Learning (IDAL) for Sub-acute Ischemic Stroke Lesion Segmentation,"['Michael Götz', 'Christian Weber', 'Christoph Kolb', 'Klaus Maier-Hein']","In machine learning larger databases are usually associated with higher classification accuracy due to better generalization. This generalization may lead to non-optimal classifiers in some medical applications with highly variable expressions of pathologies. This paper presents a method for learning from a large training base by adaptively selecting optimal training samples for given input data. In this way heterogeneous databases are supported two-fold. First, by being able to deal with sparsely annotated data allows a quick inclusion of new data set and second, by training an input-dependent classifier. The proposed approach is evaluated using the SISS challenge. The proposed algorithm leads to a significant improvement of the classification accuracy."
https://arxiv.org/abs/2403.07424,2024-03-12,Automated Discovery of Anomalous Features in Ultra-Large Planetary Remote Sensing Datasets using Variational Autoencoders,"['Adam Lesnikowski', 'Valentin T. Bickel', 'Daniel Angerhausen']","The NASA Lunar Reconnaissance Orbiter (LRO) has returned petabytes of lunar high spatial resolution surface imagery over the past decade, impractical for humans to fully review manually. Here we develop an automated method using a deep generative visual model that rapidly retrieves scientifically interesting examples of LRO surface imagery representing the first planetary image anomaly detector. We give quantitative experimental evidence that our method preferentially retrieves anomalous samples such as notable geological features and known human landing and spacecraft crash sites. Our method addresses a major capability gap in planetary science and presents a novel way to unlock insights hidden in ever-increasing remote sensing data archives, with numerous applications to other science domains. We publish our code and data along with this paper."
https://arxiv.org/abs/2403.07420,2024-03-12,DragAnything: Motion Control for Anything using Entity Representation,"['Weijia Wu', 'Zhuang Li', 'Yuchao Gu', 'Rui Zhao', 'Yefei He', 'David Junhao Zhang', 'Mike Zheng Shou', 'Yan Li', 'Tingting Gao', 'Di Zhang']","We introduce DragAnything, which utilizes a entity representation to achieve motion control for any object in controllable video generation. Comparison to existing motion control methods, DragAnything offers several advantages. Firstly, trajectory-based is more userfriendly for interaction, when acquiring other guidance signals (e.g., masks, depth maps) is labor-intensive. Users only need to draw a line (trajectory) during interaction. Secondly, our entity representation serves as an open-domain embedding capable of representing any object, enabling the control of motion for diverse entities, including background. Lastly, our entity representation allows simultaneous and distinct motion control for multiple objects. Extensive experiments demonstrate that our DragAnything achieves state-of-the-art performance for FVD, FID, and User Study, particularly in terms of object motion control, where our method surpasses the previous methods (e.g., DragNUWA) by 26% in human voting."
https://arxiv.org/abs/2403.07404,2024-03-12,Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning,"['Filip Szatkowski', 'Fei Yang', 'Bartłomiej Twardowski', 'Tomasz Trzciński', 'Joost van de Weijer']","Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention. These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources. However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data. This study aims to explore the continual learning of the early-exit networks. We adapt existing continual learning methods to fit with early-exit architectures and investigate their behavior in the continual setting. We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources. Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task-wise Logits Correction (TLC), a simple method that equalizes this bias and improves the network performance for every given compute budget in the class-incremental setting. We assess the accuracy and computational cost of various continual learning techniques enhanced with early-exits and TLC across standard class-incremental learning benchmarks such as 10 split CIFAR100 and ImageNetSubset and show that TLC can achieve the accuracy of the standard methods using less than 70\% of their computations. Moreover, at full computational budget, our method outperforms the accuracy of the standard counterparts by up to 15 percentage points. Our research underscores the inherent synergy between early-exit networks and continual learning, emphasizing their practical utility in resource-constrained environments."
https://arxiv.org/abs/2403.07393,2024-03-12,Learning on the correct class for domain inverse problems of gravimetry,"['Yihang Chen', 'Wenbin Li']","We consider end-to-end learning approaches for inverse problems of gravimetry. Due to ill-posedness of the inverse gravimetry, the reliability of learning approaches is questionable. To deal with this problem, we propose the strategy of learning on the correct class. The well-posedness theorems are employed when designing the neural-network architecture and constructing the training set. Given the density-contrast function as a priori information, the domain of mass can be uniquely determined under certain constrains, and the domain inverse problem is a correct class of the inverse gravimetry. Under this correct class, we design the neural network for learning by mimicking the level-set formulation for the inverse gravimetry. Numerical examples illustrate that the method is able to recover mass models with non-constant density contrast."
https://arxiv.org/abs/2403.07391,2024-03-12,Towards adiabatic-connection interpolation model with broader applicability,"['Lucian A. Constantin', 'Szymon Śmiga', 'Fabio Della Sala']","The Adiabatic Connection Integrand Interpolation (ACII) method represents a general path for calculating correlation energies in electronic systems within the Den sity Functional Theory. ACII functionals include both exact-exchange and the second-order correlation energy, as well as an interpolating function toward the strictly-correlated electron (SCE) regime. Several interpolating functions have been proposed in the last years targeting different properties, yet an accurate ACII approach with broad applicability is sti ll missing. Recently, we have proposed an ACII functional that was made accurate for the three-dimensional (3D) uniform electron gas as well as for model metal clusters. In this work we present an ACII functional (named genISI2) which is very accurate for both three-dimensional (3D) and two-dimensional (2D) uniform electron gases and for the q uasi-2D infinite barrier model, where most of the exchange-correlation functionals fail badly, as well as for strongly correlated two-electrons systems. Using the exact-exchange Kohn-Sham orbitals, we have also assessed the genISI2 for various molecular systems, showing a superior performance with respect to the o ther ACII methods for total energies, atomization energies, and ionization potentials. The genISI2 functional can thus find application in a broad range of systems and properties."
https://arxiv.org/abs/2403.07378,2024-03-12,SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression,"['Xin Wang', 'Yu Zheng', 'Zhongwei Wan', 'Mi Zhang']","The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven models from three different LLM families at four different scales. Our results demonstrate the superiority of SVD-LLM over state-of-the-arts, especially at high model compression ratios. The source code is available at https://github.com/AIoT-MLSys-Lab/SVD-LLM."
https://arxiv.org/abs/2403.07372,2024-03-12,Eliminating Cross-modal Conflicts in BEV Space for LiDAR-Camera 3D Object Detection,"['Jiahui Fu', 'Chen Gao', 'Zitian Wang', 'Lirong Yang', 'Xiaofei Wang', 'Beipeng Mu', 'Si Liu']","Recent 3D object detectors typically utilize multi-sensor data and unify multi-modal features in the shared bird's-eye view (BEV) representation space. However, our empirical findings indicate that previous methods have limitations in generating fusion BEV features free from cross-modal conflicts. These conflicts encompass extrinsic conflicts caused by BEV feature construction and inherent conflicts stemming from heterogeneous sensor signals. Therefore, we propose a novel Eliminating Conflicts Fusion (ECFusion) method to explicitly eliminate the extrinsic/inherent conflicts in BEV space and produce improved multi-modal BEV features. Specifically, we devise a Semantic-guided Flow-based Alignment (SFA) module to resolve extrinsic conflicts via unifying spatial distribution in BEV space before fusion. Moreover, we design a Dissolved Query Recovering (DQR) mechanism to remedy inherent conflicts by preserving objectness clues that are lost in the fusion BEV feature. In general, our method maximizes the effective information utilization of each modality and leverages inter-modal complementarity. Our method achieves state-of-the-art performance in the highly competitive nuScenes 3D object detection dataset. The code is released at https://github.com/fjhzhixi/ECFusion."
https://arxiv.org/abs/2403.07362,2024-03-12,Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning,"['Chongyu Fan', 'Jiancheng Liu', 'Alfred Hero', 'Sijia Liu']","The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization level to emulate worst-case scenarios, while simultaneously engaging in standard training and unlearning at the lower level, achieving a balance between data influence erasure and model utility. Our proposal offers a worst-case evaluation of MU's resilience and effectiveness. Through extensive experiments across different datasets (including CIFAR-10, 100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image classifiers and generative models), we expose critical pros and cons in existing (approximate) unlearning strategies. Our results illuminate the complex challenges of MU in practice, guiding the future development of more accurate and robust unlearning algorithms. The code is available at https://github.com/OPTML-Group/Unlearn-WorstCase."
https://arxiv.org/abs/2403.07360,2024-03-12,Optimization of Pressure Management Strategies for Geological CO2 Sequestration Using Surrogate Model-based Reinforcement Learning,"['Jungang Chen', 'Eduardo Gildin', 'John E. Killough']","Injecting greenhouse gas into deep underground reservoirs for permanent storage can inadvertently lead to fault reactivation, caprock fracturing and greenhouse gas leakage when the injection-induced stress exceeds the critical threshold. Extraction of pre-existing fluids at various stages of injection process, referred as pressure management, can mitigate associated risks and lessen environmental impact. However, identifying optimal pressure management strategies typically requires thousands of full-order simulations due to the need for function evaluations, making the process computationally prohibitive. This paper introduces a novel surrogate model-based reinforcement learning method for devising optimal pressure management strategies for geological CO2 sequestration efficiently. Our approach comprises two steps. Firstly, a surrogate model is developed through the embed to control method, which employs an encoder-transition-decoder structure to learn latent dynamics. Leveraging this proxy model, reinforcement learning is utilized to find an optimal strategy that maximizes economic benefits while satisfying various control constraints. The reinforcement learning agent receives the latent state space representation and immediate reward tailored for CO2 sequestration and choose real-time controls which are subject to predefined engineering constraints in order to maximize the long-term cumulative rewards. To demonstrate its effectiveness, this framework is applied to a compositional simulation model where CO2 is injected into saline aquifer. The results reveal that our surrogate model-based reinforcement learning approach significantly optimizes CO2 sequestration strategies, leading to notable economic gains compared to baseline scenarios."
https://arxiv.org/abs/2403.07358,2024-03-12,A novel fast iterative moment method for near-continuum flows,"['Guanghan Li', 'Chunwu Wang', 'Zhicheng Hu']","In this paper, we develop a novel fast iterative moment method for the steady-state simulation of near-continuum flows, which are modeled by the high-order moment system derived from the Boltzmann-BGK equation. The fast convergence of the present method is mainly achieved by alternately solving the moment system and the hydrodynamic equations with compatible constitutive relations and boundary conditions. To be specific, the compatible hydrodynamic equations are solved in each iteration to get improved predictions of macroscopic quantities, which are subsequently utilized to expedite the evolution of the moment system. Additionally, a semi-implicit scheme treating the collision term implicitly is introduced for the moment system. With cell-by-cell sweeping strategy, the resulting alternating iteration can be further accelerated for steady-state computation. It is also worth mentioning that such an alternating iteration works well with the nonlinear multigrid method. Numerical experiments for planar Couette flow, shock structure, and lid-driven cavity flow are carried out to investigate the performance of the proposed fast iterative moment method, and all results show wonderful efficiency and robustness."
https://arxiv.org/abs/2403.07357,2024-03-12,Real-time observation of picosecond-timescale optical quantum entanglement toward ultrafast quantum information processing,"['Akito Kawasaki', 'Hector Brunel', 'Ryuhoh Ide', 'Takumi Suzuki', 'Takahiro Kashiwazaki', 'Asuka Inoue', 'Takeshi Umeki', 'Taichi Yamashima', 'Atsushi Sakaguchi', 'Kan Takase', 'Mamoru Endo', 'Warit Asavanant', 'Akira Furusawa']","Entanglement is a fundamental resource of various optical quantum-information-processing (QIP) applications. Towards high-speed QIP system, entanglement should be encoded in short wavepackets. We report real-time observation of ultrafast optical Einstein-Podolsky-Rosen (EPR) correlation at a picosecond timescale in a continuous-wave (CW) system. Optical phase-sensitive amplification using 6-THz-bandwidth waveguide-optical-parametric amplifier enhances the effective efficiency of 70-GHz-bandwidth homodyne detectors, mainly used in 5th-generation telecommunication, enabling its use in real-time quantum-state measurement. While power measurement using frequency scanning, i.e., optical spectrum analyzer, is not performed in real-time, our observation is demonstrated through real-time amplitude measurement and can be directly employed in QIP applications. Observed EPR states show quantum correlation of 4.5 dB below shotnoise level encoded in wavepackets with 40-ps period, equivalent to 25-GHz repetition -- ${10^3}$ times faster than previous entanglement observation in CW system. The quantum correlation of 4.5 dB is already sufficient for several QIP applications, and our system can be readily extended to large-scale entanglement. Moreover, our scheme has high compatibility with optical communication technology such as wavelength-division multiplexing, and femtosecond-timescale observation is also feasible. Our demonstration is paradigm shift in accelerating accessible quantum correlation, the foundational resource of all quantum applications, from the nanosecond to picosecond timescale, enabling ultra-fast optical QIP."
https://arxiv.org/abs/2403.07353,2024-03-13,Graph Unlearning with Efficient Partial Retraining,"['Jiahao Zhang', 'Lin Wang', 'Shijie Wang', 'Wenqi Fan']","Graph Neural Networks (GNNs) have achieved remarkable success in various real-world applications. However, GNNs may be trained on undesirable graph data, which can degrade their performance and reliability. To enable trained GNNs to efficiently unlearn unwanted data, a desirable solution is retraining-based graph unlearning, which partitions the training graph into subgraphs and trains sub-models on them, allowing fast unlearning through partial retraining. However, the graph partition process causes information loss in the training graph, resulting in the low model utility of sub-GNN models. In this paper, we propose GraphRevoker, a novel graph unlearning framework that better maintains the model utility of unlearnable GNNs. Specifically, we preserve the graph property with graph property-aware sharding and effectively aggregate the sub-GNN models for prediction with graph contrastive sub-model aggregation. We conduct extensive experiments to demonstrate the superiority of our proposed approach."
https://arxiv.org/abs/2403.07346,2024-03-12,Complementing Event Streams and RGB Frames for Hand Mesh Reconstruction,"['Jianping Jiang', 'Xinyu Zhou', 'Bingxuan Wang', 'Xiaoming Deng', 'Chao Xu', 'Boxin Shi']","Reliable hand mesh reconstruction (HMR) from commonly-used color and depth sensors is challenging especially under scenarios with varied illuminations and fast motions. Event camera is a highly promising alternative for its high dynamic range and dense temporal resolution properties, but it lacks key texture appearance for hand mesh reconstruction. In this paper, we propose EvRGBHand -- the first approach for 3D hand mesh reconstruction with an event camera and an RGB camera compensating for each other. By fusing two modalities of data across time, space, and information dimensions,EvRGBHand can tackle overexposure and motion blur issues in RGB-based HMR and foreground scarcity and background overflow issues in event-based HMR. We further propose EvRGBDegrader, which allows our model to generalize effectively in challenging scenes, even when trained solely on standard scenes, thus reducing data acquisition costs. Experiments on real-world data demonstrate that EvRGBHand can effectively solve the challenging issues when using either type of camera alone via retaining the merits of both, and shows the potential of generalization to outdoor scenes and another type of event camera."
https://arxiv.org/abs/2403.07339,2024-03-12,IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers,"['Zhanpeng Zeng', 'Karthikeyan Sankaralingam', 'Vikas Singh']","GEneral Matrix Multiply (GEMM) is a central operation in deep learning and corresponds to the largest chunk of the compute footprint. Therefore, improving its efficiency is an active topic of ongoing research. A popular strategy is the use of low bit-width integers to approximate the original entries in a matrix. This allows efficiency gains, but often requires sophisticated techniques to control the rounding error incurred. In this work, we first verify/check that when the low bit-width restriction is removed, for a variety of Transformer-based models, whether integers are sufficient for all GEMMs need -- for {\em both} training and inference stages, and can achieve parity with floating point counterparts. No sophisticated techniques are needed. We find that while a large majority of entries in matrices (encountered in such models) can be easily represented by {\em low} bit-width integers, the existence of a few heavy hitter entries make it difficult to achieve efficiency gains via the exclusive use of low bit-width GEMMs alone. To address this issue, we develop a simple algorithm, Integer Matrix Unpacking (IM-Unpack), to {\em unpack} a matrix with large integer entries into a larger matrix whose entries all lie within the representable range of arbitrarily low bit-width integers. This allows {\em equivalence} with the original GEMM, i.e., the exact result can be obtained using purely low bit-width integer GEMMs. This comes at the cost of additional operations -- we show that for many popular models, this overhead is quite small."
https://arxiv.org/abs/2403.07332,2024-03-12,Large Window-based Mamba UNet for Medical Image Segmentation: Beyond Convolution and Self-attention,"['Jinhong Wang', 'Jintai Chen', 'Danny Chen', 'Jian Wu']","In clinical practice, medical image segmentation provides useful information on the contours and dimensions of target organs or tissues, facilitating improved diagnosis, analysis, and treatment. In the past few years, convolutional neural networks (CNNs) and Transformers have dominated this area, but they still suffer from either limited receptive fields or costly long-range modeling. Mamba, a State Space Sequence Model (SSM), recently emerged as a promising paradigm for long-range dependency modeling with linear complexity. In this paper, we introduce a Large Window-based Mamba U}-shape Network, or LMa-UNet, for 2D and 3D medical image segmentation. A distinguishing feature of our LMa-UNet is its utilization of large windows, excelling in locally spatial modeling compared to small kernel-based CNNs and small window-based Transformers, while maintaining superior efficiency in global modeling compared to self-attention with quadratic complexity. Additionally, we design a novel hierarchical and bidirectional Mamba block to further enhance the global and neighborhood spatial modeling capability of Mamba. Comprehensive experiments demonstrate the effectiveness and efficiency of our method and the feasibility of using large window size to achieve large receptive fields. Codes are available at https://github.com/wjh892521292/LMa-UNet."
https://arxiv.org/abs/2403.07326,2024-03-12,SGE: Structured Light System Based on Gray Code with an Event Camera,"['Xingyu Lu', 'Lei Sun', 'Diyang Gu', 'Zhijie Xu', 'Kaiwei Wang']","Fast and accurate depth sensing has long been a significant research challenge. Event camera, as a device that quickly responds to intensity changes, provides a new solution for structured light (SL) systems. In this paper, we introduce Gray code into event-based SL systems for the first time. Our setup includes an event camera and Digital Light Processing (DLP) projector, enabling depth estimation through high-speed projection and decoding of Gray code patterns. By employing spatio-temporal encoding for point matching, our method is immune to timestamp noise, realizing high-speed depth estimation without loss of accuracy. The binary nature of events and Gray code minimizes data redundancy, enabling us to fully utilize sensor bandwidth at 100%. Experimental results show that our approach achieves accuracy comparable to state-of-the-art scanning methods while surpassing them in data acquisition speed (up to 41 times improvement) without sacrificing accuracy. Our proposed approach offers a highly promising solution for ultra-fast, real-time, and high-precision dense depth estimation. Code and dataset will be publicly available."
https://arxiv.org/abs/2403.07321,2024-03-12,GPT-generated Text Detection: Benchmark Dataset and Tensor-based Detection Method,"['Zubair Qazi', 'William Shiao', 'Evangelos E. Papalexakis']","As natural language models like ChatGPT become increasingly prevalent in applications and services, the need for robust and accurate methods to detect their output is of paramount importance. In this paper, we present GPT Reddit Dataset (GRiD), a novel Generative Pretrained Transformer (GPT)-generated text detection dataset designed to assess the performance of detection models in identifying generated responses from ChatGPT. The dataset consists of a diverse collection of context-prompt pairs based on Reddit, with human-generated and ChatGPT-generated responses. We provide an analysis of the dataset's characteristics, including linguistic diversity, context complexity, and response quality. To showcase the dataset's utility, we benchmark several detection methods on it, demonstrating their efficacy in distinguishing between human and ChatGPT-generated responses. This dataset serves as a resource for evaluating and advancing detection techniques in the context of ChatGPT and contributes to the ongoing efforts to ensure responsible and trustworthy AI-driven communication on the internet. Finally, we propose GpTen, a novel tensor-based GPT text detection method that is semi-supervised in nature since it only has access to human-generated text and performs on par with fully-supervised baselines."
https://arxiv.org/abs/2403.07304,2024-03-12,Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models,"['Yang Jiao', 'Shaoxiang Chen', 'Zequn Jie', 'Jingjing Chen', 'Lin Ma', 'Yu-Gang Jiang']","Large Multimodal Model (LMM) is a hot research topic in the computer vision area and has also demonstrated remarkable potential across multiple disciplinary fields. A recent trend is to further extend and enhance the perception capabilities of LMMs. The current methods follow the paradigm of adapting the visual task outputs to the format of the language model, which is the main component of a LMM. This adaptation leads to convenient development of such LMMs with minimal modifications, however, it overlooks the intrinsic characteristics of diverse visual tasks and hinders the learning of perception capabilities. To address this issue, we propose a novel LMM architecture named Lumen, a Large multimodal model with versatile vision-centric capability enhancement. We decouple the LMM's learning of perception capabilities into task-agnostic and task-specific stages. Lumen first promotes fine-grained vision-language concept alignment, which is the fundamental capability for various visual tasks. Thus the output of the task-agnostic stage is a shared representation for all the tasks we address in this paper. Then the task-specific decoding is carried out by flexibly routing the shared representation to lightweight task decoders with negligible training efforts. Benefiting from such a decoupled design, our Lumen surpasses existing LMM-based approaches on the COCO detection benchmark with a clear margin and exhibits seamless scalability to additional visual tasks. Furthermore, we also conduct comprehensive ablation studies and generalization evaluations for deeper insights. The code will be released at https://github.com/SxJyJay/Lumen."
https://arxiv.org/abs/2403.07300,2024-03-12,Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation,"['Peiyuan Liu', 'Hang Guo', 'Tao Dai', 'Naiqi Li', 'Jigang Bao', 'Xudong Ren', 'Yong Jiang', 'Shu-Tao Xia']","Multivariate time series forecasting has recently gained great success with the rapid growth of deep learning models. However, existing approaches usually train models from scratch using limited temporal data, preventing their generalization. Recently, with the surge of the Large Language Models (LLMs), several works have attempted to introduce LLMs into time series forecasting. Despite promising results, these methods directly take time series as the input to LLMs, ignoring the inherent modality gap between temporal and text data. In this work, we propose a novel Large Language Models and time series alignment framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time series forecasting challenge. Based on cross-modal knowledge distillation, the proposed method exploits both input-agnostic static knowledge and input-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers the forecasting model with favorable performance as well as strong generalization abilities. Extensive experiments demonstrate the proposed method establishes a new state of the art for both long- and short-term forecasting. Code is available at \url{https://github.com/Hank0626/LLaTA}."
https://arxiv.org/abs/2403.07299,2024-03-12,Modelling response time contrasts in superconducting nanowire single photon detectors,"['Souvik Haldar', 'Arun Sehrawat', 'Krishna B. Balasubramanian']","Superconducting Nanowire Single Photon Detector (SNSPD) emerges as a potential candidate in the multiple fields requiring sensitive and fast photodetection. While nanowires of low temperature superconducting detectors are mature with commercial solutions, other material options with higher transition temperature and faster responses are currently being explored. Towards this goal, we develop a generalized numerical model that incorporates the thermodynamic properties of the superconducting material and identifies the minimum resolvable photon count for a given bias and device parameters. A phase diagram of detection and latching phases with the minimum number of photons as a function of biasing current and biasing temperature for each material system is presented. We show using the developed model that while low temperature superconducting (LTS) nanowires are more sensitive to the incident photon at different wavelengths, the ultimate limit of a single photon can be achieved using high temperature superconducting (HTS) material such as YBa2Cu3O7-δ, albeit at stringent biasing conditions. On the contrary, ultrafast response time with three orders of magnitude smaller response times can be achieved in select HTS materials making it an appealing for several practical applications."
https://arxiv.org/abs/2403.07297,2024-03-11,Optical detection of bacterial cells on stainless-steel surface with a low-magnification light microscope,"['Yuzhen Zhang', 'Zili Gao', 'Lili He']","A Rapid and cost-effective method for detecting bacterial cells on surfaces is critical to protect public health from various aspects, including food safety, clinical hygiene, and pharmacy quality. Herein, we first established an optical detection method based on a gold chip coating with 3-mercaptophenylboronic acid (3-MPBA) to capture bacterial cells, which allows for the detection and quantification of bacterial cells with a standard light microscope under low-magnification (10 fold) objective lens. Then, integrating the developed optical detection method with swab sampling to achieve to detect bacterial cells loading on stainless-steel surfaces. Using Salmonella enterica (SE1045) and Escherichia coli as model bacterial cells, we achieved a capture efficiency of up to 76.0 % for SE1045 cells and 81.1 % for E. coli cells at Log 3 CFU/mL upon the optimized conditions. Our assay showed good linear relationship between the concentrations of bacterial cells with the cell counting in images with the limit of detection (LOD) of Log 3 CFU/mL for both SE1045 and E. coli cells. A further increase in sensitivity in detecting E. coli cells was achieved through a heat treatment, enabling the LOD to be pushed as low as Log 2 CFU/mL. Furthermore, successful application was observed in assessing bacterial contamination on stainless-steel surface following integrating with swab collection, achieving a recovery rate of approximately 70 % suggests future prospects for evaluating the cleanliness of surfaces. The entire process was completed within around 2 hours, with a cost of merely 2 dollars per sample. Given a standard light microscope cost around 250 dollars, our developed method has shown great potential in practical industrial applications for bacterial contamination control on surfaces in low-resource settings."
https://arxiv.org/abs/2403.07288,2024-03-11,Efficient and Model-Agnostic Parameter Estimation Under Privacy-Preserving Post-randomization Data,"['Qinglong Tian', 'Jiwei Zhao']","Protecting individual privacy is crucial when releasing sensitive data for public use. While data de-identification helps, it is not enough. This paper addresses parameter estimation in scenarios where data are perturbed using the Post-Randomization Method (PRAM) to enhance privacy. Existing methods for parameter estimation under PRAM data suffer from limitations like being parameter-specific, model-dependent, and lacking efficiency guarantees. We propose a novel, efficient method that overcomes these limitations. Our method is applicable to general parameters defined through estimating equations and makes no assumptions about the underlying data model. We further prove that the proposed estimator achieves the semiparametric efficiency bound, making it optimal in terms of asymptotic variance."
https://arxiv.org/abs/2403.07286,2024-03-11,MENTOR: Multilingual tExt detectioN TOward leaRning by analogy,"['Hsin-Ju Lin', 'Tsu-Chun Chung', 'Ching-Chun Hsiao', 'Pin-Yu Chen', 'Wei-Chen Chiu', 'Ching-Chun Huang']","Text detection is frequently used in vision-based mobile robots when they need to interpret texts in their surroundings to perform a given task. For instance, delivery robots in multilingual cities need to be capable of doing multilingual text detection so that the robots can read traffic signs and road markings. Moreover, the target languages change from region to region, implying the need of efficiently re-training the models to recognize the novel/new languages. However, collecting and labeling training data for novel languages are cumbersome, and the efforts to re-train an existing/trained text detector are considerable. Even worse, such a routine would repeat whenever a novel language appears. This motivates us to propose a new problem setting for tackling the aforementioned challenges in a more efficient way: ""We ask for a generalizable multilingual text detection framework to detect and identify both seen and unseen language regions inside scene images without the requirement of collecting supervised training data for unseen languages as well as model re-training"". To this end, we propose ""MENTOR"", the first work to realize a learning strategy between zero-shot learning and few-shot learning for multilingual scene text detection."
https://arxiv.org/abs/2403.07269,2024-03-11,MPS: A New Method for Selecting the Stable Closed-Loop Equilibrium Attitude-Error Quaternion of a UAV During Flight,"['Francisco M. F. R. Gonçalves', 'Ryan M. Bena', 'Konstantin I. Matveev', 'Néstor O. Pérez-Arancibia']","We present model predictive selection (MPS), a new method for selecting the stable closed-loop (CL) equilibrium attitude-error quaternion (AEQ) of an uncrewed aerial vehicle (UAV) during the execution of high-speed yaw maneuvers. In this approach, we minimize the cost of yawing measured with a performance figure of merit (PFM) that takes into account both the aerodynamic-torque control input and attitude-error state of the UAV. Specifically, this method uses a control law with a term whose sign is dynamically switched in real time to select, between two options, the torque associated with the lesser cost of rotation as predicted by a dynamical model of the UAV derived from first principles. This problem is relevant because the selection of the stable CL equilibrium AEQ significantly impacts the performance of a UAV during high-speed rotational flight, from both the power and control-error perspectives. To test and demonstrate the functionality and performance of the proposed method, we present data collected during one hundred real-time high-speed yaw-tracking flight experiments. These results highlight the superior capabilities of the proposed MPS-based scheme when compared to a benchmark controller commonly used in aerial robotics, as the PFM used to quantify the cost of flight is reduced by 60.30 %, on average. To our best knowledge, these are the first flight-test results that thoroughly demonstrate, evaluate, and compare the performance of a real-time controller capable of selecting the stable CL equilibrium AEQ during operation."
https://arxiv.org/abs/2403.07255,2024-03-11,Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free NOMA in Machine-Type Communication,"['Yongjeong Oh', 'Jaehong Jo', 'Byonghyo Shim', 'Yo-Seb Jeon']","In this paper, we present a novel approach for joint activity detection (AD), channel estimation (CE), and data detection (DD) in uplink grant-free non-orthogonal multiple access (NOMA) systems. Our approach employs an iterative and parallel interference removal strategy inspired by parallel interference cancellation (PIC), enhanced with deep learning to jointly tackle the AD, CE, and DD problems. Based on this approach, we develop three PIC frameworks, each of which is designed for either coherent or non-coherence schemes. The first framework performs joint AD and CE using received pilot signals in the coherent scheme. Building upon this framework, the second framework utilizes both the received pilot and data signals for CE, further enhancing the performances of AD, CE, and DD in the coherent scheme. The third framework is designed to accommodate the non-coherent scheme involving a small number of data bits, which simultaneously performs AD and DD. Through joint loss functions and interference cancellation modules, our approach supports end-to-end training, contributing to enhanced performances of AD, CE, and DD for both coherent and non-coherent schemes. Simulation results demonstrate the superiority of our approach over traditional techniques, exhibiting enhanced performances of AD, CE, and DD while maintaining lower computational complexity."
https://arxiv.org/abs/2403.07247,2024-03-11,GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical structure Generation,"['Linrui Dai', 'Rongzhao Zhang', 'Zhongzhen Huang', 'Xiaofan Zhang']","The annotation burden and extensive labor for gathering a large medical dataset with images and corresponding labels are rarely cost-effective and highly intimidating. This results in a lack of abundant training data that undermines downstream tasks and partially contributes to the challenge image analysis faces in the medical field. As a workaround, given the recent success of generative neural models, it is now possible to synthesize image datasets at a high fidelity guided by external constraints. This paper explores this possibility and presents \textbf{GuideGen}: a pipeline that jointly generates CT images and tissue masks for abdominal organs and colorectal cancer conditioned on a text prompt. Firstly, we introduce Volumetric Mask Sampler to fit the discrete distribution of mask labels and generate low-resolution 3D tissue masks. Secondly, our Conditional Image Generator autoregressively generates CT slices conditioned on a corresponding mask slice to incorporate both style information and anatomical guidance. This pipeline guarantees high fidelity and variability as well as exact alignment between generated CT volumes and tissue masks. Both qualitative and quantitative experiments on 3D abdominal CTs demonstrate a high performance of our proposed pipeline, thereby proving our method can serve as a dataset generator and provide potential benefits to downstream tasks. It is hoped that our work will offer a promising solution on the multimodality generation of CT and its anatomical mask. Our source code is publicly available at https://github.com/OvO1111/JointImageGeneration."
https://arxiv.org/abs/2403.07226,2024-03-11,The order-theoretical foundation for data flow security,['Luigi Logrippo'],"Some theories on data flow security are based on order-theoretical concepts, most commonly on lattice concepts. This paper presents a correspondence between security concepts and partial order concepts, by which the former become an application of the latter. The formalization involves concepts of data flow, equivalence classes of entities that can access the same data, and labels. Efficient, well-known algorithms to obtain one of these from one of the others are presented. Security concepts such as secrecy (also called confidentiality), integrity and conflict can be expressed in this theory. Further, it is shown that complex tuple labels used in the literature to express security levels can be translated into equivalent set labels. A consequence is that any network's data flow or access control relationships can be defined by assigning simple set labels to the entities. Finally, it is shown how several partial orders can be combined when different data flows must coexist."
https://arxiv.org/abs/2403.07221,2024-03-11,LookupFFN: Making Transformers Compute-lite for CPU inference,"['Zhanpeng Zeng', 'Michael Davies', 'Pranav Pulijala', 'Karthikeyan Sankaralingam', 'Vikas Singh']","While GPU clusters are the de facto choice for training large deep neural network (DNN) models today, several reasons including ease of workflow, security and cost have led to efforts investigating whether CPUs may be viable for inference in routine use in many sectors of the industry. But the imbalance between the compute capabilities of GPUs and CPUs is huge. Motivated by these considerations, we study a module which is a workhorse within modern DNN architectures, GEMM based Feed Forward Networks (FFNs), and assess the extent to which it can be made compute- (or FLOP-) lite. Specifically, we propose an alternative formulation (we call it LookupFFN) to GEMM based FFNs inspired by the recent studies of using Locality Sensitive Hashing (LSH) to approximate FFNs. Our formulation recasts most essential operations as a memory look-up, leveraging the trade-off between the two resources on any platform: compute and memory (since CPUs offer it in abundance). For RoBERTa language model pretraining, our formulation achieves similar performance compared to GEMM based FFNs, while dramatically reducing the required FLOP. Our development is complemented with a detailed hardware profiling of strategies that will maximize efficiency -- not just on contemporary hardware but on products that will be offered in the near/medium term future. Code is avaiable at \url{https://github.com/mlpen/LookupFFN}."
https://arxiv.org/abs/2403.07187,2024-03-11,UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation,"['Junhong Shen', 'Tanya Marwah', 'Ameet Talwalkar']","We introduce UPS (Unified PDE Solver), an effective and data-efficient approach to solve diverse spatiotemporal PDEs defined over various domains, dimensions, and resolutions. UPS unifies different PDEs into a consistent representation space and processes diverse collections of PDE data using a unified network architecture that combines LLMs with domain-specific neural operators. We train the network via a two-stage cross-modal adaptation process, leveraging ideas of modality alignment and multi-task learning. By adapting from pretrained LLMs and exploiting text-form meta information, we are able to use considerably fewer training samples than previous methods while obtaining strong empirical results. UPS outperforms existing baselines, often by a large margin, on a wide range of 1D and 2D datasets in PDEBench, achieving state-of-the-art results on 8 of 10 tasks considered. Meanwhile, it is capable of few-shot transfer to different PDE families, coefficients, and resolutions."
https://arxiv.org/abs/2403.07179,2024-03-11,3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs,"['Huaisheng Zhu', 'Teng Xiao', 'Vasant G Honavar']","Generating molecules with desired properties is a critical task with broad applications in drug discovery and materials design. Inspired by recent advances in large language models, there is a growing interest in using natural language descriptions of molecules to generate molecules with the desired properties. Most existing methods focus on generating molecules that precisely match the text description. However, practical applications call for methods that generate diverse, and ideally novel, molecules with the desired properties. We propose 3M-Diffusion, a novel multi-modal molecular graph generation method, to address this challenge. 3M-Diffusion first encodes molecular graphs into a graph latent space aligned with text descriptions. It then reconstructs the molecular structure and atomic attributes based on the given text descriptions using the molecule decoder. It then learns a probabilistic mapping from the text space to the latent molecular graph space using a diffusion model. The results of our extensive experiments on several datasets demonstrate that 3M-Diffusion can generate high-quality, novel and diverse molecular graphs that semantically match the textual description provided."
https://arxiv.org/abs/2403.07173,2024-03-11,Mixed virtual element approximation for the five-field formulation of the steady Boussinesq problem with temperature-dependent parameters,['Zeinab Gharibi'],"In this work, we develop recent research on the fully mixed virtual element method (mixed-VEM) based on the Banach space for the stationary Boussinesq equation to suggest and analyze a new mixed-VEM for the stationary two-dimensional Boussinesq equation with temperature-dependent parameters in terms of the pseudostress, vorticity, velocity, pseudoheat vector and temperature fields. The well-posedness of the continuous formulation is analyzed utilizing a fixed-point strategy, a smallness assumption on the data, and some additional regularities on the solution. The discretization for the mentioned variables is based on the coupling $\mathbb{H}(\mathbf{div}_{6/5})$ -- and $\mathbf{H}(\mathrm{div}_{6/5})$ -- conforming virtual element techniques. The proposed scheme is rewritten as an equivalent fixed point operator equation, so that its existence and stability estimates have been proven. In addition, an a priori convergence analysis is established by utilizing the Céa estimate and a suitable assumption on data for all variables in their natural norms showing an optimal rate of convergence. Finally, several numerical examples are presented to illustrate the performance of the proposed method."
https://arxiv.org/abs/2403.07151,2024-03-11,Don't Forget What I did?: Assessing Client Contributions in Federated Learning,"['Bishwamittra Ghosh', 'Debabrota Basu', 'Fu Huazhu', 'Wang Yuan', 'Renuga Kanagavelu', 'Jiang Jin Peng', 'Liu Yong', 'Goh Siow Mong Rick', 'Wei Qingsong']","Federated Learning (FL) is a collaborative machine learning (ML) approach, where multiple clients participate in training an ML model without exposing the private data. Fair and accurate assessment of client contributions is an important problem in FL to facilitate incentive allocation and encouraging diverse clients to participate in a unified model training. Existing methods for assessing client contribution adopts co-operative game-theoretic concepts, such as Shapley values, but under simplified assumptions. In this paper, we propose a history-aware game-theoretic framework, called FLContrib, to assess client contributions when a subset of (potentially non-i.i.d.) clients participate in each epoch of FL training. By exploiting the FL training process and linearity of Shapley value, we develop FLContrib that yields a historical timeline of client contributions as FL training progresses over epochs. Additionally, to assess client contribution under limited computational budget, we propose a scheduling procedure that considers a two-sided fairness criteria to perform expensive Shapley value computation only in a subset of training epochs. In experiments, we demonstrate a controlled trade-off between the correctness and efficiency of client contributions assessed via FLContrib. To demonstrate the benefits of history-aware client contributions, we apply FLContrib to detect dishonest clients conducting data poisoning in FL training."
https://arxiv.org/abs/2403.07149,2024-03-11,Advancing Hyperspectral Targeted Alpha Therapy with Adversarial Machine Learning,"['Jim Zhao', 'Greg Leadman']","Targeted Alpha Therapy (TAT) has emerged as a promising modality for the treatment of various malignancies, leveraging the high linear energy transfer (LET) and short range of alpha particles to selectively irradiate cancer cells while sparing healthy tissue. Monitoring and optimizing TAT delivery is crucial for its clinical success. Hyper-spectral Single Photon Imaging (HSPI) presents a novel and versatile approach for the real-time assessment of TAT in vivo. This study introduces a comprehensive framework for HSPI in TAT, encompassing spectral unmixing, quantitative dosimetry, and spatiotemporal visualization. We report the development of a dedicated HSPI system tailored to alpha-emitting radionuclides, enabling the simultaneous acquisition of high-resolution spectral data and single-photon localization. Utilizing advanced spectral unmixing algorithms, we demonstrate the discrimination of alpha-induced scintillation from background fluorescence, facilitating precise alpha particle tracking with adversarial machine learning."
https://arxiv.org/abs/2403.07148,2024-03-11,Stochastic Extragradient with Random Reshuffling: Improved Convergence for Variational Inequalities,"['Konstantinos Emmanouilidis', 'René Vidal', 'Nicolas Loizou']","The Stochastic Extragradient (SEG) method is one of the most popular algorithms for solving finite-sum min-max optimization and variational inequality problems (VIPs) appearing in various machine learning tasks. However, existing convergence analyses of SEG focus on its with-replacement variants, while practical implementations of the method randomly reshuffle components and sequentially use them. Unlike the well-studied with-replacement variants, SEG with Random Reshuffling (SEG-RR) lacks established theoretical guarantees. In this work, we provide a convergence analysis of SEG-RR for three classes of VIPs: (i) strongly monotone, (ii) affine, and (iii) monotone. We derive conditions under which SEG-RR achieves a faster convergence rate than the uniform with-replacement sampling SEG. In the monotone setting, our analysis of SEG-RR guarantees convergence to an arbitrary accuracy without large batch sizes, a strong requirement needed in the classical with-replacement SEG. As a byproduct of our results, we provide convergence guarantees for Shuffle Once SEG (shuffles the data only at the beginning of the algorithm) and the Incremental Extragradient (does not shuffle the data). We supplement our analysis with experiments validating empirically the superior performance of SEG-RR over the classical with-replacement sampling SEG."
https://arxiv.org/abs/2403.07142,2024-03-11,One Category One Prompt: Dataset Distillation using Diffusion Models,"['Ali Abbasi', 'Ashkan Shahbazi', 'Hamed Pirsiavash', 'Soheil Kolouri']","The extensive amounts of data required for training deep neural networks pose significant challenges on storage and transmission fronts. Dataset distillation has emerged as a promising technique to condense the information of massive datasets into a much smaller yet representative set of synthetic samples. However, traditional dataset distillation approaches often struggle to scale effectively with high-resolution images and more complex architectures due to the limitations in bi-level optimization. Recently, several works have proposed exploiting knowledge distillation with decoupled optimization schemes to scale up dataset distillation. Although these methods effectively address the scalability issue, they rely on extensive image augmentations requiring the storage of soft labels for augmented images. In this paper, we introduce Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for dataset distillation, leveraging recent advancements in generative text-to-image foundation models. Our approach utilizes textual inversion, a technique for fine-tuning text-to-image generative models, to create concise and informative representations for large datasets. By employing these learned text prompts, we can efficiently store and infer new samples for introducing data variability within a fixed memory budget. We show the effectiveness of our method through extensive experiments across various computer vision benchmark datasets with different memory budgets."
https://arxiv.org/abs/2403.07129,2024-03-11,RaceMOP: Mapless Online Path Planning for Multi-Agent Autonomous Racing using Residual Policy Learning,"['Raphael Trumpp', 'Ehsan Javanmardi', 'Jin Nakazato', 'Manabu Tsukada', 'Marco Caccamo']","The interactive decision-making in multi-agent autonomous racing offers insights valuable beyond the domain of self-driving cars. Mapless online path planning is particularly of practical appeal but poses a challenge for safely overtaking opponents due to the limited planning horizon. Accordingly, this paper introduces RaceMOP, a novel method for mapless online path planning designed for multi-agent racing of F1TENTH cars. Unlike classical planners that depend on predefined racing lines, RaceMOP operates without a map, relying solely on local observations to overtake other race cars at high speed. Our approach combines an artificial potential field method as a base policy with residual policy learning to introduce long-horizon planning capabilities. We advance the field by introducing a novel approach for policy fusion with the residual policy directly in probability space. Our experiments for twelve simulated racetracks validate that RaceMOP is capable of long-horizon decision-making with robust collision avoidance during overtaking maneuvers. RaceMOP demonstrates superior handling over existing mapless planners while generalizing to unknown racetracks, paving the way for further use of our method in robotics. We make the open-source code for RaceMOP available at http://github.com/raphajaner/racemop."
https://arxiv.org/abs/2403.07126,2024-03-11,Heterogeneous Image-based Classification Using Distributional Data Analysis,"['Alec Reinhardt', 'Newsha Nikzad', 'Raven J. Hollis', 'Galia Jacobson', 'Millicent A. Roach', 'Mohamed Badawy', 'Peter Chul Park', 'Laura Beretta', 'Prasun K Jalal', 'David T. Fuentes', 'Eugene J. Koay', 'Suprateek Kundu']","Diagnostic imaging has gained prominence as potential biomarkers for early detection and diagnosis in a diverse array of disorders including cancer. However, existing methods routinely face challenges arising from various factors such as image heterogeneity. We develop a novel imaging-based distributional data analysis (DDA) approach that incorporates the probability (quantile) distribution of the pixel-level features as covariates. The proposed approach uses a smoothed quantile distribution (via a suitable basis representation) as functional predictors in a scalar-on-functional quantile regression model. Some distinctive features of the proposed approach include the ability to: (i) account for heterogeneity within the image; (ii) incorporate granular information spanning the entire distribution; and (iii) tackle variability in image sizes for unregistered images in cancer applications. Our primary goal is risk prediction in Hepatocellular carcinoma that is achieved via predicting the change in tumor grades at post-diagnostic visits using pre-diagnostic enhancement pattern mapping (EPM) images of the liver. Along the way, the proposed DDA approach is also used for case versus control diagnosis and risk stratification objectives. Our analysis reveals that when coupled with global structural radiomics features derived from the corresponding T1-MRI scans, the proposed smoothed quantile distributions derived from EPM images showed considerable improvements in sensitivity and comparable specificity in contrast to classification based on routinely used summary measures that do not account for image heterogeneity. Given that there are limited predictive modeling approaches based on heterogeneous images in cancer, the proposed method is expected to provide considerable advantages in image-based early detection and risk prediction."
https://arxiv.org/abs/2403.07124,2024-03-11,Stochastic gradient descent-based inference for dynamic network models with attractors,"['Hancong Pan', 'Xiaojing Zhu', 'Cantay Caliskan', 'Dino P. Christenson', 'Konstantinos Spiliopoulos', 'Dylan Walker', 'Eric D. Kolaczyk']","In Coevolving Latent Space Networks with Attractors (CLSNA) models, nodes in a latent space represent social actors, and edges indicate their dynamic interactions. Attractors are added at the latent level to capture the notion of attractive and repulsive forces between nodes, borrowing from dynamical systems theory. However, CLSNA reliance on MCMC estimation makes scaling difficult, and the requirement for nodes to be present throughout the study period limit practical applications. We address these issues by (i) introducing a Stochastic gradient descent (SGD) parameter estimation method, (ii) developing a novel approach for uncertainty quantification using SGD, and (iii) extending the model to allow nodes to join and leave over time. Simulation results show that our extensions result in little loss of accuracy compared to MCMC, but can scale to much larger networks. We apply our approach to the longitudinal social networks of members of US Congress on the social media platform X. Accounting for node dynamics overcomes selection bias in the network and uncovers uniquely and increasingly repulsive forces within the Republican Party."
https://arxiv.org/abs/2403.07118,2024-03-11,Narrating Causal Graphs with Large Language Models,"['Atharva Phatak', 'Vijay K. Mago', 'Ameeta Agrawal', 'Aravind Inbasekaran', 'Philippe J. Giabbanelli']","The use of generative AI to create text descriptions from graphs has mostly focused on knowledge graphs, which connect concepts using facts. In this work we explore the capability of large pretrained language models to generate text from causal graphs, where salient concepts are represented as nodes and causality is represented via directed, typed edges. The causal reasoning encoded in these graphs can support applications as diverse as healthcare or marketing. Using two publicly available causal graph datasets, we empirically investigate the performance of four GPT-3 models under various settings. Our results indicate that while causal text descriptions improve with training data, compared to fact-based graphs, they are harder to generate under zero-shot settings. Results further suggest that users of generative AI can deploy future applications faster since similar performances are obtained when training a model with only a few examples as compared to fine-tuning via a large curated dataset."
https://arxiv.org/abs/2403.07100,2024-03-11,Equivariant Variational Quantum Eigensolver to detect Phase Transitions through Energy Level Crossings,"['Giulio Crognaletti', 'Giovanni Di Bartolomeo', 'Michele Vischi', 'Luciano Loris Viteritti']","Level spectroscopy stands as a powerful method for identifying the transition point that delineates distinct quantum phases. Since each quantum phase exhibits a characteristic sequence of excited states, the crossing of energy levels between low-lying excited states offers a reliable mean to estimate the phase transition point. While approaches like the Variational Quantum Eigensolver are useful for approximating ground states of interacting systems using quantum computing, capturing low-energy excitations remains challenging. In our study, we introduce an equivariant quantum circuit that preserves the total spin and the translational symmetry to accurately describe singlet and triplet excited states in the $J_1$-$J_2$ Heisenberg model on a chain, which are crucial for characterizing its transition point. Additionally, we assess the impact of noise on the variational state, showing that conventional mitigation techniques like Zero Noise Extrapolation reliably restore its physical properties."
https://arxiv.org/abs/2403.07095,2024-03-11,Overcoming the Paradox of Certified Training with Gaussian Smoothing,"['Stefan Balauca', 'Mark Niklas Müller', 'Yuhao Mao', 'Maximilian Baader', 'Marc Fischer', 'Martin Vechev']","Training neural networks with high certified accuracy against adversarial examples remains an open problem despite significant efforts. While certification methods can effectively leverage tight convex relaxations for bound computation, in training, these methods perform worse than looser relaxations. Prior work hypothesized that this is caused by the discontinuity and perturbation sensitivity of the loss surface induced by these tighter relaxations. In this work, we show theoretically that Gaussian Loss Smoothing can alleviate both of these issues. We confirm this empirically by proposing a certified training method combining PGPE, an algorithm computing gradients of a smoothed loss, with different convex relaxations. When using this training method, we observe that tighter bounds indeed lead to strictly better networks that can outperform state-of-the-art methods on the same network. While scaling PGPE-based training remains challenging due to high computational cost, our results clearly demonstrate the promise of Gaussian Loss Smoothing for training certifiably robust neural networks."
https://arxiv.org/abs/2403.07094,2024-03-11,FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning,"['Xiang Meng', 'Wenyu Chen', 'Riade Benbaki', 'Rahul Mazumder']","The increasing computational demands of modern neural networks present deployment challenges on resource-constrained devices. Network pruning offers a solution to reduce model size and computational cost while maintaining performance. However, most current pruning methods focus primarily on improving sparsity by reducing the number of nonzero parameters, often neglecting other deployment costs such as inference time, which are closely related to the number of floating-point operations (FLOPs). In this paper, we propose FALCON, a novel combinatorial-optimization-based framework for network pruning that jointly takes into account model accuracy (fidelity), FLOPs, and sparsity constraints. A main building block of our approach is an integer linear program (ILP) that simultaneously handles FLOP and sparsity constraints. We present a novel algorithm to approximately solve the ILP. We propose a novel first-order method for our optimization framework which makes use of our ILP solver. Using problem structure (e.g., the low-rank structure of approx. Hessian), we can address instances with millions of parameters. Our experiments demonstrate that FALCON achieves superior accuracy compared to other pruning approaches within a fixed FLOP budget. For instance, for ResNet50 with 20% of the total FLOPs retained, our approach improves the accuracy by 48% relative to state-of-the-art. Furthermore, in gradual pruning settings with re-training between pruning steps, our framework outperforms existing pruning methods, emphasizing the significance of incorporating both FLOP and sparsity constraints for effective network pruning."
https://arxiv.org/abs/2403.07092,2024-03-11,A cascaded deep network for automated tumor detection and segmentation in clinical PET imaging of diffuse large B-cell lymphoma,"['Shadab Ahamed', 'Natalia Dubljevic', 'Ingrid Bloise', 'Claire Gowdy', 'Patrick Martineau', 'Don Wilson', 'Carlos F. Uribe', 'Arman Rahmim', 'Fereshteh Yousefirizi']","Accurate detection and segmentation of diffuse large B-cell lymphoma (DLBCL) from PET images has important implications for estimation of total metabolic tumor volume, radiomics analysis, surgical intervention and radiotherapy. Manual segmentation of tumors in whole-body PET images is time-consuming, labor-intensive and operator-dependent. In this work, we develop and validate a fast and efficient three-step cascaded deep learning model for automated detection and segmentation of DLBCL tumors from PET images. As compared to a single end-to-end network for segmentation of tumors in whole-body PET images, our three-step model is more effective (improves 3D Dice score from 58.9% to 78.1%) since each of its specialized modules, namely the slice classifier, the tumor detector and the tumor segmentor, can be trained independently to a high degree of skill to carry out a specific task, rather than a single network with suboptimal performance on overall segmentation."
https://arxiv.org/abs/2403.07077,2024-03-11,"Porous aluminium decorated with rhodium nanoparticles, preparation and use as platform for UV plasmonics","['Shrobona Banerje', 'Luca Mattarozzi', 'Nicolo Maccaferri', 'Sandro Cattarin', 'Shukun Weng', 'Ali Douaki', 'German Lanzavecchia', 'Anastasiia Sapunova', 'Francesco DAmico', 'Qifei Ma', 'Yanqui Zou', 'Roman Krahne', 'Janina Kneipp', 'Denis Garoli']","There is a high current interest for novel plasmonic platforms and materials able to extend their applicability into the ultraviolet (UV) region of the electromagnetic spectrum. In the UV it is possible to explore spectral properties of biomolecules with small cross section in the visible spectral range. However, most used metals in plasmonics have their resonances at wavelengths > 350 nm. Aluminum and rhodium are two interesting candidate materials for UV plasmonics, and in this work we developed a simple and low-cost preparation of functional substrates based on nanoporous aluminum decorated with rhodium nanoparticles. We demonstrate that these functionalized nanoporous metal films can be exploited as plasmonic materials suitable for enhanced UV Raman spectroscopy"
https://arxiv.org/abs/2403.07072,2024-03-11,Explainable Learning with Gaussian Processes,"['Kurt Butler', 'Guanchao Feng', 'Petar M. Djuric']","The field of explainable artificial intelligence (XAI) attempts to develop methods that provide insight into how complicated machine learning methods make predictions. Many methods of explanation have focused on the concept of feature attribution, a decomposition of the model's prediction into individual contributions corresponding to each input feature. In this work, we explore the problem of feature attribution in the context of Gaussian process regression (GPR). We take a principled approach to defining attributions under model uncertainty, extending the existing literature. We show that although GPR is a highly flexible and non-parametric approach, we can derive interpretable, closed-form expressions for the feature attributions. When using integrated gradients as an attribution method, we show that the attributions of a GPR model also follow a Gaussian process distribution, which quantifies the uncertainty in attribution arising from uncertainty in the model. We demonstrate, both through theory and experimentation, the versatility and robustness of this approach. We also show that, when applicable, the exact expressions for GPR attributions are both more accurate and less computationally expensive than the approximations currently used in practice. The source code for this project is freely available under MIT license at https://github.com/KurtButler/2024_attributions_paper."
https://arxiv.org/abs/2403.07071,2024-03-11,LISO: Lidar-only Self-Supervised 3D Object Detection,"['Stefan Baur', 'Frank Moosmann', 'Andreas Geiger']","3D object detection is one of the most important components in any Self-Driving stack, but current state-of-the-art (SOTA) lidar object detectors require costly & slow manual annotation of 3D bounding boxes to perform well. Recently, several methods emerged to generate pseudo ground truth without human supervision, however, all of these methods have various drawbacks: Some methods require sensor rigs with full camera coverage and accurate calibration, partly supplemented by an auxiliary optical flow engine. Others require expensive high-precision localization to find objects that disappeared over multiple drives. We introduce a novel self-supervised method to train SOTA lidar object detection networks which works on unlabeled sequences of lidar point clouds only, which we call trajectory-regularized self-training. It utilizes a SOTA self-supervised lidar scene flow network under the hood to generate, track, and iteratively refine pseudo ground truth. We demonstrate the effectiveness of our approach for multiple SOTA object detection networks across multiple real-world datasets. Code will be released."
https://arxiv.org/abs/2403.07066,2024-03-11,Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation Models,"['Philip Harris', 'Michael Kagan', 'Jeffrey Krupa', 'Benedikt Maier', 'Nathaniel Woodward']","Self-Supervised Learning (SSL) is at the core of training modern large machine learning models, providing a scheme for learning powerful representations that can be used in a variety of downstream tasks. However, SSL strategies must be adapted to the type of training data and downstream tasks required. We propose RS3L, a novel simulation-based SSL strategy that employs a method of re-simulation to drive data augmentation for contrastive learning. By intervening in the middle of the simulation process and re-running simulation components downstream of the intervention, we generate multiple realizations of an event, thus producing a set of augmentations covering all physics-driven variations available in the simulator. Using experiments from high-energy physics, we explore how this strategy may enable the development of a foundation model; we show how R3SL pre-training enables powerful performance in downstream tasks such as discrimination of a variety of objects and uncertainty mitigation. In addition to our results, we make the RS3L dataset publicly available for further studies on how to improve SSL strategies."
https://arxiv.org/abs/2403.07061,2024-03-13,Simulating Meson Scattering on Spin Quantum Simulators,"['Elizabeth R. Bennewitz', 'Brayden Ware', 'Alexander Schuckert', 'Alessio Lerose', 'Federica M. Surace', 'Ron Belyansky', 'William Morong', 'De Luo', 'Arinjoy De', 'Kate S. Collins', 'Or Katz', 'Christopher Monroe', 'Zohreh Davoudi', 'Alexey V. Gorshkov']","Studying high-energy collisions of composite particles, such as hadrons and nuclei, is an outstanding goal for quantum simulators. However, preparation of hadronic wave packets has posed a significant challenge, due to the complexity of hadrons and the precise structure of wave packets. This has limited demonstrations of hadron scattering on quantum simulators to date. Observations of confinement and composite excitations in quantum spin systems have opened up the possibility to explore scattering dynamics in spin models. In this article, we develop two methods to create entangled spin states corresponding to wave packets of composite particles in analog quantum simulators of Ising spin Hamiltonians. One wave-packet preparation method uses the blockade effect enabled by beyond-nearest-neighbor Ising spin interactions. The other method utilizes a quantum-bus-mediated exchange, such as the native spin-phonon coupling in trapped-ion arrays. With a focus on trapped-ion simulators, we numerically benchmark both methods and show that high-fidelity wave packets can be achieved in near-term experiments. We numerically study scattering of wave packets for experimentally realizable parameters in the Ising model and find inelastic-scattering regimes, corresponding to particle production in the scattering event, with prominent and distinct experimental signals. Our proposal, therefore, demonstrates the potential of observing inelastic scattering in near-term quantum simulators."
https://arxiv.org/abs/2403.07059,2024-03-11,Better than classical? The subtle art of benchmarking quantum machine learning models,"['Joseph Bowles', 'Shahnawaz Ahmed', 'Maria Schuld']","Benchmarking models via classical simulations is one of the main ways to judge ideas in quantum machine learning before noise-free hardware is available. However, the huge impact of the experimental design on the results, the small scales within reach today, as well as narratives influenced by the commercialisation of quantum technologies make it difficult to gain robust insights. To facilitate better decision-making we develop an open-source package based on the PennyLane software framework and use it to conduct a large-scale study that systematically tests 12 popular quantum machine learning models on 6 binary classification tasks used to create 160 individual datasets. We find that overall, out-of-the-box classical machine learning models outperform the quantum classifiers. Moreover, removing entanglement from a quantum model often results in as good or better performance, suggesting that ""quantumness"" may not be the crucial ingredient for the small learning tasks considered here. Our benchmarks also unlock investigations beyond simplistic leaderboard comparisons, and we identify five important questions for quantum model design that follow from our results."
https://arxiv.org/abs/2403.07033,2024-03-11,Interpreting What Typical Fault Signals Look Like via Prototype-matching,"['Qian Chen', 'Xingjian Dong', 'Zhike Peng']","Neural networks, with powerful nonlinear mapping and classification capabilities, are widely applied in mechanical fault diagnosis to ensure safety. However, being typical black-box models, their application is limited in high-reliability-required scenarios. To understand the classification logic and explain what typical fault signals look like, the prototype matching network (PMN) is proposed by combining the human-inherent prototype-matching with autoencoder (AE). The PMN matches AE-extracted feature with each prototype and selects the most similar prototype as the prediction result. It has three interpreting paths on classification logic, fault prototypes, and matching contributions. Conventional diagnosis and domain generalization experiments demonstrate its competitive diagnostic performance and distinguished advantages in representation learning. Besides, the learned typical fault signals (i.e., sample-level prototypes) showcase the ability for denoising and extracting subtle key features that experts find challenging to capture. This ability broadens human understanding and provides a promising solution from interpretability research to AI-for-Science."
https://arxiv.org/abs/2403.07031,2024-03-11,The Cram Method for Efficient Simultaneous Learning and Evaluation,"['Zeyang Jia', 'Kosuke Imai', 'Michael Lingzhi Li']","We introduce the ""cram"" method, a general and efficient approach to simultaneous learning and evaluation using a generic machine learning (ML) algorithm. In a single pass of batched data, the proposed method repeatedly trains an ML algorithm and tests its empirical performance. Because it utilizes the entire sample for both learning and evaluation, cramming is significantly more data-efficient than sample-splitting. The cram method also naturally accommodates online learning algorithms, making its implementation computationally efficient. To demonstrate the power of the cram method, we consider the standard policy learning setting where cramming is applied to the same data to both develop an individualized treatment rule (ITR) and estimate the average outcome that would result if the learned ITR were to be deployed. We show that under a minimal set of assumptions, the resulting crammed evaluation estimator is consistent and asymptotically normal. While our asymptotic results require a relatively weak stabilization condition of ML algorithm, we develop a simple, generic method that can be used with any policy learning algorithm to satisfy this condition. Our extensive simulation studies show that, when compared to sample-splitting, cramming reduces the evaluation standard error by more than 40% while improving the performance of learned policy. We also apply the cram method to a randomized clinical trial to demonstrate its applicability to real-world problems. Finally, we briefly discuss future extensions of the cram method to other learning and evaluation settings."
https://arxiv.org/abs/2403.07022,2024-03-09,A Unified Model for Spatio-Temporal Prediction Queries with Arbitrary Modifiable Areal Units,"['Liyue Chen', 'Jiangyi Fang', 'Tengfei Liu', 'Shaosheng Cao', 'Leye Wang']","Spatio-Temporal (ST) prediction is crucial for making informed decisions in urban location-based applications like ride-sharing. However, existing ST models often require region partition as a prerequisite, resulting in two main pitfalls. Firstly, location-based services necessitate ad-hoc regions for various purposes, requiring multiple ST models with varying scales and zones, which can be costly to support. Secondly, different ST models may produce conflicting outputs, resulting in confusing predictions. In this paper, we propose One4All-ST, a framework that can conduct ST prediction for arbitrary modifiable areal units using only one model. To reduce the cost of getting multi-scale predictions, we design an ST network with hierarchical spatial modeling and scale normalization modules to efficiently and equally learn multi-scale representations. To address prediction inconsistencies across scales, we propose a dynamic programming scheme to solve the formulated optimal combination problem, minimizing predicted error through theoretical analysis. Besides, we suggest using an extended quad-tree to index the optimal combinations for quick response to arbitrary modifiable areal units in practical online scenarios. Extensive experiments on two real-world datasets verify the efficiency and effectiveness of One4All-ST in ST prediction for arbitrary modifiable areal units. The source codes and data of this work are available at https://github.com/uctb/One4All-ST."
https://arxiv.org/abs/2403.07021,2024-03-09,Stochastic Quantum Dynamics Stabilization: A Lyapunov-Based Control Approach with Homodyne-Mediated Filtering,"['Nahid Binandeh Dehaghani', 'A. Pedro Aguiar', 'Rafal Wisniewski']","Efficient control of stochastic dynamics in quantum systems is pivotal for various applications, including quantum information processing and metrology. This paper introduces a Lyapunov-based control approach with homodyne-mediated filtering. We employ a modified extended Kalman filtering method to directly estimate the evolution of the quantum density operator $ρ$, considering sequential homodyne current measurements. Our method explicitly addresses the dynamics of a stochastic master equation with correlated noise, ensuring by construction the quantum properties of the estimated state variable $\hatρ$. Moreover, our proposed switching based Lyapunov control scheme that is fed with $\hatρ$, guarantees noise-to-state practically stable in probability of the desired quantum stationary target set with respect to the estimation error variance. We demonstrate our approach's efficacy in stabilizing a qubit coupled to a leaky cavity under homodyne detection."
https://arxiv.org/abs/2403.07008,2024-03-08,AutoEval Done Right: Using Synthetic Data for Model Evaluation,"['Pierre Boyeau', 'Anastasios N. Angelopoulos', 'Nir Yosef', 'Jitendra Malik', 'Michael I. Jordan']",The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased. These algorithms increase the effective human-labeled sample size by up to 50% on experiments with GPT-4.
https://arxiv.org/abs/2403.07003,2024-03-07,Evacuation Management Framework towards Smart City-wide Intelligent Emergency Interactive Response System,"['Anuj Abraham', 'Yi Zhang', 'Shitala Prasad']","A smart city solution toward future 6G network deployment allows small and medium sized enterprises (SMEs), industry, and government entities to connect with the infrastructures and play a crucial role in enhancing emergency preparedness with advanced sensors. The objective of this work is to propose a set of coordinated technological solutions to transform an existing emergency response system into an intelligent interactive system, thereby improving the public services and the quality of life for residents at home, on road, in hospitals, transport hubs, etc. In this context, we consider a city wide view from three different application scenes that are closely related to peoples daily life, to optimize the actions taken at relevant departments. Therefore, using artificial intelligence (AI) and machine learning (ML) techniques to enable the next generation connected vehicle experiences, we specifically focus on accidents happening in indoor households, urban roads, and at large public facilities. This smart interactive response system will benefit from advanced sensor fusion and AI by formulating a real time dynamic model."
https://arxiv.org/abs/2403.06998,2024-03-12,High-speed Low-consumption sEMG-based Transient-state micro-Gesture Recognition,"['Youfang Han', 'Wei Zhao', 'Xiangjin Chen', 'Xin Meng']","Gesture recognition on wearable devices is extensively applied in human-computer interaction. Electromyography (EMG) has been used in many gesture recognition systems for its rapid perception of muscle signals. However, analyzing EMG signals on devices, like smart wristbands, usually needs inference models to have high performances, such as low inference latency, low power consumption, and low memory occupation. Therefore, this paper proposes an improved spiking neural network (SNN) to achieve these goals. We propose an adaptive multi-delta coding as a spiking coding method to improve recognition accuracy. We propose two additive solvers for SNN, which can reduce inference energy consumption and amount of parameters significantly, and improve the robustness of temporal differences. In addition, we propose a linear action detection method TAD-LIF, which is suitable for SNNs. TAD-LIF is an improved LIF neuron that can detect transient-state gestures quickly and accurately. We collected two datasets from 20 subjects including 6 micro gestures. The collection devices are two designed lightweight consumer-level sEMG wristbands (3 and 8 electrode channels respectively). Compared to CNN, FCN, and normal SNN-based methods, the proposed SNN has higher recognition accuracy. The accuracy of the proposed SNN is 83.85% and 93.52% on the two datasets respectively. In addition, the inference latency of the proposed SNN is about 1% of CNN, the power consumption is about 0.1% of CNN, and the memory occupation is about 20% of CNN. The proposed methods can be used for precise, high-speed, and low-power micro-gesture recognition tasks, and are suitable for consumer-level intelligent wearable devices, which is a general way to achieve ubiquitous computing."
https://arxiv.org/abs/2403.06979,2024-03-11,Tidal synchronization trapping in stars and planets with convective envelopes,['Janosz W. Dewberry'],"Tidal torques can alter the spins of tidally interacting stars and planets, usually over shorter timescales than the tidal damping of orbital separations or eccentricities. Simple tidal models predict that in eccentric binary or planetary systems, rotation periods will evolve toward a ""pseudosynchronous"" ratio with the orbital period. However, this prediction does not account for ""inertial"" waves that are present in stars or gaseous planets with (i) convective envelopes, and (ii) even very slow rotation. We demonstrate that tidal driving of inertial oscillations in eccentric systems generically produces a network of stable ""synchronization traps"" at ratios of orbital to rotation period that are simple to predict, but can deviate significantly from pseudosynchronization. The mechanism underlying spin synchronization trapping is similar to tidal resonance locking, involving a balance between torques that is maintained automatically by the scaling of inertial mode frequencies with the rotation rate. In contrast with many resonance locking scenarios, however, the torque balance required for synchronization trapping need not drive mode amplitudes to nonlinearity. Synchronization traps may provide an explanation for low-mass stars and hot Jupiters with observed rotation rates that deviate from pseudosynchronous or synchronous expectations."
https://arxiv.org/abs/2403.06961,2024-03-11,Explainable Transformer Prototypes for Medical Diagnoses,"['Ugur Demir', 'Debesh Jha', 'Zheyuan Zhang', 'Elif Keles', 'Bradley Allen', 'Aggelos K. Katsaggelos', 'Ulas Bagci']","Deployments of artificial intelligence in medical diagnostics mandate not just accuracy and efficacy but also trust, emphasizing the need for explainability in machine decisions. The recent trend in automated medical image diagnostics leans towards the deployment of Transformer-based architectures, credited to their impressive capabilities. Since the self-attention feature of transformers contributes towards identifying crucial regions during the classification process, they enhance the trustability of the methods. However, the complex intricacies of these attention mechanisms may fall short of effectively pinpointing the regions of interest directly influencing AI decisions. Our research endeavors to innovate a unique attention block that underscores the correlation between 'regions' rather than 'pixels'. To address this challenge, we introduce an innovative system grounded in prototype learning, featuring an advanced self-attention mechanism that goes beyond conventional ad-hoc visual explanation techniques by offering comprehensible visual insights. A combined quantitative and qualitative methodological approach was used to demonstrate the effectiveness of the proposed method on the large-scale NIH chest X-ray dataset. Experimental results showed that our proposed method offers a promising direction for explainability, which can lead to the development of more trustable systems, which can facilitate easier and rapid adoption of such technology into routine clinics. The code is available at www.github.com/NUBagcilab/r2r_proto."
https://arxiv.org/abs/2403.06955,2024-03-11,Accurate Crystal Structure Prediction of New 2D Hybrid Organic Inorganic Perovskites,"['Nima Karimitari', 'William J. Baldwin', 'Evan W. Muller', 'Zachary J. L. Bare', 'W. Joshua Kennedy', 'Gábor Csányi', 'Christopher Sutton']","Low dimensional hybrid organic-inorganic perovskites (HOIPs) represent a promising class of electronically active materials for both light absorption and emission. The design space of HOIPs is extremely large, since a diverse space of organic cations can be combined with different inorganic frameworks. This immense design space allows for tunable electronic and mechanical properties, but also necessitates the development of new tools for in silico high throughput analysis of candidate structures. In this work, we present an accurate, efficient, transferable and widely applicable machine learning interatomic potential (MLIP) for predicting the structure of new 2D HOIPs. Using the MACE architecture, an MLIP is trained on 86 diverse experimentally reported HOIP structures. The model is tested on 73 unseen perovskite compositions, and achieves chemical accuracy with respect to the reference electronic structure method. Our model is then combined with a simple random structure search algorithm to predict the structure of hypothetical HOIPs given only the proposed composition. Success is demonstrated by correctly and reliably recovering the crystal structure of a set of experimentally known 2D perovskites. Such a random structure search is impossible with ab initio methods due to the associated computational cost, but is relatively inexpensive with the MACE potential. Finally, the procedure is used to predict the structure formed by a new organic cation with no previously known corresponding perovskite. Laboratory synthesis of the new hybrid perovskite confirms the accuracy of our prediction. This capability, applied at scale, enables efficient screening of thousands of combinations of organic cations and inorganic layers."
https://arxiv.org/abs/2403.06951,2024-03-11,DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations,"['Tianhao Qi', 'Shancheng Fang', 'Yanze Wu', 'Hongtao Xie', 'Jiawei Liu', 'Lang Chen', 'Qian He', 'Yongdong Zhang']","The diffusion-based text-to-image model harbors immense potential in transferring reference style. However, current encoder-based approaches significantly impair the text controllability of text-to-image models while transferring styles. In this paper, we introduce DEADiff to address this issue using the following two strategies: 1) a mechanism to decouple the style and semantics of reference images. The decoupled feature representations are first extracted by Q-Formers which are instructed by different text descriptions. Then they are injected into mutually exclusive subsets of cross-attention layers for better disentanglement. 2) A non-reconstructive learning method. The Q-Formers are trained using paired images rather than the identical target, in which the reference image and the ground-truth image are with the same style or semantics. We show that DEADiff attains the best visual stylization results and optimal balance between the text controllability inherent in the text-to-image model and style similarity to the reference image, as demonstrated both quantitatively and qualitatively. Our project page is https://tianhao-qi.github.io/DEADiff/."
https://arxiv.org/abs/2403.06942,2024-03-11,Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI,"['Lang Tong', 'Xinyi Wang', 'Qing Zhao']","Purpose This article presents a case for a next-generation grid monitoring and control system, leveraging recent advances in generative artificial intelligence (AI), machine learning, and statistical inference. Advancing beyond earlier generations of wide-area monitoring systems built upon supervisory control and data acquisition (SCADA) and synchrophasor technologies, we argue for a monitoring and control framework based on the streaming of continuous point-on-wave (CPOW) measurements with AI-powered data compression and fault detection."
https://arxiv.org/abs/2403.06912,2024-03-13,DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization,"['Jiahe Li', 'Jiawei Zhang', 'Xiao Bai', 'Jin Zheng', 'Xin Ning', 'Jun Zhou', 'Lin Gu']","Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \times$ reduction in training time, and over $3000 \times$ faster rendering speed."
https://arxiv.org/abs/2403.06902,2024-03-11,Deep adaptative spectral zoom for improved remote heart rate estimation,"['Joaquim Comas', 'Adria Ruiz', 'Federico Sukno']","Recent advances in remote heart rate measurement, motivated by data-driven approaches, have notably enhanced accuracy. However, these improvements primarily focus on recovering the rPPG signal, overlooking the implicit challenges of estimating the heart rate (HR) from the derived signal. While many methods employ the Fast Fourier Transform (FFT) for HR estimation, the performance of the FFT is inherently affected by a limited frequency resolution. In contrast, the Chirp-Z Transform (CZT), a generalization form of FFT, can refine the spectrum to the narrow-band range of interest for heart rate, providing improved frequential resolution and, consequently, more accurate estimation. This paper presents the advantages of employing the CZT for remote HR estimation and introduces a novel data-driven adaptive CZT estimator. The objective of our proposed model is to tailor the CZT to match the characteristics of each specific dataset sensor, facilitating a more optimal and accurate estimation of HR from the rPPG signal without compromising generalization across diverse datasets. This is achieved through a Sparse Matrix Optimization (SMO). We validate the effectiveness of our model through exhaustive evaluations on three publicly available datasets UCLA-rPPG, PURE, and UBFC-rPPG employing both intra- and cross-database performance metrics. The results reveal outstanding heart rate estimation capabilities, establishing the proposed approach as a robust and versatile estimator for any rPPG method."
https://arxiv.org/abs/2403.06901,2024-03-11,LIBR+: Improving Intraoperative Liver Registration by Learning the Residual of Biomechanics-Based Deformable Registration,"['Dingrong Wang', 'Soheil Azadvar', 'Jon Heiselman', 'Xiajun Jiang', 'Michael Miga', 'Linwei Wang']","The surgical environment imposes unique challenges to the intraoperative registration of organ shapes to their preoperatively-imaged geometry. Biomechanical model-based registration remains popular, while deep learning solutions remain limited due to the sparsity and variability of intraoperative measurements and the limited ground-truth deformation of an organ that can be obtained during the surgery. In this paper, we propose a novel \textit{hybrid} registration approach that leverage a linearized iterative boundary reconstruction (LIBR) method based on linear elastic biomechanics, and use deep neural networks to learn its residual to the ground-truth deformation (LIBR+). We further formulate a dual-branch spline-residual graph convolutional neural network (SR-GCN) to assimilate information from sparse and variable intraoperative measurements and effectively propagate it through the geometry of the 3D organ. Experiments on a large intraoperative liver registration dataset demonstrated the consistent improvements achieved by LIBR+ in comparison to existing rigid, biomechnical model-based non-rigid, and deep-learning based non-rigid approaches to intraoperative liver registration."
https://arxiv.org/abs/2403.06890,2024-03-11,Application of Quantum Tensor Networks for Protein Classification,"['Debarshi Kundu', 'Archisman Ghosh', 'Srinivasan Ekambaram', 'Jian Wang', 'Nikolay Dokholyan', 'Swaroop Ghosh']","We show that protein sequences can be thought of as sentences in natural language processing and can be parsed using the existing Quantum Natural Language framework into parameterized quantum circuits of reasonable qubits, which can be trained to solve various protein-related machine-learning problems. We classify proteins based on their subcellular locations, a pivotal task in bioinformatics that is key to understanding biological processes and disease mechanisms. Leveraging the quantum-enhanced processing capabilities, we demonstrate that Quantum Tensor Networks (QTN) can effectively handle the complexity and diversity of protein sequences. We present a detailed methodology that adapts QTN architectures to the nuanced requirements of protein data, supported by comprehensive experimental results. We demonstrate two distinct QTNs, inspired by classical recurrent neural networks (RNN) and convolutional neural networks (CNN), to solve the binary classification task mentioned above. Our top-performing quantum model has achieved a 94% accuracy rate, which is comparable to the performance of a classical model that uses the ESM2 protein language model embeddings. It's noteworthy that the ESM2 model is extremely large, containing 8 million parameters in its smallest configuration, whereas our best quantum model requires only around 800 parameters. We demonstrate that these hybrid models exhibit promising performance, showcasing their potential to compete with classical models of similar complexity."
https://arxiv.org/abs/2403.06888,2024-03-12,Process signature-driven high spatio-temporal resolution alignment of multimodal data,"['Abhishek Hanchate', 'Himanshu Balhara', 'Vishal S. Chindepalli', 'Satish T. S. Bukkapatnam']","We present HiRA-Pro, a novel procedure to align, at high spatio-temporal resolutions, multimodal signals from real-world processes and systems that exhibit diverse transient, nonlinear stochastic dynamics, such as manufacturing machines. It is based on discerning and synchronizing the process signatures of salient kinematic and dynamic events in these disparate signals. HiRA-Pro addresses the challenge of aligning data with sub-millisecond phenomena, where traditional timestamp, external trigger, or clock-based alignment methods fall short. The effectiveness of HiRA-Pro is demonstrated in a smart manufacturing context, where it aligns data from 13+ channels acquired during 3D-printing and milling operations on an Optomec-LENS MTS 500 hybrid machine. The aligned data is then voxelized to generate 0.25 second aligned data chunks that correspond to physical voxels on the produced part. The superiority of HiRA-Pro is further showcased through case studies in additive manufacturing, demonstrating improved machine learning-based predictive performance due to precise multimodal data alignment. Specifically, testing classification accuracies improved by almost 35% with the application of HiRA-Pro, even with limited data, allowing for precise localization of artifacts. The paper also provides a comprehensive discussion on the proposed method, its applications, and comparative qualitative analysis with a few other alignment methods. HiRA-Pro achieves temporal-spatial resolutions of 10-1000 us and 100 um in order to generate datasets that register with physical voxels on the 3D-printed and milled part. These resolutions are at least an order of magnitude finer than the existing alignment methods that employ individual timestamps, statistical correlations, or common clocks, which achieve precision of hundreds of milliseconds."
https://arxiv.org/abs/2403.06874,2024-03-11,COOD: Combined out-of-distribution detection using multiple measures for anomaly & novel class detection in large-scale hierarchical classification,"['L. E. Hogeweg', 'R. Gangireddy', 'D. Brunink', 'V. J. Kalkman', 'L. Cornelissen', 'J. W. Kamminga']","High-performing out-of-distribution (OOD) detection, both anomaly and novel class, is an important prerequisite for the practical use of classification models. In this paper, we focus on the species recognition task in images concerned with large databases, a large number of fine-grained hierarchical classes, severe class imbalance, and varying image quality. We propose a framework for combining individual OOD measures into one combined OOD (COOD) measure using a supervised model. The individual measures are several existing state-of-the-art measures and several novel OOD measures developed with novel class detection and hierarchical class structure in mind. COOD was extensively evaluated on three large-scale (500k+ images) biodiversity datasets in the context of anomaly and novel class detection. We show that COOD outperforms individual, including state-of-the-art, OOD measures by a large margin in terms of TPR@1% FPR in the majority of experiments, e.g., improving detecting ImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset. SHAP (feature contribution) analysis shows that different individual OOD measures are essential for various tasks, indicating that multiple OOD measures and combinations are needed to generalize. Additionally, we show that explicitly considering ID images that are incorrectly classified for the original (species) recognition task is important for constructing high-performing OOD detection methods and for practical applicability. The framework can easily be extended or adapted to other tasks and media modalities."
https://arxiv.org/abs/2403.06871,2024-03-11,On the Generalization Ability of Unsupervised Pretraining,"['Yuyang Deng', 'Junyuan Hong', 'Jiayu Zhou', 'Mehrdad Mahdavi']","Recent advances in unsupervised learning have shown that unsupervised pre-training, followed by fine-tuning, can improve model generalization. However, a rigorous understanding of how the representation function learned on an unlabeled dataset affects the generalization of the fine-tuned model is lacking. Existing theoretical research does not adequately account for the heterogeneity of the distribution and tasks in pre-training and fine-tuning stage. To bridge this gap, this paper introduces a novel theoretical framework that illuminates the critical factor influencing the transferability of knowledge acquired during unsupervised pre-training to the subsequent fine-tuning phase, ultimately affecting the generalization capabilities of the fine-tuned model on downstream tasks. We apply our theoretical framework to analyze generalization bound of two distinct scenarios: Context Encoder pre-training with deep neural networks and Masked Autoencoder pre-training with deep transformers, followed by fine-tuning on a binary classification task. Finally, inspired by our findings, we propose a novel regularization method during pre-training to further enhances the generalization of fine-tuned model. Overall, our results contribute to a better understanding of unsupervised pre-training and fine-tuning paradigm, and can shed light on the design of more effective pre-training algorithms."
https://arxiv.org/abs/2403.06870,2024-03-11,Semantic Residual Prompts for Continual Learning,"['Martin Menabue', 'Emanuele Frascaroli', 'Matteo Boschini', 'Enver Sangineto', 'Lorenzo Bonicelli', 'Angelo Porrello', 'Simone Calderara']","Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed prompts. Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the prompts (values). However, as keys are learned while tasks progress, the prompting selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches. For instance, prompts introduced to accommodate new tasks might end up interfering with previously learned prompts. To make the selection strategy more stable, we ask a foundational model (CLIP) to select our prompt within a two-level adaptation mechanism. Specifically, the first level leverages standard textual prompts for the CLIP textual encoder, leading to stable class prototypes. The second level, instead, uses these prototypes along with the query image as keys to index a second pool. The retrieved prompts serve to adapt a pre-trained ViT, granting plasticity. In doing so, we also propose a novel residual mechanism to transfer CLIP semantics to the ViT layers. Through extensive analysis on established CL benchmarks, we show that our method significantly outperforms both state-of-the-art CL approaches and the zero-shot CLIP test. Notably, our findings hold true even for datasets with a substantial domain gap w.r.t. the pre-training knowledge of the backbone model, as showcased by experiments on satellite imagery and medical datasets."
https://arxiv.org/abs/2403.06856,2024-03-11,Concurrent Speaker Detection: A multi-microphone Transformer-Based Approach,"['Amit Eliav', 'Sharon Gannot']","We present a deep-learning approach for the task of Concurrent Speaker Detection (CSD) using a modified transformer model. Our model is designed to handle multi-microphone data but can also work in the single-microphone case. The method can classify audio segments into one of three classes: 1) no speech activity (noise only), 2) only a single speaker is active, and 3) more than one speaker is active. We incorporate a Cost-Sensitive (CS) loss and a confidence calibration to the training procedure. The approach is evaluated using three real-world databases: AMI, AliMeeting, and CHiME 5, demonstrating an improvement over existing approaches."
https://arxiv.org/abs/2403.06844,2024-03-11,ExoCubed: A Riemann-Solver based Cubed-Sphere Dynamic Core for Planetary Atmospheres,"['Sihe Chen', 'Cheng Li']","The computational fluid dynamics on a sphere is relevant to global simulations of geophysical fluid dynamics. Using the conventional spherical-polar (or lat-lon) grid results in a singularity at the poles, with orders of magnitude smaller cell sizes at the poles in comparison to the equator. To address this problem, we developed a general circulation model (dynamic core) with a gnomonic equiangular cubed-sphere configuration. This model is developed based on the Simulating Nonhydrostatic Atmospheres on Planets (SNAP) model, using a finite volume numerical scheme with a Riemann-solver-based dynamic core and the vertical implicit correction (VIC) scheme. This change of the horizontal configuration gives a 20-time acceleration of global simulations compared to the lat-lon grid with a similar number of cells at medium resolution. We presented standard tests ranging from 2D shallow-water models to 3D general circulation tests, including earth-like planets and shallow hot Jupiters, to validate the accuracy of the model. The method described in this article is generic to transform any existing finite-volume hydrodynamic model in the Cartesian geometry to the spherical geometry."
https://arxiv.org/abs/2403.06842,2024-03-11,Hybrid optimal control with mixed-integer Lagrangian methods,"['Viktoriya Nikitina', 'Alberto De Marchi', 'Matthias Gerdts']","Models involving hybrid systems are versatile in their application, but difficult to handle and optimize efficiently due to their combinatorial nature. This work presents a method to cope with hybrid optimal control problems which, in contrast to decomposition techniques, does not require relaxing the integrality constraints. Based on the discretize-then-optimize approach, our scheme addresses mixed-integer nonlinear problems under mild assumptions. The proposed numerical algorithm builds upon the augmented Lagrangian framework, whose subproblems are handled using successive mixed-integer linearizations with trust regions. We validate the performance of the numerical routine with extensive investigations using several hybrid optimal control problems from different fields of application. Promising preliminary results are presented for a motion planning task with hysteresis and drag, a Lotka-Volterra fishing problem, and a facility location design problem."
https://arxiv.org/abs/2403.06840,2024-03-11,RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback,"['Yanming Liu', 'Xinyue Peng', 'Xuhong Zhang', 'Weihao Liu', 'Jianwei Yin', 'Jiannan Cao', 'Tianyu Du']","Large language models (LLMs) demonstrate exceptional performance in numerous tasks but still heavily rely on knowledge stored in their parameters. Moreover, updating this knowledge incurs high training costs. Retrieval-augmented generation (RAG) methods address this issue by integrating external knowledge. The model can answer questions it couldn't previously by retrieving knowledge relevant to the query. This approach improves performance in certain scenarios for specific tasks. However, if irrelevant texts are retrieved, it may impair model performance. In this paper, we propose Retrieval Augmented Iterative Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and processes them in three submodules to enhance the model's problem-solving capabilities. Experiments show that our method outperforms existing benchmarks, performing well on models like GPT3.5, Llama2, significantly enhancing factual reasoning capabilities and reducing hallucinations."
https://arxiv.org/abs/2403.06839,2024-03-11,Advanced Channel Coding Designs for Index-Modulated Fluid Antenna Systems,"['Elio Faddoul', 'Ghassan M. Kraidy', 'Constantinos Psomas', 'Ioannis Krikidis']","The concept of fluid antennas (FAs) has emerged as a promising solution to enhance the spectral efficiency of wireless networks, achieved by introducing additional degrees of freedom, including reconfigurability and flexibility. In this paper, we investigate the use of index-modulated (IM) transmissions within the framework of FA systems, where an FA position is activated during each transmission interval. This approach is motivated by the common characteristics exhibited by FAs and IM transmissions, which entails the use of a single radio-frequency chain. From this perspective, we derive a closed-form expression for the bit error rate of IM-FAs considering spatial correlation, and demonstrating superior performance compared to conventional IM systems. To enhance the performance of IM-FAs in correlated conditions, channel coding techniques are applied. We first analyze a set partition coding (SPC) scheme for IM-FAs to spatially separate the FA ports, and provide a tight performance bound over correlated channels. Furthermore, the spatial SPC scheme is extended to turbo-coded modulation where the performance is analyzed for low and high signal-to-noise ratios. Our results reveal that through the implementation of channel coding techniques designed for FAs and IM transmission, the performance of coded IM-FAs exhibits notable enhancements, particularly in high correlation scenarios."
https://arxiv.org/abs/2403.06828,2024-03-11,NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning,"['Ruihua Han', 'Shuai Wang', 'Shuaijun Wang', 'Zeqing Zhang', 'Jianjun Chen', 'Shijie Lin', 'Chengyang Li', 'Chengzhong Xu', 'Yonina C. Eldar', 'Qi Hao', 'Jia Pan']","Navigating a nonholonomic robot in a cluttered environment requires extremely accurate perception and locomotion for collision avoidance. This paper presents NeuPAN: a real-time, highly-accurate, map-free, robot-agnostic, and environment-invariant robot navigation solution. Leveraging a tightly-coupled perception-locomotion framework, NeuPAN has two key innovations compared to existing approaches: 1) it directly maps raw points to a learned multi-frame distance space, avoiding error propagation from perception to control; 2) it is interpretable from an end-to-end model-based learning perspective, enabling provable convergence. The crux of NeuPAN is to solve a high-dimensional end-to-end mathematical model with various point-level constraints using the plug-and-play (PnP) proximal alternating-minimization network (PAN) with neurons in the loop. This allows NeuPAN to generate real-time, end-to-end, physically-interpretable motions directly from point clouds, which seamlessly integrates data- and knowledge-engines, where its network parameters are adjusted via back propagation. We evaluate NeuPAN on car-like robot, wheel-legged robot, and passenger autonomous vehicle, in both simulated and real-world environments. Experiments demonstrate that NeuPAN outperforms various benchmarks, in terms of accuracy, efficiency, robustness, and generalization capability across various environments, including the cluttered sandbox, office, corridor, and parking lot. We show that NeuPAN works well in unstructured environments with arbitrary-shape undetectable objects, making impassable ways passable."
https://arxiv.org/abs/2403.06826,2024-03-11,In-context Exploration-Exploitation for Reinforcement Learning,"['Zhenwen Dai', 'Federico Tomasi', 'Sina Ghiassian']","In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method."
https://arxiv.org/abs/2403.06814,2024-03-11,ε-Neural Thompson Sampling of Deep Brain Stimulation for Parkinson Disease Treatment,"['Hao-Lun Hsu', 'Qitong Gao', 'Miroslav Pajic']","Deep Brain Stimulation (DBS) stands as an effective intervention for alleviating the motor symptoms of Parkinson's disease (PD). Traditional commercial DBS devices are only able to deliver fixed-frequency periodic pulses to the basal ganglia (BG) regions of the brain, i.e., continuous DBS (cDBS). However, they in general suffer from energy inefficiency and side effects, such as speech impairment. Recent research has focused on adaptive DBS (aDBS) to resolve the limitations of cDBS. Specifically, reinforcement learning (RL) based approaches have been developed to adapt the frequencies of the stimuli in order to achieve both energy efficiency and treatment efficacy. However, RL approaches in general require significant amount of training data and computational resources, making it intractable to integrate RL policies into real-time embedded systems as needed in aDBS. In contrast, contextual multi-armed bandits (CMAB) in general lead to better sample efficiency compared to RL. In this study, we propose a CMAB solution for aDBS. Specifically, we define the context as the signals capturing irregular neuronal firing activities in the BG regions (i.e., beta-band power spectral density), while each arm signifies the (discretized) pulse frequency of the stimulation. Moreover, an ε-exploring strategy is introduced on top of the classic Thompson sampling method, leading to an algorithm called ε-Neural Thompson sampling (ε-NeuralTS), such that the learned CMAB policy can better balance exploration and exploitation of the BG environment. The ε-NeuralTS algorithm is evaluated using a computation BG model that captures the neuronal activities in PD patients' brains. The results show that our method outperforms both existing cDBS methods and CMAB baselines."
https://arxiv.org/abs/2403.06804,2024-03-11,Shape Non-rigid Kinematics (SNK): A Zero-Shot Method for Non-Rigid Shape Matching via Unsupervised Functional Map Regularized Reconstruction,"['Souhaib Attaiki', 'Maks Ovsjanikov']","We present Shape Non-rigid Kinematics (SNK), a novel zero-shot method for non-rigid shape matching that eliminates the need for extensive training or ground truth data. SNK operates on a single pair of shapes, and employs a reconstruction-based strategy using an encoder-decoder architecture, which deforms the source shape to closely match the target shape. During the process, an unsupervised functional map is predicted and converted into a point-to-point map, serving as a supervisory mechanism for the reconstruction. To aid in training, we have designed a new decoder architecture that generates smooth, realistic deformations. SNK demonstrates competitive results on traditional benchmarks, simplifying the shape-matching process without compromising accuracy. Our code can be found online: https://github.com/pvnieo/SNK"
https://arxiv.org/abs/2403.06803,2024-03-11,Data-Independent Operator: A Training-Free Artifact Representation Extractor for Generalizable Deepfake Detection,"['Chuangchuang Tan', 'Ping Liu', 'RenShuai Tao', 'Huan Liu', 'Yao Zhao', 'Baoyuan Wu', 'Yunchao Wei']","Recently, the proliferation of increasingly realistic synthetic images generated by various generative adversarial networks has increased the risk of misuse. Consequently, there is a pressing need to develop a generalizable detector for accurately recognizing fake images. The conventional methods rely on generating diverse training sources or large pretrained models. In this work, we show that, on the contrary, the small and training-free filter is sufficient to capture more general artifact representations. Due to its unbias towards both the training and test sources, we define it as Data-Independent Operator (DIO) to achieve appealing improvements on unseen sources. In our framework, handcrafted filters and the randomly-initialized convolutional layer can be used as the training-free artifact representations extractor with excellent results. With the data-independent operator of a popular classifier, such as Resnet50, one could already reach a new state-of-the-art without bells and whistles. We evaluate the effectiveness of the DIO on 33 generation models, even DALLE and Midjourney. Our detector achieves a remarkable improvement of $13.3\%$, establishing a new state-of-the-art performance. The DIO and its extension can serve as strong baselines for future methods. The code is available at \url{https://github.com/chuangchuangtan/Data-Independent-Operator}."
https://arxiv.org/abs/2403.06801,2024-03-11,CT2Rep: Automated Radiology Report Generation for 3D Medical Imaging,"['Ibrahim Ethem Hamamci', 'Sezgin Er', 'Bjoern Menze']","Medical imaging plays a crucial role in diagnosis, with radiology reports serving as vital documentation. Automating report generation has emerged as a critical need to alleviate the workload of radiologists. While machine learning has facilitated report generation for 2D medical imaging, extending this to 3D has been unexplored due to computational complexity and data scarcity. We introduce the first method to generate radiology reports for 3D medical imaging, specifically targeting chest CT volumes. Given the absence of comparable methods, we establish a baseline using an advanced 3D vision encoder in medical imaging to demonstrate our method's effectiveness, which leverages a novel auto-regressive causal transformer. Furthermore, recognizing the benefits of leveraging information from previous visits, we augment CT2Rep with a cross-attention-based multi-modal fusion module and hierarchical memory, enabling the incorporation of longitudinal multimodal data. Access our code at: https://github.com/ibrahimethemhamamci/CT2Rep"
https://arxiv.org/abs/2403.06797,2024-03-11,Leveraging Internal Representations of Model for Magnetic Image Classification,"['Adarsh N L', 'Arun P V', 'Alok Porwal', 'Malcolm Aranha']","Data generated by edge devices has the potential to train intelligent autonomous systems across various domains. Despite the emergence of diverse machine learning approaches addressing privacy concerns and utilizing distributed data, security issues persist due to the sensitive storage of data shards in disparate locations. This paper introduces a potentially groundbreaking paradigm for machine learning model training, specifically designed for scenarios with only a single magnetic image and its corresponding label image available. We harness the capabilities of Deep Learning to generate concise yet informative samples, aiming to overcome data scarcity. Through the utilization of deep learning's internal representations, our objective is to efficiently address data scarcity issues and produce meaningful results. This methodology presents a promising avenue for training machine learning models with minimal data."
https://arxiv.org/abs/2403.06786,2024-03-11,Genetic Learning for Designing Sim-to-Real Data Augmentations,"['Bram Vanherle', 'Nick Michiels', 'Frank Van Reeth']","Data augmentations are useful in closing the sim-to-real domain gap when training on synthetic data. This is because they widen the training data distribution, thus encouraging the model to generalize better to other domains. Many image augmentation techniques exist, parametrized by different settings, such as strength and probability. This leads to a large space of different possible augmentation policies. Some policies work better than others for overcoming the sim-to-real gap for specific datasets, and it is unclear why. This paper presents two different interpretable metrics that can be combined to predict how well a certain augmentation policy will work for a specific sim-to-real setting, focusing on object detection. We validate our metrics by training many models with different augmentation policies and showing a strong correlation with performance on real data. Additionally, we introduce GeneticAugment, a genetic programming method that can leverage these metrics to automatically design an augmentation policy for a specific dataset without needing to train a model."
https://arxiv.org/abs/2403.06779,2024-03-11,From Factor Models to Deep Learning: Machine Learning in Reshaping Empirical Asset Pricing,"['Junyi Ye', 'Bhaskar Goswami', 'Jingyi Gu', 'Ajim Uddin', 'Guiling Wang']","This paper comprehensively reviews the application of machine learning (ML) and AI in finance, specifically in the context of asset pricing. It starts by summarizing the traditional asset pricing models and examining their limitations in capturing the complexities of financial markets. It explores how 1) ML models, including supervised, unsupervised, semi-supervised, and reinforcement learning, provide versatile frameworks to address these complexities, and 2) the incorporation of advanced ML algorithms into traditional financial models enhances return prediction and portfolio optimization. These methods can adapt to changing market dynamics by modeling structural changes and incorporating heterogeneous data sources, such as text and images. In addition, this paper explores challenges in applying ML in asset pricing, addressing the growing demand for explainability in decision-making and mitigating overfitting in complex models. This paper aims to provide insights into novel methodologies showcasing the potential of ML to reshape the future of quantitative finance."
https://arxiv.org/abs/2403.06764,2024-03-11,An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models,"['Liang Chen', 'Haozhe Zhao', 'Tianyu Liu', 'Shuai Bai', 'Junyang Lin', 'Chang Zhou', 'Baobao Chang']","In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones. Our evaluations demonstrate FastV's ability to dramatically reduce computational costs (e.g., a 45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks. The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve a lower budget than that of a 7B-parameter model, while still maintaining superior performance. We believe FastV has practical values for deployment of LVLMs in edge devices and commercial models. Code is released at https://github.com/pkunlp-icler/FastV."
https://arxiv.org/abs/2403.06757,2024-03-13,Koopman Ensembles for Probabilistic Time Series Forecasting,"['Anthony Frion', 'Lucas Drumetz', 'Guillaume Tochon', 'Mauro Dalla Mura', 'Albdeldjalil Aïssa El Bey']","In the context of an increasing popularity of data-driven models to represent dynamical systems, many machine learning-based implementations of the Koopman operator have recently been proposed. However, the vast majority of those works are limited to deterministic predictions, while the knowledge of uncertainty is critical in fields like meteorology and climatology. In this work, we investigate the training of ensembles of models to produce stochastic outputs. We show through experiments on real remote sensing image time series that ensembles of independently trained models are highly overconfident and that using a training criterion that explicitly encourages the members to produce predictions with high inter-model variances greatly improves the uncertainty quantification of the ensembles."
https://arxiv.org/abs/2403.06756,2024-03-11,One-Bit Target Detection in Collocated MIMO Radar with Colored Background Noise,"['Yu-Hang Xiao', 'David Ramírez', 'Lei Huang', 'Xiao Peng Li', 'Hing Cheung So']","One-bit sampling has emerged as a promising technique in multiple-input multiple-output (MIMO) radar systems due to its ability to significantly reduce data volume and processing requirements. Nevertheless, current detection methods have not adequately addressed the impact of colored noise, which is frequently encountered in real scenarios. In this paper, we present a novel detection method that accounts for colored noise in MIMO radar systems. Specifically, we derive Rao's test by computing the derivative of the likelihood function with respect to the target reflectivity parameter and the Fisher information matrix, resulting in a detector that takes the form of a weighted matched filter. To ensure the constant false alarm rate (CFAR) property, we also consider noise covariance uncertainty and examine its effect on the probability of false alarm. The detection probability is also studied analytically. Simulation results demonstrate that the proposed detector provides considerable performance gains in the presence of colored noise."
https://arxiv.org/abs/2403.06754,2024-03-11,ALaRM: Align Language Models via Hierarchical Rewards Modeling,"['Yuhang Lai', 'Siyuan Wang', 'Shujun Liu', 'Xuanjing Huang', 'Zhongyu Wei']","We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over existing baselines. Our work underscores the effectiveness of hierarchical rewards modeling in refining LLM training processes for better human preference alignment. We release our code at https://ALaRM-fdu.github.io."
https://arxiv.org/abs/2403.06748,2024-03-11,Shortcut Learning in Medical Image Segmentation,"['Manxi Lin', 'Nina Weng', 'Kamil Mikolaj', 'Zahra Bashir', 'Morten Bo Søndergaard Svendsen', 'Martin Tolsgaard', 'Anders Nymark Christensen', 'Aasa Feragen']","Shortcut learning is a phenomenon where machine learning models prioritize learning simple, potentially misleading cues from data that do not generalize well beyond the training set. While existing research primarily investigates this in the realm of image classification, this study extends the exploration of shortcut learning into medical image segmentation. We demonstrate that clinical annotations such as calipers, and the combination of zero-padded convolutions and center-cropped training sets in the dataset can inadvertently serve as shortcuts, impacting segmentation accuracy. We identify and evaluate the shortcut learning on two different but common medical image segmentation tasks. In addition, we suggest strategies to mitigate the influence of shortcut learning and improve the generalizability of the segmentation models. By uncovering the presence and implications of shortcuts in medical image segmentation, we provide insights and methodologies for evaluating and overcoming this pervasive challenge and call for attention in the community for shortcuts in segmentation."
https://arxiv.org/abs/2403.06746,2024-03-11,Integration of Physics-Derived Memristor Models with Machine Learning Frameworks,"['Zhenming Yu', 'Stephan Menzel', 'John Paul Strachan', 'Emre Neftci']","Simulation frameworks such MemTorch, DNN+NeuroSim, and aihwkit are commonly used to facilitate the end-to-end co-design of memristive machine learning (ML) accelerators. These simulators can take device nonidealities into account and are integrated with modern ML frameworks. However, memristors in these simulators are modeled with either lookup tables or simple analytic models with basic nonlinearities. These simple models are unable to capture certain performance-critical aspects of device nonidealities. For example, they ignore the physical cause of switching, which induces errors in switching timings and thus incorrect estimations of conductance states. This work aims at bringing physical dynamics into consideration to model nonidealities while being compatible with GPU accelerators. We focus on Valence Change Memory (VCM) cells, where the switching nonlinearity and SET/RESET asymmetry relate tightly with the thermal resistance, ion mobility, Schottky barrier height, parasitic resistance, and other effects. The resulting dynamics require solving an ODE that captures changes in oxygen vacancies. We modified a physics-derived SPICE-level VCM model, integrated it with the aihwkit simulator and tested the performance with the MNIST dataset. Results show that noise that disrupts the SET/RESET matching affects network performance the most. This work serves as a tool for evaluating how physical dynamics in memristive devices affect neural network accuracy and can be used to guide the development of future integrated devices."
https://arxiv.org/abs/2403.06745,2024-03-11,ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation,"['Shaojie Dai', 'Xin Liu', 'Ping Luo', 'Yue Yu']","Large language model (LLM) has achieved promising performance in multilingual machine translation tasks through zero/few-shot prompts or prompt-tuning. However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation. For this issue, this paper introduces an \textbf{\underline{A}}uto-\textbf{\underline{C}}onstriction \textbf{\underline{T}}urning mechanism for \textbf{\underline{M}}ultilingual \textbf{\underline{N}}eural \textbf{\underline{M}}achine \textbf{\underline{T}}ranslation (\model), which is a novel supervised fine-tuning mechanism and orthogonal to the traditional prompt-based methods. In this method, \model automatically constructs a constrained template in the target side by adding trigger tokens ahead of the ground truth. Furthermore, trigger tokens can be arranged and combined freely to represent different task semantics, and they can be iteratively updated to maximize the label likelihood. Experiments are performed on WMT test sets with multiple metrics, and the experimental results demonstrate that \model achieves substantially improved performance across multiple translation directions and reduce the off-target phenomena in the translation."
https://arxiv.org/abs/2403.06726,2024-03-11,Probabilistic Contrastive Learning for Long-Tailed Visual Recognition,"['Chaoqun Du', 'Yulin Wang', 'Shiji Song', 'Gao Huang']","Long-tailed distributions frequently emerge in real-world data, where a large number of minority categories contain a limited number of samples. Such imbalance issue considerably impairs the performance of standard supervised learning algorithms, which are mainly designed for balanced training sets. Recent investigations have revealed that supervised contrastive learning exhibits promising potential in alleviating the data imbalance. However, the performance of supervised contrastive learning is plagued by an inherent challenge: it necessitates sufficiently large batches of training data to construct contrastive pairs that cover all categories, yet this requirement is difficult to meet in the context of class-imbalanced data. To overcome this obstacle, we propose a novel probabilistic contrastive (ProCo) learning algorithm that estimates the data distribution of the samples from each class in the feature space, and samples contrastive pairs accordingly. In fact, estimating the distributions of all classes using features in a small batch, particularly for imbalanced data, is not feasible. Our key idea is to introduce a reasonable and simple assumption that the normalized features in contrastive learning follow a mixture of von Mises-Fisher (vMF) distributions on unit space, which brings two-fold benefits. First, the distribution parameters can be estimated using only the first sample moment, which can be efficiently computed in an online manner across different batches. Second, based on the estimated distribution, the vMF distribution allows us to sample an infinite number of contrastive pairs and derive a closed form of the expected contrastive loss for efficient optimization. Our code is available at https://github.com/LeapLabTHU/ProCo."
https://arxiv.org/abs/2403.06712,2024-03-11,The Ouroboros of Memristors: Neural Networks Facilitating Memristor Programming,"['Zhenming Yu', 'Ming-Jay Yang', 'Jan Finkbeiner', 'Sebastian Siegel', 'John Paul Strachan', 'Emre Neftci']","Memristive devices hold promise to improve the scale and efficiency of machine learning and neuromorphic hardware, thanks to their compact size, low power consumption, and the ability to perform matrix multiplications in constant time. However, on-chip training with memristor arrays still faces challenges, including device-to-device and cycle-to-cycle variations, switching non-linearity, and especially SET and RESET asymmetry. To combat device non-linearity and asymmetry, we propose to program memristors by harnessing neural networks that map desired conductance updates to the required pulse times. With our method, approximately 95% of devices can be programmed within a relative percentage difference of +-50% from the target conductance after just one attempt. Our approach substantially reduces memristor programming delays compared to traditional write-and-verify methods, presenting an advantageous solution for on-chip training scenarios. Furthermore, our proposed neural network can be accelerated by memristor arrays upon deployment, providing assistance while reducing hardware overhead compared with previous works."
https://arxiv.org/abs/2403.06702,2024-03-11,Fast Text-to-3D-Aware Face Generation and Manipulation via Direct Cross-modal Mapping and Geometric Regularization,"['Jinlu Zhang', 'Yiyi Zhou', 'Qiancheng Zheng', 'Xiaoxiong Du', 'Gen Luo', 'Jun Peng', 'Xiaoshuai Sun', 'Rongrong Ji']","Text-to-3D-aware face (T3D Face) generation and manipulation is an emerging research hot spot in machine learning, which still suffers from low efficiency and poor quality. In this paper, we propose an End-to-End Efficient and Effective network for fast and accurate T3D face generation and manipulation, termed $E^3$-FaceNet. Different from existing complex generation paradigms, $E^3$-FaceNet resorts to a direct mapping from text instructions to 3D-aware visual space. We introduce a novel Style Code Enhancer to enhance cross-modal semantic alignment, alongside an innovative Geometric Regularization objective to maintain consistency across multi-view generations. Extensive experiments on three benchmark datasets demonstrate that $E^3$-FaceNet can not only achieve picture-like 3D face generation and manipulation, but also improve inference speed by orders of magnitudes. For instance, compared with Latent3D, $E^3$-FaceNet speeds up the five-view generations by almost 470 times, while still exceeding in generation quality. Our code are released at https://github.com/Aria-Zhangjl/E3-FaceNet."
https://arxiv.org/abs/2403.06700,2024-03-11,Enhancing Adversarial Training with Prior Knowledge Distillation for Robust Image Compression,"['Cao Zhi', 'Bao Youneng', 'Meng Fanyang', 'Li Chao', 'Tan Wen', 'Wang Genhong', 'Liang Yongsheng']","Deep neural network-based image compression (NIC) has achieved excellent performance, but NIC method models have been shown to be susceptible to backdoor attacks. Adversarial training has been validated in image compression models as a common method to enhance model robustness. However, the improvement effect of adversarial training on model robustness is limited. In this paper, we propose a prior knowledge-guided adversarial training framework for image compression models. Specifically, first, we propose a gradient regularization constraint for training robust teacher models. Subsequently, we design a knowledge distillation based strategy to generate a priori knowledge from the teacher model to the student model for guiding adversarial training. Experimental results show that our method improves the reconstruction quality by about 9dB when the Kodak dataset is elected as the backdoor attack object for psnr attack. Compared with Ma2023, our method has a 5dB higher PSNR output at high bitrate points."
https://arxiv.org/abs/2403.06698,2024-03-11,PCLD: Point Cloud Layerwise Diffusion for Adversarial Purification,"['Mert Gulsen', 'Batuhan Cengiz', 'Yusuf H. Sahin', 'Gozde Unal']","Point clouds are extensively employed in a variety of real-world applications such as robotics, autonomous driving and augmented reality. Despite the recent success of point cloud neural networks, especially for safety-critical tasks, it is essential to also ensure the robustness of the model. A typical way to assess a model's robustness is through adversarial attacks, where test-time examples are generated based on gradients to deceive the model. While many different defense mechanisms are studied in 2D, studies on 3D point clouds have been relatively limited in the academic field. Inspired from PointDP, which denoises the network inputs by diffusion, we propose Point Cloud Layerwise Diffusion (PCLD), a layerwise diffusion based 3D point cloud defense strategy. Unlike PointDP, we propagated the diffusion denoising after each layer to incrementally enhance the results. We apply our defense method to different types of commonly used point cloud models and adversarial attacks to evaluate its robustness. Our experiments demonstrate that the proposed defense method achieved results that are comparable to or surpass those of existing methodologies, establishing robustness through a novel technique. Code is available at https://github.com/batuceng/diffusion-layer-robustness-pc."
10.3386/w32186,2024-03-04 00:00:00,Friends with Benefits: Social Capital and Household Financial Behavior,"Brad Cannon, David Hirshleifer, Joshua Thornton","Using friendship data from Facebook, we study the effects of three aspects of social capital on household financial behavior. We find that the most important measure of social capital in explaining stock market and saving participation is Economic Connectedness, defined as the fraction of one’s social network with high socioeconomic status. One standard-deviation greater Economic Connectedness is associated with 2.9% greater stock market participation and 5.0% greater saving participation. Compared to Cohesiveness or Civic Engagement, Economic Connectedness explains more than 6 times the variation in stock market participation and more than 4 times the variation in saving participation. Using data on nonlocal friendships, we provide evidence supporting a causal link between household financial behavior and the income of one's friends. Furthermore, we provide evidence that greater opportunities for social interaction with wealthy individuals is associated with increased stock market and saving participation."
https://doi.org/10.31234/osf.io/6txyf,2024-03-13,"Art, Music, and Sport: Catalysts of Growth for Adolescents in Extreme Contexts - A Qualitative Inquiry",To be fetched,"Limited research has focused on the experiences of adolescents in extracurricular programs and their potential to enhance their optimal development. Moreover, less studies examine the participation in such programs for those developing in extreme conditions (i.e. extreme poverty, sexual exploitation, refugee camps, conflicted areas, areas affected by climate changes). This study focuses on the experience of youth living in the difficult context of Southern Madagascar. The aims of this study are to explore and gain a better understanding of the experience of adolescents engaging in extracurricular activities within this extreme context. A total of 14 (N=14) semi-structured interviews were conducted with 14 participants (M=7; F=7), aged 15 to 19 years old, involved in extracurricular programs like art, music and sport, provided by a local non-governmental organisation. Based on a thematic analysis, the results show that this extracurricular program support the process of socialization of youth, personal growth, and give a sense of protection. Such programs also support their psychological well-being and their basic psychological needs in a challenging and insecure environment, and attempt to bridge the gap in basic resources. Overall, these results offer interesting insights on the value of these programs for youth living in extreme contexts of development."
https://doi.org/10.31234/osf.io/msuwz,2024-03-13,Long-term perspectives of participating in extracurricular activities in an extreme context in Madagascar,To be fetched,"In recent decades, extreme development contexts (i.e., extreme poverty, refugee camps, conflicted areas, areas affected by climate change, sexual exploitation) are gaining ground, increasingly affecting youth throughout the world. It is crucial to examine how youth optimal development can be sustained in these extreme contexts. Extracurricular activities are considered a potential avenue, yet there is limited research on their impact, especially concerning long-term effects in adulthood. This collaborative study aims to explore and gain a deeper understanding of the long-term perspectives of young adults who engaged in extracurricular activities (specifically art-music and sports) during adolescence in the challenging southern region of Madagascar. Semi-structured interviews were conducted with eight participants (M=4; F=4), aged 19 to 27. The results indicate the value of these programs during their involvement, supporting basic psychological needs, acting as a protective factor against risky behaviors, providing a meaningful time investment, offering a nurturing environment for developing personal values, fostering personal growth, and focusing on education. Importantly, the programs appear to have a lasting impact, influencing participants' adult lives. They contribute to the development of enduring relationships, promote the acquisition of transferable cognitive and non-cognitive skills, facilitate the pursuit of new personal projects, and open doors to employment opportunities. Overall, the findings offer valuable insights into the significance of participating in extracurricular programs for youth in extreme contexts of development, both during and years after their involvement. Results underscore extracurricular programs as a valuable means to sustain optimal youth development over time, emphasizing their importance in challenging environments."
https://doi.org/10.31234/osf.io/6txyf,2024-03-13,"Art, Music, and Sport: Catalysts of Growth for Adolescents in Extreme Contexts - A Qualitative Inquiry","Laurie Decarpentrie, Claude Bélanger, Bryan Rakotondramanana, Olivier Rakotomalala, José Luis Guirao, Miranto Andrianina Ramarokoto Ny Aina, Tegwen Gadais","Limited research has focused on the experiences of adolescents in extracurricular programs and their potential to enhance their optimal development. Moreover, less studies examine the participation in such programs for those developing in extreme conditions (i.e. extreme poverty, sexual exploitation, refugee camps, conflicted areas, areas affected by climate changes). This study focuses on the experience of youth living in the difficult context of Southern Madagascar. The aims of this study are to explore and gain a better understanding of the experience of adolescents engaging in extracurricular activities within this extreme context. A total of 14 (N=14) semi-structured interviews were conducted with 14 participants (M=7; F=7), aged 15 to 19 years old, involved in extracurricular programs like art, music and sport, provided by a local non-governmental organisation. Based on a thematic analysis, the results show that this extracurricular program support the process of socialization of youth, personal growth, and give a sense of protection. Such programs also support their psychological well-being and their basic psychological needs in a challenging and insecure environment, and attempt to bridge the gap in basic resources. Overall, these results offer interesting insights on the value of these programs for youth living in extreme contexts of development."
https://doi.org/10.31234/osf.io/msuwz,2024-03-13,Long-term perspectives of participating in extracurricular activities in an extreme context in Madagascar,"Laurie Decarpentrie, Claude Bélanger, Bryan Rakotondramanana, Sitrakarivo Rakoto, Olivier Rakotomalala, José Luis Guirao, Tegwen Gadais","In recent decades, extreme development contexts (i.e., extreme poverty, refugee camps, conflicted areas, areas affected by climate change, sexual exploitation) are gaining ground, increasingly affecting youth throughout the world. It is crucial to examine how youth optimal development can be sustained in these extreme contexts. Extracurricular activities are considered a potential avenue, yet there is limited research on their impact, especially concerning long-term effects in adulthood. This collaborative study aims to explore and gain a deeper understanding of the long-term perspectives of young adults who engaged in extracurricular activities (specifically art-music and sports) during adolescence in the challenging southern region of Madagascar. Semi-structured interviews were conducted with eight participants (M=4; F=4), aged 19 to 27. The results indicate the value of these programs during their involvement, supporting basic psychological needs, acting as a protective factor against risky behaviors, providing a meaningful time investment, offering a nurturing environment for developing personal values, fostering personal growth, and focusing on education. Importantly, the programs appear to have a lasting impact, influencing participants' adult lives. They contribute to the development of enduring relationships, promote the acquisition of transferable cognitive and non-cognitive skills, facilitate the pursuit of new personal projects, and open doors to employment opportunities. Overall, the findings offer valuable insights into the significance of participating in extracurricular programs for youth in extreme contexts of development, both during and years after their involvement. Results underscore extracurricular programs as a valuable means to sustain optimal youth development over time, emphasizing their importance in challenging environments."
https://doi.org/10.31234/osf.io/yvph8,2024-03-13,Determinants of economic risk preferences across adolescence,To be fetched,"This study examined economic risk using a multidimensional approach in a large sample of human adolescents and adults (N = 444, ages 13-27). Protracted development of regulatory neural circuits underpins adolescent-specific risk increases in socioemotional contexts. Although these trends are likely context-specific, comparatively little is known about adolescent economic risk taking. Even less is known about the role of affect in economic risk, which is critical as affect fluctuates throughout adolescence and adolescence is a time of important future-oriented financial decisions. We tested six psychological determinants of risk preference (age, gender, positive affect, negative affect, anxiety, and indecisiveness) on two economic decision tasks (loss aversion and skewness). Adolescents demonstrated higher positive affect and reduced negativity bias in economic risk preference compared to adults. Positive affect boosted risk preference for all subjects, but adults and adolescents diverged in risk direction pointing to differences in subjective valuation. Anxiety was age invariant whereas indecision was not significant. These results highlight the evolving nature of valence bias and affect, with relevance for economic risk, from adolescence into adulthood. This study contributes to understanding of the development of economic choice, and to broader efforts to improve individual and societal risk-related decisions."
https://doi.org/10.31234/osf.io/yvph8,2024-03-13,Determinants of economic risk preferences across adolescence,"Yubing Zhang, Sarah Tashjian","This study examined economic risk using a multidimensional approach in a large sample of human adolescents and adults (N = 444, ages 13-27). Protracted development of regulatory neural circuits underpins adolescent-specific risk increases in socioemotional contexts. Although these trends are likely context-specific, comparatively little is known about adolescent economic risk taking. Even less is known about the role of affect in economic risk, which is critical as affect fluctuates throughout adolescence and adolescence is a time of important future-oriented financial decisions. We tested six psychological determinants of risk preference (age, gender, positive affect, negative affect, anxiety, and indecisiveness) on two economic decision tasks (loss aversion and skewness). Adolescents demonstrated higher positive affect and reduced negativity bias in economic risk preference compared to adults. Positive affect boosted risk preference for all subjects, but adults and adolescents diverged in risk direction pointing to differences in subjective valuation. Anxiety was age invariant whereas indecision was not significant. These results highlight the evolving nature of valence bias and affect, with relevance for economic risk, from adolescence into adulthood. This study contributes to understanding of the development of economic choice, and to broader efforts to improve individual and societal risk-related decisions."
https://doi.org/10.31234/osf.io/qp5x4,2024-03-12,Emerging Adults’ Journeys out of the Shutdown: Longitudinal Narrative Patterns in a College Career Defined by COVID,To be fetched,"The COVID-19 pandemic has defined the college career for this generation of learners, threatening mental health, identity development, and college functioning. We began tracking the impacts of this pandemic for 633 first-year college students from four US universities (M age = 18.8 years) in Spring 2020 and followed students to Spring 2023. Students provided narratives about the impacts of COVID and reports of mental health concerns, identity development, well-being. Students reported concerns for mental health, identity, and well-being during the first year of COVID impacts. The return to in-person activities predicted broad increases in narrative growth and concomitant decreases in COVID stressors, increases in identity exploration and commitment and increases in psychological and academic well-being. Changes in COVID stressors and narrative growth served as mediators between the return to in-person activities around campus and student outcomes. Findings expand insights of development and mental health across much of this generation-defining event."
https://doi.org/10.31234/osf.io/s346z,2024-03-12,The Probabilistic Price of Life Across Time: Generational and Probabilistic Distance Render a Life Today Worth More than Ten Tomorrow,To be fetched,"Is the certainty of saving a life today worth more than the less-certain outcome of saving 10 lives tomorrow? In six pre-registered studies with US samples from Prolific (N = 5,095), we employed an intergenerational probability discounting task, discovering people discount the value of life as uncertainty and intergenerational distance from the present increase. Specifically, as uncertainty about impacting the future rises, individuals increasingly prioritize saving fewer present lives over more future lives, particularly for more distant future beneficiaries (Studies 1-2). Experimental evidence (Studies 3-4) suggests that certainty perceptions drive intergenerational concern, rather than the reverse. Drawing upon seminal research from cognitive science and behavioral economics, these findings address gaps in emerging social psychological inquiry into long-term intergenerational concern, reconcile debates on the ethical philosophy of longtermism, and offer practical implications for decision-makers in public policy, emphasizing the need to balance hypothetical threats to society’s future with its present-day needs."
https://doi.org/10.31234/osf.io/s346z,2024-03-12,The Probabilistic Price of Life Across Time: Generational and Probabilistic Distance Render a Life Today Worth More than Ten Tomorrow,"Kyle Fiore Law, Stylianos Syropoulos, Brendan Bo O'Connor, Liane Young","Is the certainty of saving a life today worth more than the less-certain outcome of saving 10 lives tomorrow? In six pre-registered studies with US samples from Prolific (N = 5,095), we employed an intergenerational probability discounting task, discovering people discount the value of life as uncertainty and intergenerational distance from the present increase. Specifically, as uncertainty about impacting the future rises, individuals increasingly prioritize saving fewer present lives over more future lives, particularly for more distant future beneficiaries (Studies 1-2). Experimental evidence (Studies 3-4) suggests that certainty perceptions drive intergenerational concern, rather than the reverse. Drawing upon seminal research from cognitive science and behavioral economics, these findings address gaps in emerging social psychological inquiry into long-term intergenerational concern, reconcile debates on the ethical philosophy of longtermism, and offer practical implications for decision-makers in public policy, emphasizing the need to balance hypothetical threats to society’s future with its present-day needs."
https://doi.org/10.31234/osf.io/qp5x4,2024-03-12,Emerging Adults’ Journeys out of the Shutdown: Longitudinal Narrative Patterns in a College Career Defined by COVID,"Jordan Ashton Booker, Robyn Fivush, Andrea Greenhoot, Kate C. McLean, Cecilia Wainryb, Monisha Pasupathi","The COVID-19 pandemic has defined the college career for this generation of learners, threatening mental health, identity development, and college functioning. We began tracking the impacts of this pandemic for 633 first-year college students from four US universities (M age = 18.8 years) in Spring 2020 and followed students to Spring 2023. Students provided narratives about the impacts of COVID and reports of mental health concerns, identity development, well-being. Students reported concerns for mental health, identity, and well-being during the first year of COVID impacts. The return to in-person activities predicted broad increases in narrative growth and concomitant decreases in COVID stressors, increases in identity exploration and commitment and increases in psychological and academic well-being. Changes in COVID stressors and narrative growth served as mediators between the return to in-person activities around campus and student outcomes. Findings expand insights of development and mental health across much of this generation-defining event."
https://doi.org/10.31234/osf.io/948fg,2024-03-12,Overt and Subtle Discrimination and Psychological Well-Being: Examining the Mediating and Moderating Role of Ethnic-Racial Identity Among Emerging Adults,To be fetched,"Ethnic-racial identity (ERI) has been reported as mediator and moderator of the relation between discrimination and psychological well-being. However, it remains unclear how different forms of discrimination (i.e., overt and subtle) predict well-being over time, and whether ERI exploration and commitment mediate or moderate this association. This preregistered study explored the associations of overt and subtle discrimination with well-being (i.e., depression, substance use, life satisfaction) in a sample of 323 ethnic-racial minoritized college students (Mage W1 = 18.03, 62.7% female) from longitudinal data collected in the US. Cross-lagged panel models across three waves indicated no associations of overt discrimination, but participants experiencing more subtle discrimination during their transition to college reported more depressive symptoms after four months. ERI did not function as mediator or moderator. Findings indicate the need for a more nuanced understanding of the role of ERI during emerging adulthood."
https://doi.org/10.31234/osf.io/a4beh,2024-03-11,Influence of Social Media Campaigns on College Students  Mental Health Awareness,To be fetched,"Mental health awareness among college students has garnered increasing attention due to the prevalence of mental health issues in this demographic. This study aims to explore the efficacy of social media campaigns in promoting mental health awareness among college students. Leveraging a mixed-methods approach, both quantitative and qualitative data will be gathered to comprehensively assess the impact of social media campaigns on students' knowledge, attitudes, and behaviors regarding mental health. The quantitative aspect of the study will involve surveying a diverse sample of college students to measure their exposure to mental health-related content on social media platforms, their perceptions of the effectiveness of such campaigns, and their attitudes towards seeking help for mental health concerns. Additionally, the survey will assess changes in students' knowledge of mental health resources and their willingness to engage in supportive behaviors towards peers experiencing mental health challenges. Complementing the quantitative findings, qualitative data will be collected through in-depth interviews or focus group discussions with a subset of participants. This qualitative inquiry will delve deeper into students' experiences with social media campaigns related to mental health, exploring the specific content, platforms, and messaging strategies that resonate most with them. Moreover, these qualitative insights will shed light on the perceived barriers and facilitators to engaging with mental health-related content on social media, as well as the potential unintended consequences of such campaigns. Through the synthesis of quantitative and qualitative data, this study seeks to provide a nuanced understanding of the role that social media campaigns play in shaping mental health awareness among college students. Findings from this research will not only contribute to the academic literature on mental health promotion but also inform the development of more effective and culturally sensitive social media interventions aimed at supporting the mental well-being of college students."
https://doi.org/10.31234/osf.io/a4beh,2024-03-11,Influence of Social Media Campaigns on College Students  Mental Health Awareness,"Terry Tera, Kelly Kelvin, Bamigboye Tobiloba","Mental health awareness among college students has garnered increasing attention due to the prevalence of mental health issues in this demographic. This study aims to explore the efficacy of social media campaigns in promoting mental health awareness among college students. Leveraging a mixed-methods approach, both quantitative and qualitative data will be gathered to comprehensively assess the impact of social media campaigns on students' knowledge, attitudes, and behaviors regarding mental health. The quantitative aspect of the study will involve surveying a diverse sample of college students to measure their exposure to mental health-related content on social media platforms, their perceptions of the effectiveness of such campaigns, and their attitudes towards seeking help for mental health concerns. Additionally, the survey will assess changes in students' knowledge of mental health resources and their willingness to engage in supportive behaviors towards peers experiencing mental health challenges. Complementing the quantitative findings, qualitative data will be collected through in-depth interviews or focus group discussions with a subset of participants. This qualitative inquiry will delve deeper into students' experiences with social media campaigns related to mental health, exploring the specific content, platforms, and messaging strategies that resonate most with them. Moreover, these qualitative insights will shed light on the perceived barriers and facilitators to engaging with mental health-related content on social media, as well as the potential unintended consequences of such campaigns. Through the synthesis of quantitative and qualitative data, this study seeks to provide a nuanced understanding of the role that social media campaigns play in shaping mental health awareness among college students. Findings from this research will not only contribute to the academic literature on mental health promotion but also inform the development of more effective and culturally sensitive social media interventions aimed at supporting the mental well-being of college students."
https://doi.org/10.31234/osf.io/948fg,2024-03-12,Overt and Subtle Discrimination and Psychological Well-Being: Examining the Mediating and Moderating Role of Ethnic-Racial Identity Among Emerging Adults,"Tugce Aral, Chiara Ceccon, Elisabeth. L. de Moor, Yixin Tang, Mariëlle Osinga, Mariam Fishere, Moin Syed","Ethnic-racial identity (ERI) has been reported as mediator and moderator of the relation between discrimination and psychological well-being. However, it remains unclear how different forms of discrimination (i.e., overt and subtle) predict well-being over time, and whether ERI exploration and commitment mediate or moderate this association. This preregistered study explored the associations of overt and subtle discrimination with well-being (i.e., depression, substance use, life satisfaction) in a sample of 323 ethnic-racial minoritized college students (Mage W1 = 18.03, 62.7% female) from longitudinal data collected in the US. Cross-lagged panel models across three waves indicated no associations of overt discrimination, but participants experiencing more subtle discrimination during their transition to college reported more depressive symptoms after four months. ERI did not function as mediator or moderator. Findings indicate the need for a more nuanced understanding of the role of ERI during emerging adulthood."
https://doi.org/10.31234/osf.io/5vbef,2024-03-11,Efficacy of Solution-Focused Brief Therapy versus case management for psychological distress in help-seeking adolescents and young adults in Singapore: protocol for a randomised controlled trial,To be fetched,"Introduction: There are insufficient scalable, evidence-based treatments to meet the increasing mental health needs of young persons in Singapore. Offering interim, brief interventions for distressed treatment-seekers can improve care access and mitigate adverse effects of long waiting times. This study proposes to test the efficacy of Solution-Focused Brief Therapy (SFBT), a strengths-based, goal-directed intervention, for psychological distress in help-seeking adolescents and young adults.  Methods: We will conduct a fully-powered, randomised, single-centre, two-armed, parallel, superiority, controlled trial. From September 2023 to March 2025, the study will recruit 124 participants (ages 16-30) presenting at a national youth mental health service in Singapore (CHAT, Centre of Excellence for Youth Mental Health) with clinically-assessed non-specific psychological distress, subthreshold or prodromal symptoms, or a first episode of a mood disorder. Participants will be excluded if they have high suicidal risk, psychosis, cognitive impairments, or current psychological treatments. Participants will be randomised in a 1:1 ratio to receive six-session, case manager-delivered SFBT or treatment as usual (TAU), case management. Participants receiving SFBT are hypothesized to have greater improvements in self-reported psychological distress, from baseline to eight weeks, compared to the control group. Secondary outcomes are self-reported depression and anxiety symptoms, and functional impairment. The study will additionally explore if SFBT is associated with: 1) increased self-efficacy and decreased hopelessness; 2) decreased downstream referrals at post-intervention; and 3) sustained clinical gains at three months post-intervention, compared to TAU.   Significance: Results have implications for expanding capacity of mental health services and enhancing capabilities of frontline case managers to provide timely and low-intensity evidence-based interventions to improve the mental health of young persons in Singapore."
https://doi.org/10.31234/osf.io/5vbef,2024-03-11,Efficacy of Solution-Focused Brief Therapy versus case management for psychological distress in help-seeking adolescents and young adults in Singapore: protocol for a randomised controlled trial,"Cheryl Yunn Shee Foo, Hui Tianyi, Nur Khairunisa Binte Ngaiman, Darshan s/o Dahjalarrajah, Chua Yi Chian, Lee Yi Ping, Edimansyah Abdin, Janhavi Vaingankar, Charmaine Tang Yu Zheng","Introduction: There are insufficient scalable, evidence-based treatments to meet the increasing mental health needs of young persons in Singapore. Offering interim, brief interventions for distressed treatment-seekers can improve care access and mitigate adverse effects of long waiting times. This study proposes to test the efficacy of Solution-Focused Brief Therapy (SFBT), a strengths-based, goal-directed intervention, for psychological distress in help-seeking adolescents and young adults.  Methods: We will conduct a fully-powered, randomised, single-centre, two-armed, parallel, superiority, controlled trial. From September 2023 to March 2025, the study will recruit 124 participants (ages 16-30) presenting at a national youth mental health service in Singapore (CHAT, Centre of Excellence for Youth Mental Health) with clinically-assessed non-specific psychological distress, subthreshold or prodromal symptoms, or a first episode of a mood disorder. Participants will be excluded if they have high suicidal risk, psychosis, cognitive impairments, or current psychological treatments. Participants will be randomised in a 1:1 ratio to receive six-session, case manager-delivered SFBT or treatment as usual (TAU), case management. Participants receiving SFBT are hypothesized to have greater improvements in self-reported psychological distress, from baseline to eight weeks, compared to the control group. Secondary outcomes are self-reported depression and anxiety symptoms, and functional impairment. The study will additionally explore if SFBT is associated with: 1) increased self-efficacy and decreased hopelessness; 2) decreased downstream referrals at post-intervention; and 3) sustained clinical gains at three months post-intervention, compared to TAU.   Significance: Results have implications for expanding capacity of mental health services and enhancing capabilities of frontline case managers to provide timely and low-intensity evidence-based interventions to improve the mental health of young persons in Singapore."
https://doi.org/10.31234/osf.io/qzdbu,2024-03-08,Emotion is a multi-componential experience guided by appraisal: evidence from multi-level annotation during naturalistic stimulation,To be fetched,"This study discerns the relationship between discrete emotions and their underlying components from a detailed dataset of continuous annotations of more than 50 emotion variables during short films. Appraisal theories predict that discrete emotions arise from a combination of components. Specifically, the Component Process Model (CPM) highlights the prime role of appraisal following motivation, expression, physiology and feeling. We include annotations from all these domains and reveal a hierarchical organisation of discrete emotions by appraisal of valence and self-relevance. Furthermore, we apply predictive models to understand the contribution of emotion components to discrete emotions. We find that all 13 discrete emotions in our dataset can be significantly predicted as a function of emotion components. Our study contributes key insights using machine learning to the longstanding question of what is an emotion and underscores the centrality of appraisal in the generation of emotion. This has important implications on the complexity and function of emotion as an adaptive process."
https://doi.org/10.31234/osf.io/qzdbu,2024-03-08,Emotion is a multi-componential experience guided by appraisal: evidence from multi-level annotation during naturalistic stimulation,"Elenor Morgenroth, Rukshani Somarathna, Dimitri Van De Ville, Gelareh Mohammadi, Patrik Vuilleumier","This study discerns the relationship between discrete emotions and their underlying components from a detailed dataset of continuous annotations of more than 50 emotion variables during short films. Appraisal theories predict that discrete emotions arise from a combination of components. Specifically, the Component Process Model (CPM) highlights the prime role of appraisal following motivation, expression, physiology and feeling. We include annotations from all these domains and reveal a hierarchical organisation of discrete emotions by appraisal of valence and self-relevance. Furthermore, we apply predictive models to understand the contribution of emotion components to discrete emotions. We find that all 13 discrete emotions in our dataset can be significantly predicted as a function of emotion components. Our study contributes key insights using machine learning to the longstanding question of what is an emotion and underscores the centrality of appraisal in the generation of emotion. This has important implications on the complexity and function of emotion as an adaptive process."
https://doi.org/10.31234/osf.io/kp8yx,2024-03-07,Hierarchische Taxonomie der Psychopathologie (HiTOP): Ein neues Modell zur Beschreibung psychischer Probleme,To be fetched,"Aktuelle Klassifikationssysteme für psychische Störungen wie das DSM-5 und ICD-11 stehen zunehmend in der Kritik, die Komplexität psychischer Probleme nicht adäquat abzubilden. Ungünstig sind insbesondere die implizite Orientierung am medizinischen Krankheitsmodell, die mangelnde strukturelle Validität sowie die kategoriale Abgrenzung zu psychischer Gesundheit. Als Antwort auf diese Herausforderungen wird aktuell im Rahmen eines Konsortiums eine Hierarchische Taxonomie der Psychopathologie (HiTOP) entwickelt – ein empirisch fundiertes, hierarchisch organisiertes, und dimensionales Modell zur Beschreibung psychischer Probleme. In diesem Artikel wird das HiTOP-Modell erstmals in umfassender Weise für den deutschsprachigen Raum zugänglich gemacht. Nach einer Kritik an aktuellen Klassifikationssystemen erläutern wir zunächst die methodischen Grundprinzien von HiTOP und stellen das aktuell gültige Modell vor. Anschließend berichten wird von neuen Studien und Überlegungen zur Validierung und Weiterentwicklung des Modells, geben einen Einblick in die Entwicklung eines umfassenden Selbsteinschätzungsfragebogens zur Erfassung der HiTOP-Dimensionen und veranschaulichen den Einsatz von HiTOP in der diagnostischen Praxis. In der Diskussion klären wir Bezüge zu alternativen Ansätzen zur Konzeptualisierung psychischer Störungen, arbeiten die Implikationen von HiTOP für zukünftige Forschung und Praxis heraus und thematisieren Forschungslücken und Limitationen."
https://doi.org/10.31234/osf.io/cxabn,2024-03-07,Intergroup Exclusion Elicits Levels of Distress and Hostility Similar to Interpersonal Exclusion,To be fetched,"Marginalizing social groups has been identified as one of the main drivers of violent extremism across countries. However, most psychological research has focused on interpersonal rather than intergroup effects to understand the link between social exclusion and aggression. We compared the effects of intergroup and interpersonal using RateME, a new task designed to dissociate both types of exclusion, and Cyberball in a sample of 1200 UK residents. Intergroup exclusion elicited by RateME and interpersonal exclusion elicited by both RateME and Cyberball were independently associated with increased psychological distress and state hostility, regardless of participants’ degree of identification with the group. Exclusion of either type also increased collective narcissism, especially in participants who had experienced higher levels of psychological distress. Our work reveals similar detrimental psychological effects of intergroup exclusion compared to interpersonal exclusion, highlighting intergroup exclusion as a risk factor for mental health with potential implications for violent extremism."
https://doi.org/10.31234/osf.io/qhf43,2024-03-07,The role of lack of grandparental support in perinatal depression,To be fetched,"Background: A lack of social support has been identified as a risk factor for perinatal mental health problems. However, previous studies mainly focused on support from partner or general social support and neglected the roles of grandparents. Here, we examine whether a lack of grandparental support is related to increased risk of a diagnosis of perinatal depression. In addition, we examine whether poor grandparental support is related to more depressive symptoms in mothers with and without perinatal depression and whether perceived grandparental support buffers against parenting difficulties in mothers with perinatal depression. Methods: The sample was drawn from an Australian pregnancy cohort study and consisted of 725 women, including 230 women who met criteria for Major Depression. At 12 months postpartum, women reported on grandparental geographical proximity and hours of grandparental childcare support. Perceived grandparental support was assessed with the Postpartum Social Support Questionnaire and parenting difficulties and depressive symptoms with the Parenting Stress Index and the Edinburgh Postnatal Depression Scale. Results: Perceived grandparental support was related to fewer depressive symptoms among mothers with perinatal depression. In addition, higher levels of perceived grandparental support were related to lower parenting stress in mothers with and without perinatal depression. Limitations: Intergenerational conflicts and quality of grandparenting were not assessed. Conclusions: Our findings indicate that supportive grandparents may prevent the development of more severe perinatal depression in mothers experiencing perinatal mental health problems. Future studies should examine whether involving grandparents in treatment may add to the effectiveness of existing perinatal mental health interventions."
https://doi.org/10.31234/osf.io/qhf43,2024-03-07,The role of lack of grandparental support in perinatal depression,"Madelon Riem, Kelsey Perrykkad, Stuart J. Watson, Karen Wynter, Marinus H. van IJzendoorn, Megan Galbally","Background: A lack of social support has been identified as a risk factor for perinatal mental health problems. However, previous studies mainly focused on support from partner or general social support and neglected the roles of grandparents. Here, we examine whether a lack of grandparental support is related to increased risk of a diagnosis of perinatal depression. In addition, we examine whether poor grandparental support is related to more depressive symptoms in mothers with and without perinatal depression and whether perceived grandparental support buffers against parenting difficulties in mothers with perinatal depression. Methods: The sample was drawn from an Australian pregnancy cohort study and consisted of 725 women, including 230 women who met criteria for Major Depression. At 12 months postpartum, women reported on grandparental geographical proximity and hours of grandparental childcare support. Perceived grandparental support was assessed with the Postpartum Social Support Questionnaire and parenting difficulties and depressive symptoms with the Parenting Stress Index and the Edinburgh Postnatal Depression Scale. Results: Perceived grandparental support was related to fewer depressive symptoms among mothers with perinatal depression. In addition, higher levels of perceived grandparental support were related to lower parenting stress in mothers with and without perinatal depression. Limitations: Intergenerational conflicts and quality of grandparenting were not assessed. Conclusions: Our findings indicate that supportive grandparents may prevent the development of more severe perinatal depression in mothers experiencing perinatal mental health problems. Future studies should examine whether involving grandparents in treatment may add to the effectiveness of existing perinatal mental health interventions."
https://doi.org/10.31234/osf.io/cxabn,2024-03-07,Intergroup Exclusion Elicits Levels of Distress and Hostility Similar to Interpersonal Exclusion,"Luis Marcos Vidal, Boryana Todorova, Clara Pretus","Marginalizing social groups has been identified as one of the main drivers of violent extremism across countries. However, most psychological research has focused on interpersonal rather than intergroup effects to understand the link between social exclusion and aggression. We compared the effects of intergroup and interpersonal using RateME, a new task designed to dissociate both types of exclusion, and Cyberball in a sample of 1200 UK residents. Intergroup exclusion elicited by RateME and interpersonal exclusion elicited by both RateME and Cyberball were independently associated with increased psychological distress and state hostility, regardless of participants’ degree of identification with the group. Exclusion of either type also increased collective narcissism, especially in participants who had experienced higher levels of psychological distress. Our work reveals similar detrimental psychological effects of intergroup exclusion compared to interpersonal exclusion, highlighting intergroup exclusion as a risk factor for mental health with potential implications for violent extremism."
https://doi.org/10.31234/osf.io/kp8yx,2024-03-07,Hierarchische Taxonomie der Psychopathologie (HiTOP): Ein neues Modell zur Beschreibung psychischer Probleme,"Johannes Zimmermann, Michael Witthöft, David C Cicero, Miriam K. Forbes, Chris Hopwood, Roman Kotov, Robert Krueger, Martin Sellbom, Leonard Simms, Aidan G.C. Wright","Aktuelle Klassifikationssysteme für psychische Störungen wie das DSM-5 und ICD-11 stehen zunehmend in der Kritik, die Komplexität psychischer Probleme nicht adäquat abzubilden. Ungünstig sind insbesondere die implizite Orientierung am medizinischen Krankheitsmodell, die mangelnde strukturelle Validität sowie die kategoriale Abgrenzung zu psychischer Gesundheit. Als Antwort auf diese Herausforderungen wird aktuell im Rahmen eines Konsortiums eine Hierarchische Taxonomie der Psychopathologie (HiTOP) entwickelt – ein empirisch fundiertes, hierarchisch organisiertes, und dimensionales Modell zur Beschreibung psychischer Probleme. In diesem Artikel wird das HiTOP-Modell erstmals in umfassender Weise für den deutschsprachigen Raum zugänglich gemacht. Nach einer Kritik an aktuellen Klassifikationssystemen erläutern wir zunächst die methodischen Grundprinzien von HiTOP und stellen das aktuell gültige Modell vor. Anschließend berichten wird von neuen Studien und Überlegungen zur Validierung und Weiterentwicklung des Modells, geben einen Einblick in die Entwicklung eines umfassenden Selbsteinschätzungsfragebogens zur Erfassung der HiTOP-Dimensionen und veranschaulichen den Einsatz von HiTOP in der diagnostischen Praxis. In der Diskussion klären wir Bezüge zu alternativen Ansätzen zur Konzeptualisierung psychischer Störungen, arbeiten die Implikationen von HiTOP für zukünftige Forschung und Praxis heraus und thematisieren Forschungslücken und Limitationen."
https://doi.org/10.31234/osf.io/nu76j,2024-03-06,Students’ emotional experiences with climate change and how universities can help,To be fetched,"Research suggests that a large majority of young people in the UK experience worry and negative emotions about climate change, which affects their functioning in daily life. University students may be particularly likely to experience climate anxiety and worry if, for example, exposed to distressing climate change content in their studies. In a pre-registered online mixed-methods study, we investigated climate anxiety, climate change-related emotions, thoughts, and views about their university’s role in climate change action among 869 students at a large UK university. Results showed that students generally experienced moderate climate anxiety intensity across different situations. Students also reported high levels of negative emotions, including sadness, helplessness, and powerlessness, and low optimism and indifference. Students also experienced moderate to high levels of negative climate change related thoughts, such as “The future is frightening”. As for their university’s role, many students wanted more climate change related teaching and mental health support for climate change impacts. On average, students endorsed the thought that their university was “Dismissing people’s distress” about climate change moderately, which correlated significantly with students’ situationally assessed climate anxiety intensity (r = .32, p &lt; .01) and general climate anxiety frequency (r = .30, p &lt; .01). These results demonstrate the serious negative impact of climate change on university students’ mental health. They also highlight the importance of universities recognising their responsibilities in climate action and protecting students’ wellbeing."
https://doi.org/10.31234/osf.io/nu76j,2024-03-06,Students’ emotional experiences with climate change and how universities can help,"Chiara Hill-Harding, Lawrence Barsalou, Esther K. Papies","Research suggests that a large majority of young people in the UK experience worry and negative emotions about climate change, which affects their functioning in daily life. University students may be particularly likely to experience climate anxiety and worry if, for example, exposed to distressing climate change content in their studies. In a pre-registered online mixed-methods study, we investigated climate anxiety, climate change-related emotions, thoughts, and views about their university’s role in climate change action among 869 students at a large UK university. Results showed that students generally experienced moderate climate anxiety intensity across different situations. Students also reported high levels of negative emotions, including sadness, helplessness, and powerlessness, and low optimism and indifference. Students also experienced moderate to high levels of negative climate change related thoughts, such as “The future is frightening”. As for their university’s role, many students wanted more climate change related teaching and mental health support for climate change impacts. On average, students endorsed the thought that their university was “Dismissing people’s distress” about climate change moderately, which correlated significantly with students’ situationally assessed climate anxiety intensity (r = .32, p &lt; .01) and general climate anxiety frequency (r = .30, p &lt; .01). These results demonstrate the serious negative impact of climate change on university students’ mental health. They also highlight the importance of universities recognising their responsibilities in climate action and protecting students’ wellbeing."
https://doi.org/10.31234/osf.io/9v7xa,2024-03-05,"Associations between intolerance of uncertainty, paranoia, anxiety and depression:  Evidence from an international multi-site sample",To be fetched,"Intolerance of uncertainty (IU: the tendency to find uncertainty aversive) and paranoia (e.g., excessive mistrust of others), are both associated with anxiety and depression symptoms.  While previous research has primarily focused on IU and paranoia separately, there is recent evidence to suggest that IU and paranoia are linked and may interact to increase risk for anxiety, depression, and schizophrenia-spectrum conditions. The aims of the current study were to assess: (1) the extent to which IU (total score and subscales), paranoia, anxiety, and depression are associated and (2) whether the interaction between IU and paranoia is associated with greater anxiety and depression symptoms. To examine these aims, we conducted a survey on an international multi-site sample (n = 2510). Questionnaires included: IU (total score and subscales), paranoia (RGPTS persecution subscale), anxiety, and depression.  The findings revealed that: (1) IU was positively associated with paranoia (r = .43), anxiety (r = .48) and depression (r = .49), and (2) People with high scores on IU and paranoia showed higher anxiety and depression symptoms. Importantly, these effects remained when controlling for negative beliefs about the self and others and demographic factors. Additionally, the inhibitory IU subscale (uncertainty paralysis) was related to paranoia, anxiety, and depression. However, the prospective IU subscale (desire for predictability) was only related to anxiety, but not paranoia and depression. Overall, these findings reliably demonstrate that IU and paranoia are linked, and that IU and paranoia interactions may synergistically work to affect current levels of anxiety and depression symptoms."
https://doi.org/10.31234/osf.io/9v7xa,2024-03-05,"Associations between intolerance of uncertainty, paranoia, anxiety and depression:  Evidence from an international multi-site sample","Jayne Morriss, Brandon Gaudiano, Suzanne So, Jess L Kingston, Tania Marie Lincoln, Eric M. J. Morris, Lyn Ellett","Intolerance of uncertainty (IU: the tendency to find uncertainty aversive) and paranoia (e.g., excessive mistrust of others), are both associated with anxiety and depression symptoms.  While previous research has primarily focused on IU and paranoia separately, there is recent evidence to suggest that IU and paranoia are linked and may interact to increase risk for anxiety, depression, and schizophrenia-spectrum conditions. The aims of the current study were to assess: (1) the extent to which IU (total score and subscales), paranoia, anxiety, and depression are associated and (2) whether the interaction between IU and paranoia is associated with greater anxiety and depression symptoms. To examine these aims, we conducted a survey on an international multi-site sample (n = 2510). Questionnaires included: IU (total score and subscales), paranoia (RGPTS persecution subscale), anxiety, and depression.  The findings revealed that: (1) IU was positively associated with paranoia (r = .43), anxiety (r = .48) and depression (r = .49), and (2) People with high scores on IU and paranoia showed higher anxiety and depression symptoms. Importantly, these effects remained when controlling for negative beliefs about the self and others and demographic factors. Additionally, the inhibitory IU subscale (uncertainty paralysis) was related to paranoia, anxiety, and depression. However, the prospective IU subscale (desire for predictability) was only related to anxiety, but not paranoia and depression. Overall, these findings reliably demonstrate that IU and paranoia are linked, and that IU and paranoia interactions may synergistically work to affect current levels of anxiety and depression symptoms."
https://doi.org/10.31234/osf.io/a6zrs,2024-03-05,Postpartum Intrusions are Associated with Child and Maternal Mental Health,To be fetched,"Background: Postpartum intrusive thoughts are common in new mothers, close to 100% report thoughts of accidental harm befalling their child, and approximately 50% report intrusive thoughts of intentionally harming their child1. These thoughts have been associated with maternal anxiety and depression2. However, their association with infant mental wellbeing is little understood. Methods: 246 female participants (mean age = 32 years)who were pregnant during the first wave (May-Sep 2020) of the COVID-19 Risks Across the Lifespan study and who completed the final assessment (Oct 2021-April 2022) were included in the present analyses. Results: Maternal postpartum intrusions were significantly associated with maternal depression and anxiety symptoms as well as child negative affectivity. Sensitivity analyses revealed that the associations with both maternal and infant mental health remained significant after controlling for antenatal stress, antenatal depression and anxiety, and infant age; as well as maternal postpartum distress, depression, and anxiety for the infant negative affectivity analyses. Conclusion: These findings suggest that reducing postpartum intrusions is a clinical priority for paediatric and women’s mental health."
https://doi.org/10.31234/osf.io/a6zrs,2024-03-05,Postpartum Intrusions are Associated with Child and Maternal Mental Health,"Susanne Schweizer, Sarah Daniels","Background: Postpartum intrusive thoughts are common in new mothers, close to 100% report thoughts of accidental harm befalling their child, and approximately 50% report intrusive thoughts of intentionally harming their child1. These thoughts have been associated with maternal anxiety and depression2. However, their association with infant mental wellbeing is little understood. Methods: 246 female participants (mean age = 32 years)who were pregnant during the first wave (May-Sep 2020) of the COVID-19 Risks Across the Lifespan study and who completed the final assessment (Oct 2021-April 2022) were included in the present analyses. Results: Maternal postpartum intrusions were significantly associated with maternal depression and anxiety symptoms as well as child negative affectivity. Sensitivity analyses revealed that the associations with both maternal and infant mental health remained significant after controlling for antenatal stress, antenatal depression and anxiety, and infant age; as well as maternal postpartum distress, depression, and anxiety for the infant negative affectivity analyses. Conclusion: These findings suggest that reducing postpartum intrusions is a clinical priority for paediatric and women’s mental health."
https://doi.org/10.31234/osf.io/392an,2024-03-04,Framing for action? Assessing microplastic-related threat potential for planetary health as a political participation catalyzer,To be fetched,"As the 1.5-degree goal threatens to grow more distant, the need to address human behavior in strategies to mitigate climate change further increases. With recent evidence indicating harmful effects of microplastic exposure to human and natural systems, we tested based on Rogers’ Protection Motivation Theory whether planetary health framing, which is considered a promising tool for climate change communication, is suitable for raising concerns about the issue. Following a mixed-methods design, we employed a  2 x 2 between-subjects vignette experiment (N = 898) to explore the connection between planetary health framing and one’s intention to participate politically across different levels of psychological distance, while strengthening the understanding of the behavior itself. Therefore, we subjected 1,992 open-text responses to a qualitative analysis assessing which dimensions of political participation are considered relevant, and what obstacles populate the gap between intention to participate and actual participation.  The experimental data shows that varying the planetary health condition does not result in significant differences, whereas distance framing showed an inversed effect pattern contrary to what our hypotheses expected. This highlights the urgent need for further research on planetary health as a framing subject. The qualitative findings on behavior dimensions provide an enriched proposal for adapting existing instruments to measure political participation in a variety of public life contexts. Data on barriers to political participation show various internal and external reasons that can be used to develop a scale for response costs, which could lead to a more comprehensive model of political participation readiness in the context of planetary health framing."
https://doi.org/10.31234/osf.io/392an,2024-03-04,Framing for action? Assessing microplastic-related threat potential for planetary health as a political participation catalyzer,"Robert Bruckmann, Janina Huber, Kira Maur","As the 1.5-degree goal threatens to grow more distant, the need to address human behavior in strategies to mitigate climate change further increases. With recent evidence indicating harmful effects of microplastic exposure to human and natural systems, we tested based on Rogers’ Protection Motivation Theory whether planetary health framing, which is considered a promising tool for climate change communication, is suitable for raising concerns about the issue. Following a mixed-methods design, we employed a  2 x 2 between-subjects vignette experiment (N = 898) to explore the connection between planetary health framing and one’s intention to participate politically across different levels of psychological distance, while strengthening the understanding of the behavior itself. Therefore, we subjected 1,992 open-text responses to a qualitative analysis assessing which dimensions of political participation are considered relevant, and what obstacles populate the gap between intention to participate and actual participation.  The experimental data shows that varying the planetary health condition does not result in significant differences, whereas distance framing showed an inversed effect pattern contrary to what our hypotheses expected. This highlights the urgent need for further research on planetary health as a framing subject. The qualitative findings on behavior dimensions provide an enriched proposal for adapting existing instruments to measure political participation in a variety of public life contexts. Data on barriers to political participation show various internal and external reasons that can be used to develop a scale for response costs, which could lead to a more comprehensive model of political participation readiness in the context of planetary health framing."
https://doi.org/10.31234/osf.io/aqty2,2024-03-03,Psychological Factors that Influence Behavioral Intentions towards Bottled Green Tea in the Japan Market,To be fetched,"What factors influence consumer choice is an ongoing debate. This study delved into Japanese consumers’ perceptual model of bottled green tea beverages in the fiercely competitive market, exploring the psychological factors that lead to behavioral intentions using an extended Mehrabian and Russell Stimulus-Organism-Response model. The model extension incorporated a panoply of factors that we laid against behavioral intentions, among which we explored ones with significant influence. In Study 1, we executed a screening survey, and after surveying 381 valid participants, we identified factors significant to purchase intent and built a model via structural equation modeling. In Study 2, we validated the model, analyzing how Japanese consumers make purchase decisions in a commoditized marketplace with many beverage options. Our results demonstrated that while a variety of factors would be related to Behavioral Intention, most notable among them was Reward, comprising facets such as taste, satisfaction, addictiveness, and desire. It was further affected by Mood, Perceived Quality, and Nostalgia. Our findings bring to light the importance of hedonic factors as they drive consumer choice by emphasizing the intrinsic value of sensory enjoyment and gratification in the experience of consuming green tea beverages. This study would provide practical marketing insight as to how bottled green tea, and possibly other beverage products, in a red ocean market, can be positioned and marketed to Japanese consumers in the most effective way."
https://doi.org/10.31234/osf.io/aqty2,2024-03-03,Psychological Factors that Influence Behavioral Intentions towards Bottled Green Tea in the Japan Market,"Christopher Demetrakos, Shannen Romero-Perez, Itsuki Nakato, Yasushi Kyutoku, Ippeita Dan","What factors influence consumer choice is an ongoing debate. This study delved into Japanese consumers’ perceptual model of bottled green tea beverages in the fiercely competitive market, exploring the psychological factors that lead to behavioral intentions using an extended Mehrabian and Russell Stimulus-Organism-Response model. The model extension incorporated a panoply of factors that we laid against behavioral intentions, among which we explored ones with significant influence. In Study 1, we executed a screening survey, and after surveying 381 valid participants, we identified factors significant to purchase intent and built a model via structural equation modeling. In Study 2, we validated the model, analyzing how Japanese consumers make purchase decisions in a commoditized marketplace with many beverage options. Our results demonstrated that while a variety of factors would be related to Behavioral Intention, most notable among them was Reward, comprising facets such as taste, satisfaction, addictiveness, and desire. It was further affected by Mood, Perceived Quality, and Nostalgia. Our findings bring to light the importance of hedonic factors as they drive consumer choice by emphasizing the intrinsic value of sensory enjoyment and gratification in the experience of consuming green tea beverages. This study would provide practical marketing insight as to how bottled green tea, and possibly other beverage products, in a red ocean market, can be positioned and marketed to Japanese consumers in the most effective way."
https://doi.org/10.31234/osf.io/7vjd9,2024-03-01,Religious Identity Development and Psychological Adjustment Among Muslim Adolescents: Results from the Identity Project Intervention in Germany,To be fetched,"Identity formation constitutes a key developmental process during adolescence. This study extends previous research on ethnic identity development by focusing on the religious identity development of Muslim youth in Germany. Using a longitudinal waitlist control group design, we tested if Muslim ethnic minority students (N = 128; Mage = 13.43, SDage = 0.84, 43% female) who participated in an 8-week school-based intervention, the Identity Project, would show greater exploration of their religious identity, leading to increases in their religious identity resolution and in turn resulting in better psychological adjustment (i.e., higher self-esteem, less depressive symptoms) and higher global identity. Preliminary analyses revealed that Muslim youth in our sample are characterized by high religious identity during adolescence, represented by high values on religious identity exploration and resolution. Latent-change score models demonstrated that greater changes in Muslim youth’s religious identity exploration predicted higher religious identity resolution, resulting in better self-acceptance and a higher sense of global identity. However, Muslim youth who showed a clear sense of their religious identity, did not indicate less affective and somatic depressive symptoms. We conclude that for Muslim youth religion seems to constitute an important part of their identity while promoting their positive adjustment. Our results indicate that multicultural approaches in schools – such as the Identity Project – should not only respond to students’ ethnic identities but should also more explicitly cater to their religious identities."
https://doi.org/10.31234/osf.io/rn2kd,2024-03-01,“They are my safe haven”: The Importance of Personal and Social Resources for Refugee Youth and Youth of Immigrant Descent Mastering Acculturative Challenges and Developmental Tasks,To be fetched,"Immigrant-origin youth in Germany are faced with various acculturative challenges, including discrimination, language, and sociocultural hassles. Constituting additional stress to normative developmental tasks in adolescence, immigrant-origin youth are in need of supportive resources fostering their well-being. Drawing on the risk and resilience framework and applying a strength-based perspective, the current study investigated refugee youth’s and youth of immigrant descents’ perspectives on their main challenges, tasks, and resources in Germany. The present study paid special attention to the interplay of acculturative challenges and developmental tasks in relation to the youth’s resources and their psychological well-being. Semi-structured interviews with six refugee youth and five second-generation youth of immigrant descent including nine boys and two girls between the ages of 14 – 16 (Mage = 15.45, SDage = 0.69) were analyzed. Using thematic analysis, data was structured in four themes: 1) acculturative challenges, 2) developmental tasks, 3) personal resources, and 4) social resources. Results highlight the importance of personal (incl., individual characteristics, identity, religion) and social (incl., supportive friends, family, teachers) resources to master the range of acculturative challenges (incl., language barriers, perceived ethnic and religious discrimination, cultural differences) and developmental tasks (incl., negotiation of autonomy, academic future planning). In addition, results indicate the importance of social resources for refugee youth and youth of immigrant descent when dealing with discriminatory experiences. Our findings offer practical implications for schools in terms of interventions focusing on both the strengthening of personal and social resources, and the reduction of challenges to promote immigrant-origin youth’s psychological well-being."
https://doi.org/10.31234/osf.io/rn2kd,2024-03-01,“They are my safe haven”: The Importance of Personal and Social Resources for Refugee Youth and Youth of Immigrant Descent Mastering Acculturative Challenges and Developmental Tasks,"Julia Marie Christina Wenzing, Lina Alhaddad, Maja Schachner, Sophie Hölscher","Immigrant-origin youth in Germany are faced with various acculturative challenges, including discrimination, language, and sociocultural hassles. Constituting additional stress to normative developmental tasks in adolescence, immigrant-origin youth are in need of supportive resources fostering their well-being. Drawing on the risk and resilience framework and applying a strength-based perspective, the current study investigated refugee youth’s and youth of immigrant descents’ perspectives on their main challenges, tasks, and resources in Germany. The present study paid special attention to the interplay of acculturative challenges and developmental tasks in relation to the youth’s resources and their psychological well-being. Semi-structured interviews with six refugee youth and five second-generation youth of immigrant descent including nine boys and two girls between the ages of 14 – 16 (Mage = 15.45, SDage = 0.69) were analyzed. Using thematic analysis, data was structured in four themes: 1) acculturative challenges, 2) developmental tasks, 3) personal resources, and 4) social resources. Results highlight the importance of personal (incl., individual characteristics, identity, religion) and social (incl., supportive friends, family, teachers) resources to master the range of acculturative challenges (incl., language barriers, perceived ethnic and religious discrimination, cultural differences) and developmental tasks (incl., negotiation of autonomy, academic future planning). In addition, results indicate the importance of social resources for refugee youth and youth of immigrant descent when dealing with discriminatory experiences. Our findings offer practical implications for schools in terms of interventions focusing on both the strengthening of personal and social resources, and the reduction of challenges to promote immigrant-origin youth’s psychological well-being."
https://doi.org/10.31234/osf.io/7vjd9,2024-03-01,Religious Identity Development and Psychological Adjustment Among Muslim Adolescents: Results from the Identity Project Intervention in Germany,"Julia Marie Christina Wenzing, Maja Schachner, Savaş Karataş, Linda P. Juang","Identity formation constitutes a key developmental process during adolescence. This study extends previous research on ethnic identity development by focusing on the religious identity development of Muslim youth in Germany. Using a longitudinal waitlist control group design, we tested if Muslim ethnic minority students (N = 128; Mage = 13.43, SDage = 0.84, 43% female) who participated in an 8-week school-based intervention, the Identity Project, would show greater exploration of their religious identity, leading to increases in their religious identity resolution and in turn resulting in better psychological adjustment (i.e., higher self-esteem, less depressive symptoms) and higher global identity. Preliminary analyses revealed that Muslim youth in our sample are characterized by high religious identity during adolescence, represented by high values on religious identity exploration and resolution. Latent-change score models demonstrated that greater changes in Muslim youth’s religious identity exploration predicted higher religious identity resolution, resulting in better self-acceptance and a higher sense of global identity. However, Muslim youth who showed a clear sense of their religious identity, did not indicate less affective and somatic depressive symptoms. We conclude that for Muslim youth religion seems to constitute an important part of their identity while promoting their positive adjustment. Our results indicate that multicultural approaches in schools – such as the Identity Project – should not only respond to students’ ethnic identities but should also more explicitly cater to their religious identities."
https://doi.org/10.31234/osf.io/2jz6t,2024-02-29,COVID-19-Related Stress Scale: Development and Initial Validation,To be fetched,"Background The COVID-19 pandemic had an extensive impact on people’s personal and professional lives across the world. Psychologists, among other professionals, were required to suddenly work remotely via telehealth during a period of global uncertainty, illness and mortality, community anxiety and lockdowns. This drastically altered the psychology practice, with the effects on clients and professionals still not fully understood. Aim To develop and validate a COVID-19-Related Stress Scale to evaluate psychologists’ experiences relating to the pandemic.  Methods The items of the COVID-19-Related Stress Scale were constructed based on an extensive qualitative report from the British Psychological Society, which identified key themes in psychologists’ pandemic experiences. Eight items were initially developed. To determine construct validity, exploratory factor analysis (EFA) was conducted, as well as tests for convergent and discriminant validity.  Results The sample comprised 99 psychologists in Aotearoa, New Zealand. After corrected item-total correlation testing, five items with significant and strong factor loadings (0.60 to 0.74) and two items with acceptable corrected item-total correlation (0.54 to 0.56) were confirmed, resulting in a scale of seven items.  Cronbach’s alpha for internal consistency was α = .83, supporting a single-factor model (COVID-related stress, CVRS). A moderate relationship was found between CVRS and stress and compassion fatigue, validating its utility as a measure of stress. In contrast, no relationship was seen with resilience as a discriminant construct. Conclusion The Covid-19-related stress scale shows robust psychometric properties, which usefully assessed the impact of COVID-19 among psychologists in NZ. Although the lockdown phase of the pandemic has since ended, this measure provides valuable insights into the retrospective experiences of healthcare professionals during the pandemic. It may be adapted for unexpected scenarios requiring remote therapy, such as pandemics and natural disasters. The effect of remote work and community stress on mental health professionals requires ongoing consideration, and we propose that the CVRS is a useful tool for this research."
https://doi.org/10.31234/osf.io/2jz6t,2024-02-29,COVID-19-Related Stress Scale: Development and Initial Validation,"Jodie Rahman, Amy Kercher, Mangor Pedersen","Background The COVID-19 pandemic had an extensive impact on people’s personal and professional lives across the world. Psychologists, among other professionals, were required to suddenly work remotely via telehealth during a period of global uncertainty, illness and mortality, community anxiety and lockdowns. This drastically altered the psychology practice, with the effects on clients and professionals still not fully understood. Aim To develop and validate a COVID-19-Related Stress Scale to evaluate psychologists’ experiences relating to the pandemic.  Methods The items of the COVID-19-Related Stress Scale were constructed based on an extensive qualitative report from the British Psychological Society, which identified key themes in psychologists’ pandemic experiences. Eight items were initially developed. To determine construct validity, exploratory factor analysis (EFA) was conducted, as well as tests for convergent and discriminant validity.  Results The sample comprised 99 psychologists in Aotearoa, New Zealand. After corrected item-total correlation testing, five items with significant and strong factor loadings (0.60 to 0.74) and two items with acceptable corrected item-total correlation (0.54 to 0.56) were confirmed, resulting in a scale of seven items.  Cronbach’s alpha for internal consistency was α = .83, supporting a single-factor model (COVID-related stress, CVRS). A moderate relationship was found between CVRS and stress and compassion fatigue, validating its utility as a measure of stress. In contrast, no relationship was seen with resilience as a discriminant construct. Conclusion The Covid-19-related stress scale shows robust psychometric properties, which usefully assessed the impact of COVID-19 among psychologists in NZ. Although the lockdown phase of the pandemic has since ended, this measure provides valuable insights into the retrospective experiences of healthcare professionals during the pandemic. It may be adapted for unexpected scenarios requiring remote therapy, such as pandemics and natural disasters. The effect of remote work and community stress on mental health professionals requires ongoing consideration, and we propose that the CVRS is a useful tool for this research."
https://doi.org/10.31234/osf.io/2vga5,2024-02-29,Pathways to adolescent social anxiety: Testing interactions between neural social reward function and perceived social threat in daily life,To be fetched,"Recent theories suggest that for youth highly sensitive to incentives, perceiving more social threat may contribute to social anxiety (SA) symptoms. In 129 girls (ages 11-13) oversampled for shy/fearful temperament, we thus examined how interactions between neural responses to social reward (vs. neutral) cues (measured during anticipation of peer feedback) and perceived social threat in daily peer interactions (measured using ecological momentary assessment) predict SA symptoms two years later. No significant interactions emerged when neural reward function was modeled as a latent factor. Secondary analyses showed that higher perceived social threat was associated with more severe SA symptoms two years later only for girls with higher basolateral amygdala (BLA) activation to social reward cues at baseline. Interaction effects were specific to BLA activation to social reward (not threat) cues, though a main effect of BLA activation to social threat (vs. neutral) cues on SA emerged. Unexpectedly, interactions between social threat and BLA activation to social reward cues also predicted generalized anxiety and depression symptoms two years later, suggesting possible transdiagnostic risk pathways. Perceiving high social threat may be particularly detrimental for youth highly sensitive to reward incentives, potentially due to mediating reward learning processes, though this remains to be tested."
https://doi.org/10.31234/osf.io/2vga5,2024-02-29,Pathways to adolescent social anxiety: Testing interactions between neural social reward function and perceived social threat in daily life,"Stefanie Sequeira, Jennifer Silk, Neil Jones, Erika Forbes, Jamie L Hanson, Lauren S. Hallion, Cecile D. Ladouceur","Recent theories suggest that for youth highly sensitive to incentives, perceiving more social threat may contribute to social anxiety (SA) symptoms. In 129 girls (ages 11-13) oversampled for shy/fearful temperament, we thus examined how interactions between neural responses to social reward (vs. neutral) cues (measured during anticipation of peer feedback) and perceived social threat in daily peer interactions (measured using ecological momentary assessment) predict SA symptoms two years later. No significant interactions emerged when neural reward function was modeled as a latent factor. Secondary analyses showed that higher perceived social threat was associated with more severe SA symptoms two years later only for girls with higher basolateral amygdala (BLA) activation to social reward cues at baseline. Interaction effects were specific to BLA activation to social reward (not threat) cues, though a main effect of BLA activation to social threat (vs. neutral) cues on SA emerged. Unexpectedly, interactions between social threat and BLA activation to social reward cues also predicted generalized anxiety and depression symptoms two years later, suggesting possible transdiagnostic risk pathways. Perceiving high social threat may be particularly detrimental for youth highly sensitive to reward incentives, potentially due to mediating reward learning processes, though this remains to be tested."
https://doi.org/10.31234/osf.io/wd4tr,2024-02-28,The Digital Divide in Action: How Experiences of Digital Technology Shape Future Relationships with Artificial Intelligence,To be fetched,"The digital divide remains an ongoing societal concern, with digital exclusion shown to have a significantly detrimental impact on people’s quality of life. Artificial intelligence (AI), the latest wave of digitalisation, is being integrated into the fabric of society at an accelerated rate, the speed of which has prompted ethical concerns. Without addressing the digital divide, the AI revolution risks exacerbating the existing consequences of digital exclusion and limiting the potential for all people to reap the benefits provided by AI. To understand the factors that might contribute to experiences of AI, and how these might be related to digital exclusion, we surveyed a diverse online community sample (N = 303). We created a novel measure of digital confidence capturing individual levels of awareness, familiarity, and sense of competence with digital technology. Results indicated that measures of digital confidence were predicted by structural, behavioural, and psychological differences, such that women, older people, those on lower salaries, people with less digital access, and those with lower digital well-being, reported significantly less digital confidence. Furthermore, digital confidence significantly moderated the relationship between people’s experiences with everyday AI technologies and their general attitudes towards AI. This understanding of the spill-over effects of digital exclusion onto experiences of AI is fundamental to the articulation and delivery of inclusive AI."
https://doi.org/10.31234/osf.io/7b3mx,2024-02-28,Psychological flexibility: An attempt to remove theoretical elements from the estimation,To be fetched,"Psychological flexibility is an important concept that has long been addressed in basic and clinical domains. However, as noted in many comprehensive reviews, it needs more conceptual clarity and measurement reliability and validity. Any use of this concept necessarily requires consideration of a theoretical or conceptual framework. Therefore, measuring or estimating psychological flexibility that removes these theoretical elements would help understand psychological flexibility in increasingly complex situations. Based on the concept of unit price created in behavioral experiments, which facilitated comparisons among behavioral experiments, this study constructed a state-space model with unit price as an explanatory variable. It tested the validity of a new measure of psychological flexibility through simulation experiments. Closely related to whether being flexible is a positive or negative state, the issue remains to propose a framework for examining whether psychological flexibility reinforces the behavior of others through certain cultural practices in a particular environment and, in turn, reinforces that practice in that population."
https://doi.org/10.31234/osf.io/7b3mx,2024-02-28,Psychological flexibility: An attempt to remove theoretical elements from the estimation,Naoki Kamiya,"Psychological flexibility is an important concept that has long been addressed in basic and clinical domains. However, as noted in many comprehensive reviews, it needs more conceptual clarity and measurement reliability and validity. Any use of this concept necessarily requires consideration of a theoretical or conceptual framework. Therefore, measuring or estimating psychological flexibility that removes these theoretical elements would help understand psychological flexibility in increasingly complex situations. Based on the concept of unit price created in behavioral experiments, which facilitated comparisons among behavioral experiments, this study constructed a state-space model with unit price as an explanatory variable. It tested the validity of a new measure of psychological flexibility through simulation experiments. Closely related to whether being flexible is a positive or negative state, the issue remains to propose a framework for examining whether psychological flexibility reinforces the behavior of others through certain cultural practices in a particular environment and, in turn, reinforces that practice in that population."
https://doi.org/10.31234/osf.io/wd4tr,2024-02-28,The Digital Divide in Action: How Experiences of Digital Technology Shape Future Relationships with Artificial Intelligence,"Sarah Bentley, Claire K. Naughtin, Melanie J McGrath, Jessica L. Irons, Patrick Cooper","The digital divide remains an ongoing societal concern, with digital exclusion shown to have a significantly detrimental impact on people’s quality of life. Artificial intelligence (AI), the latest wave of digitalisation, is being integrated into the fabric of society at an accelerated rate, the speed of which has prompted ethical concerns. Without addressing the digital divide, the AI revolution risks exacerbating the existing consequences of digital exclusion and limiting the potential for all people to reap the benefits provided by AI. To understand the factors that might contribute to experiences of AI, and how these might be related to digital exclusion, we surveyed a diverse online community sample (N = 303). We created a novel measure of digital confidence capturing individual levels of awareness, familiarity, and sense of competence with digital technology. Results indicated that measures of digital confidence were predicted by structural, behavioural, and psychological differences, such that women, older people, those on lower salaries, people with less digital access, and those with lower digital well-being, reported significantly less digital confidence. Furthermore, digital confidence significantly moderated the relationship between people’s experiences with everyday AI technologies and their general attitudes towards AI. This understanding of the spill-over effects of digital exclusion onto experiences of AI is fundamental to the articulation and delivery of inclusive AI."
https://doi.org/10.31234/osf.io/wtbyd,2024-02-28,The effectiveness of meditation in preventing burnout in high-risk occupations: a systematic review protocol,To be fetched,"This is a protocol outlining a review with the aim to evaluate the effectiveness of meditation in preventing burnout in emergency services workers, military personnel, corporate executives, and healthcare workers. A meta-analysis will be conducted using psychological and physiological measures of stress and burnout. Burnout is framed as a state of physical and mental exhaustion encoded in a diagnosis by Z73.0 in the ICD-10-CM and negatively affects an individual’s psychological and physical health. It also decreases work performance and affects employers through increased absenteeism and employee turnover. Mindfulness intervention is the most reported intervention for burnout and has been found to offer statistically significant effects in improving stress levels and reducing burnout. Recent research has examined focused meditation, a particular type of mindfulness training, and the role of its effects of attentional regulation and emotion regulation in reducing stress and burnout. Studies will be included which have participants working in high-risk areas including emergency services, the military, corporate executive management or healthcare workers who participated in a meditation intervention prior to the onset of burnout. Psychological outcomes, including Maslach Burnout Inventory (MBI), Shirom-Melamed Burnout Measure (SMBM), Health and Safety Executive (HSE), Bergen Burnout Inventory (BBI) and Perceived Stress Scale (PSS), and physiological outcomes including cortisol levels, heart rate variability, blood pressure, stress reactivity, hypothalamic-pituitary adrenal axis (HPA) and levels of neurotransmitters (dopamine, serotonin, oxytocin and vasopressin) will be used to assess the effectiveness of the meditation intervention. The PsycINFO, PubMed and CINAHL database search and a meta-analysis will be conducted comparing data extracted from standard burnout measures and inventories and physiological measures."
https://doi.org/10.31234/osf.io/wtbyd,2024-02-28,The effectiveness of meditation in preventing burnout in high-risk occupations: a systematic review protocol,"Peter Chu, Carolyn Semmler, Fiona Kerr","This is a protocol outlining a review with the aim to evaluate the effectiveness of meditation in preventing burnout in emergency services workers, military personnel, corporate executives, and healthcare workers. A meta-analysis will be conducted using psychological and physiological measures of stress and burnout. Burnout is framed as a state of physical and mental exhaustion encoded in a diagnosis by Z73.0 in the ICD-10-CM and negatively affects an individual’s psychological and physical health. It also decreases work performance and affects employers through increased absenteeism and employee turnover. Mindfulness intervention is the most reported intervention for burnout and has been found to offer statistically significant effects in improving stress levels and reducing burnout. Recent research has examined focused meditation, a particular type of mindfulness training, and the role of its effects of attentional regulation and emotion regulation in reducing stress and burnout. Studies will be included which have participants working in high-risk areas including emergency services, the military, corporate executive management or healthcare workers who participated in a meditation intervention prior to the onset of burnout. Psychological outcomes, including Maslach Burnout Inventory (MBI), Shirom-Melamed Burnout Measure (SMBM), Health and Safety Executive (HSE), Bergen Burnout Inventory (BBI) and Perceived Stress Scale (PSS), and physiological outcomes including cortisol levels, heart rate variability, blood pressure, stress reactivity, hypothalamic-pituitary adrenal axis (HPA) and levels of neurotransmitters (dopamine, serotonin, oxytocin and vasopressin) will be used to assess the effectiveness of the meditation intervention. The PsycINFO, PubMed and CINAHL database search and a meta-analysis will be conducted comparing data extracted from standard burnout measures and inventories and physiological measures."
https://doi.org/10.31234/osf.io/sk2m4,2024-02-27,"Relations between Social Camouflaging, Life Satisfaction, and Depression among Polish Women with ADHD",To be fetched,"Background. This study investigated the relationship between social camouflaging, life satisfaction, and depression symptoms in Polish women with ADHD. It aimed to fill a research gap in understanding ADHD manifestations in women, particularly focusing on social camouflaging—a concept known in autism, referring to strategies used to mask symptoms in social interactions. Methods: A total of 329 women with ADHD took part in an online survey. The survey measured ADHD symptoms using the ADHD Self Report Scale (ASRS-vI.I), life satisfaction with the Satisfaction With Life Scale (SWLS), and depressive symptoms using the Patient Health Questionnaire-9 (PHQ-9). Additionally, a novel set of questions, inspired by the Camouflaging Autistic Traits Questionnaire (CAT-Q) but tailored for ADHD, was utilized to assess social camouflaging. Results: Results indicated a significant negative association between social camouflaging and life satisfaction and a positive correlation with depressive symptoms, even after controlling for demographic variables. Additionally, factors like being in a relationship and having a positive financial perception correlated with higher well-being. Limitations: Reliance on self-report measures, the cross-sectional design, and participants' self-reported ADHD diagnosis are the main limitations of the study. Conclusion: The research underscores the importance of social camouflaging in understanding ADHD in women, suggesting that efforts to mask symptoms and meet societal expectations may link to lower life satisfaction and increased depressive symptoms. These findings advocate for continued research into these dynamics to develop more effective support for women with ADHD."
https://doi.org/10.31234/osf.io/38ygu,2024-02-27,“It’s not OK to talk to anyone this way”:    Responding to Disability-Based Hate Crime,To be fetched,"Psychological research has highlighted national differences in responses to disability-based hate crime.  However, there has been limited systematic exploration of the disability represented in hate crime scenarios, and of responses of disabled people1. We surveyed N= 467 adults online, presenting them with disability-based hate crime scenarios. We orthogonally changed the disability of the hate crime target (deaf person or wheelchair user) across Hungary, Italy, Nordic countries, and the UK. We measured responses and intentions, direct contact with disabled people, identification as disabled, and reasons for their responses. Results showed cross-national differences, and increased helping intentions among disabled participants. Path analyses showed that anger and anxiety significantly mediated the association between direct contact with disabled people and helping intentions. Qualitative template analysis showed that helping and avoidance intentions from disabled and non-disabled people are motivated by a variety of reasons."
https://doi.org/10.31234/osf.io/sk2m4,2024-02-27,"Relations between Social Camouflaging, Life Satisfaction, and Depression among Polish Women with ADHD","Fryderyka Wicherkiewicz, Małgorzata Gambin","Background. This study investigated the relationship between social camouflaging, life satisfaction, and depression symptoms in Polish women with ADHD. It aimed to fill a research gap in understanding ADHD manifestations in women, particularly focusing on social camouflaging—a concept known in autism, referring to strategies used to mask symptoms in social interactions. Methods: A total of 329 women with ADHD took part in an online survey. The survey measured ADHD symptoms using the ADHD Self Report Scale (ASRS-vI.I), life satisfaction with the Satisfaction With Life Scale (SWLS), and depressive symptoms using the Patient Health Questionnaire-9 (PHQ-9). Additionally, a novel set of questions, inspired by the Camouflaging Autistic Traits Questionnaire (CAT-Q) but tailored for ADHD, was utilized to assess social camouflaging. Results: Results indicated a significant negative association between social camouflaging and life satisfaction and a positive correlation with depressive symptoms, even after controlling for demographic variables. Additionally, factors like being in a relationship and having a positive financial perception correlated with higher well-being. Limitations: Reliance on self-report measures, the cross-sectional design, and participants' self-reported ADHD diagnosis are the main limitations of the study. Conclusion: The research underscores the importance of social camouflaging in understanding ADHD in women, suggesting that efforts to mask symptoms and meet societal expectations may link to lower life satisfaction and increased depressive symptoms. These findings advocate for continued research into these dynamics to develop more effective support for women with ADHD."
https://doi.org/10.31234/osf.io/38ygu,2024-02-27,“It’s not OK to talk to anyone this way”:    Responding to Disability-Based Hate Crime,"Sian E Jones, Clare Uytman, Laura Salminen, Laura Dalnoki, Valentina Sartore, Daphne Lola-Luz, Leanne Ali, William Muir, Kiia Uusitalo, Daniela Schintu","Psychological research has highlighted national differences in responses to disability-based hate crime.  However, there has been limited systematic exploration of the disability represented in hate crime scenarios, and of responses of disabled people1. We surveyed N= 467 adults online, presenting them with disability-based hate crime scenarios. We orthogonally changed the disability of the hate crime target (deaf person or wheelchair user) across Hungary, Italy, Nordic countries, and the UK. We measured responses and intentions, direct contact with disabled people, identification as disabled, and reasons for their responses. Results showed cross-national differences, and increased helping intentions among disabled participants. Path analyses showed that anger and anxiety significantly mediated the association between direct contact with disabled people and helping intentions. Qualitative template analysis showed that helping and avoidance intentions from disabled and non-disabled people are motivated by a variety of reasons."
https://doi.org/10.31234/osf.io/rcseq,2024-02-26,Using machine learning to discover diverse emotional-semantic representations for sonic branding: a generalisable approach,To be fetched,"Discovering emotional-semantic dimensions underlying music description contributes to music psychological theory. In applied branding contexts, such dimensions are employed to find music that conveys core brand values. A practically small number of dimensions for consumers to rate music assets on are desirable and often derived based on factor analytic methods. However, often resulting factor analytic structures must be subjectively reinterpreted and analyses begin by discovering an a priori unknown number of latent variables in a dataset, or, if a desirable number of dimensions is requested, produces solutions with highly unequal component importances. Alternatively, we propose a machine learning approach to discovering useful dimensions in high-dimensional data: Diversity-Induced Self-Representation Feature Selection (D-ISR; Liu, Liu, Zhang, Wang, and Wang, 2017), a data-driven methodology which can transition between simple and complex representations of the same (emotional-semantic) space. With a large dataset (NParticipants = 55,593; NResponses = 5,820,188; NAudioTracks = 251), we use D-ISR to select 14 core attributes from a pool of 212. The subset can reconstruct the original multi- dimensional space of all 212 attributes with good reconstruction accuracy (Mean RMSE = 0.32 [0, 10]). Consequently, whilst good reconstruction accuracy cannot always be guaranteed, we demonstrate a principled approach for discovering any a priori number of features to represent a large dimensional space. Framed this way, the decision about whether to use N vs. N+1 features only marginally affects reconstruction error and hence has broad practical utility. The method can be generalised to similar domains (e.g., personality measurement)."
https://doi.org/10.31234/osf.io/rcseq,2024-02-26,Using machine learning to discover diverse emotional-semantic representations for sonic branding: a generalisable approach,"Sebastian Jacob Silas, Daniel Müllensiefen, David John Baker","Discovering emotional-semantic dimensions underlying music description contributes to music psychological theory. In applied branding contexts, such dimensions are employed to find music that conveys core brand values. A practically small number of dimensions for consumers to rate music assets on are desirable and often derived based on factor analytic methods. However, often resulting factor analytic structures must be subjectively reinterpreted and analyses begin by discovering an a priori unknown number of latent variables in a dataset, or, if a desirable number of dimensions is requested, produces solutions with highly unequal component importances. Alternatively, we propose a machine learning approach to discovering useful dimensions in high-dimensional data: Diversity-Induced Self-Representation Feature Selection (D-ISR; Liu, Liu, Zhang, Wang, and Wang, 2017), a data-driven methodology which can transition between simple and complex representations of the same (emotional-semantic) space. With a large dataset (NParticipants = 55,593; NResponses = 5,820,188; NAudioTracks = 251), we use D-ISR to select 14 core attributes from a pool of 212. The subset can reconstruct the original multi- dimensional space of all 212 attributes with good reconstruction accuracy (Mean RMSE = 0.32 [0, 10]). Consequently, whilst good reconstruction accuracy cannot always be guaranteed, we demonstrate a principled approach for discovering any a priori number of features to represent a large dimensional space. Framed this way, the decision about whether to use N vs. N+1 features only marginally affects reconstruction error and hence has broad practical utility. The method can be generalised to similar domains (e.g., personality measurement)."
https://doi.org/10.31234/osf.io/9d8g7,2024-02-25,Replication of De Vries et al. (2015) - Perceptions of greenwashing,To be fetched,"As the climate change crisis is becoming more evident, a growing number of businesses and organizations are getting involved in sustainability efforts. But not all corporate sustainability efforts are applauded: sometimes the public accuse companies of greenwashing, i.e., overstating the extent to which the company is environmentally friendly. There is little research on the factors that influence perceived greenwashing amongst the public. Here, we report a replication and extension of one of the few studies of this topic, Experiment 2 in De Vries et al. (2015). The original study found that people perceived more greenwashing when an oil company communicated an environmental motive for a sustainability investment (carbon capture and storage), as opposed to a profit motive, d = 0.98 [0.37, 1.59]. The present pre-registered replication (n = 516) did not find support for this effect, with very little difference in perceived greenwashing depending on communicated motive, d = -0.09 [-0.38, 0.21]. As extensions, we included a condition where a mixed motive (both environment and profits) was communicated, tested the effect using a different type of company than the original, included a measure of general attitudes to the company in addition to perceived greenwashing, and included measures of individual differences in attitudes towards corporate social responsibility and belief in climate change. The most noteworthy exploratory finding was that attitudes were more positive when an environmental or a mixed motive was communicated rather than a profit motive."
https://doi.org/10.31234/osf.io/uxj8e,2024-02-24,Memorials and political memory: A text analysis of online reviews,To be fetched,"Memorials serve a purpose far beyond their physical structure. They preserve and reinforce national and international narratives, and as such play a crucial role in the social psychological divides within political spaces. This paper explores our perceptions of these embodied divides and the role of memorials in the maintenance of political narratives. We conducted text analysis of over 158,000 online memorial reviews to explore sentiment differences between traditional and counter-memorials, and found that these memorials elicited quantitatively different reactions from visitors. We discuss the implications of these findings in the realm of contemporary political understandings and narratives. This research addresses a critical gap in understanding how physical memorials, as representations of past geopolitical events, influence public perception of political narratives in the present and future. By examining the role of memorials in the spatial and social psychological dimensions of politics, we provide novel insights and contributions to the burgeoning field of social psychology in understanding macro-level phenomena."
https://doi.org/10.31234/osf.io/uxj8e,2024-02-24,Memorials and political memory: A text analysis of online reviews,"Bethany Mulderig, Kevin Carriere, Brady Wagoner","Memorials serve a purpose far beyond their physical structure. They preserve and reinforce national and international narratives, and as such play a crucial role in the social psychological divides within political spaces. This paper explores our perceptions of these embodied divides and the role of memorials in the maintenance of political narratives. We conducted text analysis of over 158,000 online memorial reviews to explore sentiment differences between traditional and counter-memorials, and found that these memorials elicited quantitatively different reactions from visitors. We discuss the implications of these findings in the realm of contemporary political understandings and narratives. This research addresses a critical gap in understanding how physical memorials, as representations of past geopolitical events, influence public perception of political narratives in the present and future. By examining the role of memorials in the spatial and social psychological dimensions of politics, we provide novel insights and contributions to the burgeoning field of social psychology in understanding macro-level phenomena."
https://doi.org/10.31234/osf.io/9d8g7,2024-02-25,Replication of De Vries et al. (2015) - Perceptions of greenwashing,"Erik Løhre, Markus Høstaker, Øystein Løvik Hoprekstad","As the climate change crisis is becoming more evident, a growing number of businesses and organizations are getting involved in sustainability efforts. But not all corporate sustainability efforts are applauded: sometimes the public accuse companies of greenwashing, i.e., overstating the extent to which the company is environmentally friendly. There is little research on the factors that influence perceived greenwashing amongst the public. Here, we report a replication and extension of one of the few studies of this topic, Experiment 2 in De Vries et al. (2015). The original study found that people perceived more greenwashing when an oil company communicated an environmental motive for a sustainability investment (carbon capture and storage), as opposed to a profit motive, d = 0.98 [0.37, 1.59]. The present pre-registered replication (n = 516) did not find support for this effect, with very little difference in perceived greenwashing depending on communicated motive, d = -0.09 [-0.38, 0.21]. As extensions, we included a condition where a mixed motive (both environment and profits) was communicated, tested the effect using a different type of company than the original, included a measure of general attitudes to the company in addition to perceived greenwashing, and included measures of individual differences in attitudes towards corporate social responsibility and belief in climate change. The most noteworthy exploratory finding was that attitudes were more positive when an environmental or a mixed motive was communicated rather than a profit motive."
https://doi.org/10.31234/osf.io/9e5f7,2024-02-23,Emotional Contagion in Scandinavia during the COVID-19 Public Health Crisis,To be fetched,"In this article we present the findings of social media analysis of the spread of misinformation in the wake of the COVID-19 pandemic and outline how analyses of the psychological properties of a text can be used to optimize strategic messaging online. Our data used Twitter data, collected during the COVID-19 pandemic and analyzed using a suite of AI based analytical tools, which provided data for further empirical analysis. The analysis yielded insights related to the differences in the dynamics of the spread of misinformation within (and outside of) Scandinavian countries. Analysing this data enabled us to explore three hypotheses: (1) Misinformation will be associated with specific moral signatures, which will differ between Scandinavian and non-Scandinavian samples, (2) Levels of engagement will be associated with specific themes and moral concerns, which will differ between Scandinavian and non-Scandinavian samples, and (3) Within Scandinavia, similar unique signatures will be discernible at the country level, with Sweden driving significant differences. These specific results provide guidance for healthcare professionals responsible for communicating information and crafting messages that are more resonant with their target population and more generally demonstrate the ability for social media analysis to be useful in strategic decision making when going beyond focusing on engagement metrics or sentiment alone."
https://doi.org/10.31234/osf.io/9e5f7,2024-02-23,Emotional Contagion in Scandinavia during the COVID-19 Public Health Crisis,"Justin Lane, Ivan Puga-Gonzalez, Roger Normann, LeRon Shults, Jan Pastorek","In this article we present the findings of social media analysis of the spread of misinformation in the wake of the COVID-19 pandemic and outline how analyses of the psychological properties of a text can be used to optimize strategic messaging online. Our data used Twitter data, collected during the COVID-19 pandemic and analyzed using a suite of AI based analytical tools, which provided data for further empirical analysis. The analysis yielded insights related to the differences in the dynamics of the spread of misinformation within (and outside of) Scandinavian countries. Analysing this data enabled us to explore three hypotheses: (1) Misinformation will be associated with specific moral signatures, which will differ between Scandinavian and non-Scandinavian samples, (2) Levels of engagement will be associated with specific themes and moral concerns, which will differ between Scandinavian and non-Scandinavian samples, and (3) Within Scandinavia, similar unique signatures will be discernible at the country level, with Sweden driving significant differences. These specific results provide guidance for healthcare professionals responsible for communicating information and crafting messages that are more resonant with their target population and more generally demonstrate the ability for social media analysis to be useful in strategic decision making when going beyond focusing on engagement metrics or sentiment alone."
https://doi.org/10.31234/osf.io/h5vyu,2024-02-21,"Treating Maternal Mental Health Problems with an App-Based Program: A Randomized Control Trial of BEAM, for Mothers of Young Children",To be fetched,"Importance: Exposure to maternal mental illness in the first 3 years of life is associated with poor child outcomes. Maternal mental health problems increased dramatically during the COVID-19 pandemic with many parents not having access to evidence-based treatments. Mobile health (mHealth) treatments show promise for adult mood and anxiety disorders but rarely include parenting strategies and have high dropout rates.   Objective: This study aimed to evaluate the efficacy of the Building Emotion Awareness and Mental Health (BEAM) app-based program, which responds to maternal mental health and parenting needs while building social connection between participants.   Design: A two-arm, phase III randomized controlled trial (RCT) was conducted to evaluate the BEAM intervention compared to unrestricted services-as-usual (US). Participants completed self-report measures at eligibility screening (baseline assessment, T0), prior to randomization (pre-intervention, T1) and immediately following the intervention (post-intervention, T2).   Setting: Individuals were recruited and completed surveys online.   Participants: A final sample of 119 mothers with children aged 18 to 36 months, who self-reported moderate-to-severe symptoms of depression and/or anxiety.  Intervention: Individuals randomized to treatment participated in the 10-week BEAM program. It was hypothesized that the treatment group would report decreases in mental health symptoms (anxiety, depression, anger, alcohol use, sleep problems) and harsh parenting (overreactive parenting, conflictual parent-child interactions) compared to the US group.     Results: BEAM out-performed the US condition in reducing anxiety symptoms. Participants in both groups experienced significant decreases in depression. Participants with higher levels of anxiety and depression symptoms at screening, experienced significant decreases in mental health symptoms and harsh parenting composite scores, if they received the BEAM program, compared to US. This included specific declines in anxiety, anger, and dysfunctional parenting interactions. There were no significant effects for sleep problems, alcohol misuse, or overactive discipline.    Conclusion and Relevance: BEAM is a highly scalable intervention that has the potential to rapidly reach underserved groups in need of mental health and parenting support. Next steps include improving the user interface and exploring engagement and implementation of the program within existing health and social service systems for long-term improvements in family health and well-being."
https://doi.org/10.31234/osf.io/h5vyu,2024-02-21,"Treating Maternal Mental Health Problems with an App-Based Program: A Randomized Control Trial of BEAM, for Mothers of Young Children","Leslie E Roos, Anna MacKinnon, Elisabeth Bailin Xie, Kaeley M. Simpson, Charlie Rioux, Kristin Reynolds, Ryan Jeffrey Giuliano, Jennifer Harrington, Jennifer Protudjer, Melanie Soderstrom","Importance: Exposure to maternal mental illness in the first 3 years of life is associated with poor child outcomes. Maternal mental health problems increased dramatically during the COVID-19 pandemic with many parents not having access to evidence-based treatments. Mobile health (mHealth) treatments show promise for adult mood and anxiety disorders but rarely include parenting strategies and have high dropout rates.   Objective: This study aimed to evaluate the efficacy of the Building Emotion Awareness and Mental Health (BEAM) app-based program, which responds to maternal mental health and parenting needs while building social connection between participants.   Design: A two-arm, phase III randomized controlled trial (RCT) was conducted to evaluate the BEAM intervention compared to unrestricted services-as-usual (US). Participants completed self-report measures at eligibility screening (baseline assessment, T0), prior to randomization (pre-intervention, T1) and immediately following the intervention (post-intervention, T2).   Setting: Individuals were recruited and completed surveys online.   Participants: A final sample of 119 mothers with children aged 18 to 36 months, who self-reported moderate-to-severe symptoms of depression and/or anxiety.  Intervention: Individuals randomized to treatment participated in the 10-week BEAM program. It was hypothesized that the treatment group would report decreases in mental health symptoms (anxiety, depression, anger, alcohol use, sleep problems) and harsh parenting (overreactive parenting, conflictual parent-child interactions) compared to the US group.     Results: BEAM out-performed the US condition in reducing anxiety symptoms. Participants in both groups experienced significant decreases in depression. Participants with higher levels of anxiety and depression symptoms at screening, experienced significant decreases in mental health symptoms and harsh parenting composite scores, if they received the BEAM program, compared to US. This included specific declines in anxiety, anger, and dysfunctional parenting interactions. There were no significant effects for sleep problems, alcohol misuse, or overactive discipline.    Conclusion and Relevance: BEAM is a highly scalable intervention that has the potential to rapidly reach underserved groups in need of mental health and parenting support. Next steps include improving the user interface and exploring engagement and implementation of the program within existing health and social service systems for long-term improvements in family health and well-being."
https://doi.org/10.31234/osf.io/5k47x,2024-02-21,Decreased CO2 saturation during circular breathwork supports emergence of altered states of consciousness,To be fetched,"Altered states of consciousness (ASCs), induced e.g. during psychedelic-augmented therapy, show great potential to treat highly prevalent mental health disorders like depression and posttraumatic stress disorder. However, such treatment approaches are not widely accessible due to legal, medical, and financial limitations. In this study, we explore the potential of circular breathwork to serve as a non-pharmacological and hence more easily accessible alternative to engage similar therapeutic processes. Scientific studies investigating the effects of breathwork on mental health are only just emerging and the underlying physiological and psychological mechanisms are largely unknown. In this study, we aim to address these questions by for the first time tracking physiological and experiential dynamics throughout the time course of a breathwork session, comparing two popular forms of breathwork: Holotropic Breathwork® and Consciously-Connected breathwork. We show that a reduction in end-tidal CO2 pressure due to deliberate hyperventilation is instrumental in catalyzing ASCs during breathwork. The ASCs evoked by breathwork were comparable to those produced by psychedelics, and their depth predicted psychological and physiological follow-on effects, including improved well-being and a reduction of depressive symptoms. Further analysis showed that different breathwork traditions impacted physiological markers as well as experiential and psychological outcomes in a similar way. Our findings identify physiological boundary conditions in which ASCs can arise in a non-pharmacological context, offering insights into the functional mechanisms of breathwork as well as its potential as a psychotherapeutic tool."
https://doi.org/10.31234/osf.io/5k47x,2024-02-21,Decreased CO2 saturation during circular breathwork supports emergence of altered states of consciousness,"Martha N. Havenith, Max Leidenberger, Jelena Brasanac, Mafalda Corvacho, Inês Carmo Figueiredo, Leonie Schwarz, Malin Uthaug, Simona Rakusa, Marijan Bernardic, Liliana Vasquez-Mock","Altered states of consciousness (ASCs), induced e.g. during psychedelic-augmented therapy, show great potential to treat highly prevalent mental health disorders like depression and posttraumatic stress disorder. However, such treatment approaches are not widely accessible due to legal, medical, and financial limitations. In this study, we explore the potential of circular breathwork to serve as a non-pharmacological and hence more easily accessible alternative to engage similar therapeutic processes. Scientific studies investigating the effects of breathwork on mental health are only just emerging and the underlying physiological and psychological mechanisms are largely unknown. In this study, we aim to address these questions by for the first time tracking physiological and experiential dynamics throughout the time course of a breathwork session, comparing two popular forms of breathwork: Holotropic Breathwork® and Consciously-Connected breathwork. We show that a reduction in end-tidal CO2 pressure due to deliberate hyperventilation is instrumental in catalyzing ASCs during breathwork. The ASCs evoked by breathwork were comparable to those produced by psychedelics, and their depth predicted psychological and physiological follow-on effects, including improved well-being and a reduction of depressive symptoms. Further analysis showed that different breathwork traditions impacted physiological markers as well as experiential and psychological outcomes in a similar way. Our findings identify physiological boundary conditions in which ASCs can arise in a non-pharmacological context, offering insights into the functional mechanisms of breathwork as well as its potential as a psychotherapeutic tool."
https://doi.org/10.31234/osf.io/jwzyg,2024-02-19,How Effective Are Fact-checks in Pakistan and Who Engages with Them?,To be fetched,"For fact-checks to be effective, they must first and foremost reach people. Yet, little is known about what determines engagement with fact-checks and how to enhance their reach. We conducted a pre-registered online survey experiment in Pakistan (N participants  = 302, N observations = 1208) investigating the effectiveness of fact-checking, and the determinants of engagement with fact-checks and misinformation. We found that fact-checking reduced misperceptions, especially among the most misinformed. Trust was an important moderator of the effectiveness of fact-checking and of engagement with the fact-checks and misinformation. For instance, fact-checks were more effective among participants who trust the news the most and least effective among participants who trust social media the most. Participants more concerned about misinformation were more likely to like and share fact-checks on social media. Understanding and promoting engagement with factual corrections on social media is a pressing challenge to increase the quality of our information ecosystem."
https://doi.org/10.31234/osf.io/jwzyg,2024-02-19,How Effective Are Fact-checks in Pakistan and Who Engages with Them?,"Waqas Ejaz, Sacha Altay, Muhammad Ittefaq","For fact-checks to be effective, they must first and foremost reach people. Yet, little is known about what determines engagement with fact-checks and how to enhance their reach. We conducted a pre-registered online survey experiment in Pakistan (N participants  = 302, N observations = 1208) investigating the effectiveness of fact-checking, and the determinants of engagement with fact-checks and misinformation. We found that fact-checking reduced misperceptions, especially among the most misinformed. Trust was an important moderator of the effectiveness of fact-checking and of engagement with the fact-checks and misinformation. For instance, fact-checks were more effective among participants who trust the news the most and least effective among participants who trust social media the most. Participants more concerned about misinformation were more likely to like and share fact-checks on social media. Understanding and promoting engagement with factual corrections on social media is a pressing challenge to increase the quality of our information ecosystem."
https://doi.org/10.31234/osf.io/ydafj,2024-02-19,Stimulating Regulatory Compliance and Ethical Behavior of Organizations: a Review,To be fetched,"Regulators aim to influence behavior of regulatees, such as compliance (i.e., following rules and regulations), but also ethical behavior (i.e., doing the right thing, irrespective of the rules and regulations). A literature review was conducted to collect, summarize, and analyze empirical evidence on how regulators can stimulate regulatees’ compliance and ethical behavior. We introduce a novel framework, in which we propose that regulatory actions influence compliance and ethical behavior through regulatees’ capability, opportunity, and motivation. Combining the findings of 35 articles, we showed that studies on ‘sanctions’ and 'cooperation' demonstrated mixed results regarding their effectiveness, whereas ‘inspections’ were found more effective. Notably, the subcomponents psychological capability, social opportunity, and reflective motivation were more effective in stimulating behavior than physical capability, physical opportunity, and automatic motivation. We reflect on how these insights can be used by regulators to increase their effectiveness, as well as for the aim to further develop regulatory theory."
https://doi.org/10.31234/osf.io/ydafj,2024-02-19,Stimulating Regulatory Compliance and Ethical Behavior of Organizations: a Review,"Sarwesh Ishwardat, Elianne van Steenbergen, Tessa Coffeng, Naomi Ellemers","Regulators aim to influence behavior of regulatees, such as compliance (i.e., following rules and regulations), but also ethical behavior (i.e., doing the right thing, irrespective of the rules and regulations). A literature review was conducted to collect, summarize, and analyze empirical evidence on how regulators can stimulate regulatees’ compliance and ethical behavior. We introduce a novel framework, in which we propose that regulatory actions influence compliance and ethical behavior through regulatees’ capability, opportunity, and motivation. Combining the findings of 35 articles, we showed that studies on ‘sanctions’ and 'cooperation' demonstrated mixed results regarding their effectiveness, whereas ‘inspections’ were found more effective. Notably, the subcomponents psychological capability, social opportunity, and reflective motivation were more effective in stimulating behavior than physical capability, physical opportunity, and automatic motivation. We reflect on how these insights can be used by regulators to increase their effectiveness, as well as for the aim to further develop regulatory theory."
https://doi.org/10.31234/osf.io/k3mdg,2024-02-16,"Group-based injustice, but not group-based economic inequality, predicts political violence across 18 African countries",To be fetched,"Political violence causes immense human suffering. Scholars pinpoint economic inequalities between ethnic groups as a major cause of such violence. However, the relationships between group-based inequality, group-based injustice, and political violence are not well understood. Combining insights from social psychological research on collective action and political science research on civil conflict, we underscore that it is group-based injustice that motivates violence. A perception that one’s group has been treated unfairly tends to produce conflict-related emotions (e.g., anger). By contrast, a mere perception that one’s group is of lower economic status rarely produces such affect. Furthermore, perceived economic disadvantage negatively relates to perceived political efficacy, which may dissuade engagement in political violence. To assess these arguments, we analyzed attitudes toward, intentions to engage, and self-reported engagement in political violence, utilizing probability samples from 18 African countries (N &gt; 37,000). We found that measures of group-based perceived injustice, controlling for group-based economic inequality, predicted all violent outcomes. By contrast, measures of perceived group-based inequality, controlling for group-based injustice, negatively related to self-reported participation in violence and did not predict attitudes toward, and intentions to engage in, violence. We advance both social psychological and political science literatures, suggesting that group-based injustice and inequality are distinct constructs, relating to political violence via different pathways."
https://doi.org/10.31234/osf.io/k65yx,2024-02-16,"An asynchronous, automated workflow for looking time experiments with infants",To be fetched,"The study of infant gaze has long been a key tool for understanding the developing mind. However, labor-intensive data collection and processing limit the speed at which this understanding can be advanced. Here, we demonstrate a fully asynchronous, automated workflow for conducting Violation-of-Expectation (VoE) experiments. We first replicate four classic VoE experiments in a synchronous online setting, and show that VoE can generate highly replicable effects through remote testing. We then confirm the accuracy of a state-of-the- art gaze annotation software, iCatcher+ in a new setting. Third, we train parents to control the experiment flow based on the infant's gaze. Combining all three innovations, we then conduct an asynchronous automated infant-contingent VoE experiment. The automatic workflow successfully replicates a classic VoE effect: infants look longer at inefficient actions than efficient ones. We compare the resulting effect size and statistical power to the same study run in-lab and synchronously via Zoom. The automated workflow significantly reduces the marginal cost and time per participant, enabling larger sample sizes. By enhancing the reproducibility and robustness of findings relying on infant looking, this workflow could help support a cumulative science of infant cognition. Tools to implement the workflow are openly available."
https://doi.org/10.31234/osf.io/k3mdg,2024-02-16,"Group-based injustice, but not group-based economic inequality, predicts political violence across 18 African countries","Henrikas Bartusevicius, Casper Sakstrup","Political violence causes immense human suffering. Scholars pinpoint economic inequalities between ethnic groups as a major cause of such violence. However, the relationships between group-based inequality, group-based injustice, and political violence are not well understood. Combining insights from social psychological research on collective action and political science research on civil conflict, we underscore that it is group-based injustice that motivates violence. A perception that one’s group has been treated unfairly tends to produce conflict-related emotions (e.g., anger). By contrast, a mere perception that one’s group is of lower economic status rarely produces such affect. Furthermore, perceived economic disadvantage negatively relates to perceived political efficacy, which may dissuade engagement in political violence. To assess these arguments, we analyzed attitudes toward, intentions to engage, and self-reported engagement in political violence, utilizing probability samples from 18 African countries (N &gt; 37,000). We found that measures of group-based perceived injustice, controlling for group-based economic inequality, predicted all violent outcomes. By contrast, measures of perceived group-based inequality, controlling for group-based injustice, negatively related to self-reported participation in violence and did not predict attitudes toward, and intentions to engage in, violence. We advance both social psychological and political science literatures, suggesting that group-based injustice and inequality are distinct constructs, relating to political violence via different pathways."
https://doi.org/10.31234/osf.io/k65yx,2024-02-16,"An asynchronous, automated workflow for looking time experiments with infants","Gal Raz, Sabrina Piccolo, Janine A Medrano, Shari Liu, Kirsten Lydic, Catherine Mei, Victoria Nguyen, Tianmin Shu, Rebecca Saxe","The study of infant gaze has long been a key tool for understanding the developing mind. However, labor-intensive data collection and processing limit the speed at which this understanding can be advanced. Here, we demonstrate a fully asynchronous, automated workflow for conducting Violation-of-Expectation (VoE) experiments. We first replicate four classic VoE experiments in a synchronous online setting, and show that VoE can generate highly replicable effects through remote testing. We then confirm the accuracy of a state-of-the- art gaze annotation software, iCatcher+ in a new setting. Third, we train parents to control the experiment flow based on the infant's gaze. Combining all three innovations, we then conduct an asynchronous automated infant-contingent VoE experiment. The automatic workflow successfully replicates a classic VoE effect: infants look longer at inefficient actions than efficient ones. We compare the resulting effect size and statistical power to the same study run in-lab and synchronously via Zoom. The automated workflow significantly reduces the marginal cost and time per participant, enabling larger sample sizes. By enhancing the reproducibility and robustness of findings relying on infant looking, this workflow could help support a cumulative science of infant cognition. Tools to implement the workflow are openly available."
https://doi.org/10.31234/osf.io/cv3n4,2024-02-15,The Effects of Climate Action Interventions along Cultural Individualism-Collectivism,To be fetched,"As the climate crisis demands global engagement, it is crucial to understand how interventions influence individuals across cultural backgrounds. Are interventions more effective when aligned with the cultural values of a target population? To investigate, we evaluated eleven behavioral interventions aimed at stimulating climate change mitigation, along cultural individualism and collectivism orientations, in a large sample (N=59,440) spanning 63 countries. At baseline, we found the more individualistic a nation, the less its residents believed in climate change, supported mitigation policy, and intended to share information, but did not plant fewer trees in an online task. Critically, while some interventions were more effective in individualistic (decreasing psychological distance), and some in collectivistic nations (emphasizing social norms), others were effective in both (writing a letter to the future generation). These results reveal that individualism is a significant barrier to climate mitigation, and the efficacy of interventions hinges on cultural contexts."
https://doi.org/10.31234/osf.io/cv3n4,2024-02-15,The Effects of Climate Action Interventions along Cultural Individualism-Collectivism,"Danielle Goldwert, Yun Evelina Bao, Kimberly  C. Doell, Jay J Van Bavel, Madalina Vlasceanu","As the climate crisis demands global engagement, it is crucial to understand how interventions influence individuals across cultural backgrounds. Are interventions more effective when aligned with the cultural values of a target population? To investigate, we evaluated eleven behavioral interventions aimed at stimulating climate change mitigation, along cultural individualism and collectivism orientations, in a large sample (N=59,440) spanning 63 countries. At baseline, we found the more individualistic a nation, the less its residents believed in climate change, supported mitigation policy, and intended to share information, but did not plant fewer trees in an online task. Critically, while some interventions were more effective in individualistic (decreasing psychological distance), and some in collectivistic nations (emphasizing social norms), others were effective in both (writing a letter to the future generation). These results reveal that individualism is a significant barrier to climate mitigation, and the efficacy of interventions hinges on cultural contexts."
https://doi.org/10.31234/osf.io/p9gxr,2024-02-14,"Are Brenda, Juanita, and Latoya more feminine than Jia and Neha? Gendered Evaluations of Racialized Names",To be fetched,"Names are frequently used in social science research to manipulate identities such as race and gender. However, names may signal unintended identities or downplay intended identities. Three studies (N = 1,100 US participants) examined the gendered evaluations of names from five racial groups: Chinese, Indian, Black, Hispanic, and White. Studies 1 and 2 consistently found that Chinese and Indian female names were perceived as less feminine and more masculine than the three other racialized female names, which contradicts extant findings using Asian female faces. Chinese and Indian male names, on the other hand, were considered more feminine and less masculine than the other racialized male names. Study 3 found that participants expressed greater uncertainty and lower confidence about the gender of Chinese and Indian names compared to other racialized names. This research raises potential methodological concerns regarding the effectiveness of racialized names in signaling the gender of Asian ethnic groups."
https://doi.org/10.31234/osf.io/p9gxr,2024-02-14,"Are Brenda, Juanita, and Latoya more feminine than Jia and Neha? Gendered Evaluations of Racialized Names",Jin X. Goh,"Names are frequently used in social science research to manipulate identities such as race and gender. However, names may signal unintended identities or downplay intended identities. Three studies (N = 1,100 US participants) examined the gendered evaluations of names from five racial groups: Chinese, Indian, Black, Hispanic, and White. Studies 1 and 2 consistently found that Chinese and Indian female names were perceived as less feminine and more masculine than the three other racialized female names, which contradicts extant findings using Asian female faces. Chinese and Indian male names, on the other hand, were considered more feminine and less masculine than the other racialized male names. Study 3 found that participants expressed greater uncertainty and lower confidence about the gender of Chinese and Indian names compared to other racialized names. This research raises potential methodological concerns regarding the effectiveness of racialized names in signaling the gender of Asian ethnic groups."
https://doi.org/10.31234/osf.io/xjkns,2024-02-14,An Examination of Intolerance of Uncertainty in Schizophrenia,To be fetched,"Schizophrenia is associated with multiple comorbidities and symptoms, suggestive of common transdiagnostic processes. Elevated intolerance of uncertainty (IOU) is one such transdiagnostic process, but little research has been conducted in schizophrenia. This study assessed the associations between IOU, schizophrenia diagnosis, and schizophrenia symptoms using a between-groups cross-sectional design. The sample comprised 113 participants, 72 people with a schizophrenia diagnosis and 41 control participants without (73 male, 40 female, age range 19-69 (M = 42.1, SD = 13.0). Measures of schizophrenia symptoms, intolerance of uncertainty, depression and anxiety symptoms, rumination, executive functioning, and rumination were taken. Schizophrenia diagnosis was predicted by lower prospective IOU and higher levels of inhibitory IOU. Specifically, the prospective IOU subscale was uniquely associated with general schizophrenia symptoms. Higher levels of positive symptoms were associated with lower inhibitory IOU and higher prospective IOU, however, not when anxiety and depressive symptoms, rumination, and verbal fluency were controlled for. No unique associations were found with negative symptoms. Rumination did not mediate prospective IOU and general schizophrenia symptoms. Different directions of association between the subtypes of IOU and schizophrenia diagnosis, as well as distinct relationships between IOU and symptom subtypes suggest that prospective and inhibitory IOU have distinct associations with the disorder."
https://doi.org/10.31234/osf.io/xjkns,2024-02-14,An Examination of Intolerance of Uncertainty in Schizophrenia,"Yoon Hee Yang, David John Hallford, Clara M. Villanueva-Romero, Jose V. Hernández-Viadel, Jorge Ricarte","Schizophrenia is associated with multiple comorbidities and symptoms, suggestive of common transdiagnostic processes. Elevated intolerance of uncertainty (IOU) is one such transdiagnostic process, but little research has been conducted in schizophrenia. This study assessed the associations between IOU, schizophrenia diagnosis, and schizophrenia symptoms using a between-groups cross-sectional design. The sample comprised 113 participants, 72 people with a schizophrenia diagnosis and 41 control participants without (73 male, 40 female, age range 19-69 (M = 42.1, SD = 13.0). Measures of schizophrenia symptoms, intolerance of uncertainty, depression and anxiety symptoms, rumination, executive functioning, and rumination were taken. Schizophrenia diagnosis was predicted by lower prospective IOU and higher levels of inhibitory IOU. Specifically, the prospective IOU subscale was uniquely associated with general schizophrenia symptoms. Higher levels of positive symptoms were associated with lower inhibitory IOU and higher prospective IOU, however, not when anxiety and depressive symptoms, rumination, and verbal fluency were controlled for. No unique associations were found with negative symptoms. Rumination did not mediate prospective IOU and general schizophrenia symptoms. Different directions of association between the subtypes of IOU and schizophrenia diagnosis, as well as distinct relationships between IOU and symptom subtypes suggest that prospective and inhibitory IOU have distinct associations with the disorder."
https://doi.org/10.31234/osf.io/tmwq2,2024-02-13,The burden of experience: Remembering life events increases poor people's rejection of risky but favorable bets,To be fetched,"The burden of poverty is caused by both material deprivation and the psychological threat associated with it. Poverty has been found to diminish cognitive ability and cause the poor to forego beneficial programs. In the present study, we examined how exacerbating the psychological burden of poverty through an intervention that asked participants to recall a negative personal experience that made them feel unsuccessful influences the rate of acceptance of mixed gambles. Mixed gambles yield both gains and losses and simulate choices in life that contain both a risk of loss and an opportunity for gain. A second group of participants was asked to recall a positive personal experience that made them feel successful or proud, while a third group was not assigned any recall task. Experiment 1 targeted the intervention to a group of people living in extreme poverty in Kenya. Experiment 2 replicated the study in a group of non-poor students in Italy. Experiment 3 replicated the study online with a heterogeneous sample of people living in the UK. Results showed that recalling a negative life experience significantly reduced the average expected value of the accepted gambles. Recalling a positive life experience also significantly reduced the average expected value of accepted bets, although to a lesser extent. However, the treatments only affected poor individuals (Experiment 1), whereas non-poor individuals (Experiments 2 and 3) were immune to this effect. In addition, neither education nor cognitive ability was found to mediate the effect. These findings show that the burden of poverty can increase loss aversion, discouraging the poor from accepting risky investments with a positive expected value that could help them escape poverty in the long run, such as investments in personal education, small businesses, or retirement plans."
https://doi.org/10.31234/osf.io/tmwq2,2024-02-13,The burden of experience: Remembering life events increases poor people's rejection of risky but favorable bets,"Piero Ronzani, Luigi Mittone, Lucia Savadori","The burden of poverty is caused by both material deprivation and the psychological threat associated with it. Poverty has been found to diminish cognitive ability and cause the poor to forego beneficial programs. In the present study, we examined how exacerbating the psychological burden of poverty through an intervention that asked participants to recall a negative personal experience that made them feel unsuccessful influences the rate of acceptance of mixed gambles. Mixed gambles yield both gains and losses and simulate choices in life that contain both a risk of loss and an opportunity for gain. A second group of participants was asked to recall a positive personal experience that made them feel successful or proud, while a third group was not assigned any recall task. Experiment 1 targeted the intervention to a group of people living in extreme poverty in Kenya. Experiment 2 replicated the study in a group of non-poor students in Italy. Experiment 3 replicated the study online with a heterogeneous sample of people living in the UK. Results showed that recalling a negative life experience significantly reduced the average expected value of the accepted gambles. Recalling a positive life experience also significantly reduced the average expected value of accepted bets, although to a lesser extent. However, the treatments only affected poor individuals (Experiment 1), whereas non-poor individuals (Experiments 2 and 3) were immune to this effect. In addition, neither education nor cognitive ability was found to mediate the effect. These findings show that the burden of poverty can increase loss aversion, discouraging the poor from accepting risky investments with a positive expected value that could help them escape poverty in the long run, such as investments in personal education, small businesses, or retirement plans."
https://doi.org/10.31234/osf.io/ufxjp,2024-02-13,Who responds to meditation training? Examining predictors of self- and teacher-perceived effects of an 18-month randomised controlled trial,To be fetched,"Understanding the factors that predict why some individuals perceive to respond more to meditation training than others could impact the development, efficacy, adherence levels, and implementation of meditation-based interventions. We investigated individual-level variables associated with self- and teacher-perceived responsiveness to longer-term meditation training. This study presents a secondary analysis of the Age-Well trial (NCT02977819) and includes 90 healthy older adults (65-84 years) that were randomised to an 18-month meditation training or a non-native language (English) training. Responsiveness was measured post-intervention using participants’ and teachers’ ratings of four psychological domains (connection, positive/negative emotions, meta-awareness) in relation to two contexts (during sessions, in daily life), teachers’ perception of overall benefit, and a global composite comprising all self- and teacher-perceived responsiveness measures. Linear regression modelling indicates that, when including baseline variables (sex, education, neuroticism, cognition, expectancy) and engagement (hours of formal practice during intervention), only higher levels of engagement were associated with higher global composite scores (standardised estimate=0.50, 95% CI: 0.24-0.77, p&lt;0.001). Global composite scores were not correlated with pre-post changes in well-being. Findings indicate that more time spent practising meditation was related to greater perceived intervention effects. We suggest that future studies closely monitor levels of engagement and map reasons for disengagement."
https://doi.org/10.31234/osf.io/m67nt,2024-02-13,Do vegetarians expect to find meat tasty?,To be fetched,"Psychological interest in vegetarianism is on the rise due to its social significance (health, climate change, and animal welfare). It is unclear whether and how the vegetarians’ cognitions and emotions towards meat influence psychophysiological aspects such as salivation to food. The present study analysed voluntary (attitudes towards meat) and involuntary (emotions and salivation in response to images of food and real meat) behaviours, comparing vegetarians and omnivores. This study uses real meat and a measure of salivation to examine vegetarianism. The results showed that in vegetarians, attitudes and emotions towards meat were more negative than those of omnivores, while arousal towards meat was not different for vegetarians and omnivores. The salivation of vegetarians in response to real meat was not statistically different from that of omnivores. However, our results show that vegetarians’ salivation to real meat was similar to their baseline salivation. Our findings suggest that while vegetarians perceive meat as a negative non-appetizing stimulus at the cognitive and emotional level, at the involuntary psychophysiological level the perception of meat did not wholly suppress salivation and was perceived as a neutral non-appetizing stimulus."
https://doi.org/10.31234/osf.io/m67nt,2024-02-13,Do vegetarians expect to find meat tasty?,"Blanca Aguado-López, Antonio Cándido","Psychological interest in vegetarianism is on the rise due to its social significance (health, climate change, and animal welfare). It is unclear whether and how the vegetarians’ cognitions and emotions towards meat influence psychophysiological aspects such as salivation to food. The present study analysed voluntary (attitudes towards meat) and involuntary (emotions and salivation in response to images of food and real meat) behaviours, comparing vegetarians and omnivores. This study uses real meat and a measure of salivation to examine vegetarianism. The results showed that in vegetarians, attitudes and emotions towards meat were more negative than those of omnivores, while arousal towards meat was not different for vegetarians and omnivores. The salivation of vegetarians in response to real meat was not statistically different from that of omnivores. However, our results show that vegetarians’ salivation to real meat was similar to their baseline salivation. Our findings suggest that while vegetarians perceive meat as a negative non-appetizing stimulus at the cognitive and emotional level, at the involuntary psychophysiological level the perception of meat did not wholly suppress salivation and was perceived as a neutral non-appetizing stimulus."
https://doi.org/10.31234/osf.io/ufxjp,2024-02-13,Who responds to meditation training? Examining predictors of self- and teacher-perceived effects of an 18-month randomised controlled trial,"Marco Schlosser, Stefano Poletti, Fabienne Collette, Natalie L. Marchant, Antoine Lutz","Understanding the factors that predict why some individuals perceive to respond more to meditation training than others could impact the development, efficacy, adherence levels, and implementation of meditation-based interventions. We investigated individual-level variables associated with self- and teacher-perceived responsiveness to longer-term meditation training. This study presents a secondary analysis of the Age-Well trial (NCT02977819) and includes 90 healthy older adults (65-84 years) that were randomised to an 18-month meditation training or a non-native language (English) training. Responsiveness was measured post-intervention using participants’ and teachers’ ratings of four psychological domains (connection, positive/negative emotions, meta-awareness) in relation to two contexts (during sessions, in daily life), teachers’ perception of overall benefit, and a global composite comprising all self- and teacher-perceived responsiveness measures. Linear regression modelling indicates that, when including baseline variables (sex, education, neuroticism, cognition, expectancy) and engagement (hours of formal practice during intervention), only higher levels of engagement were associated with higher global composite scores (standardised estimate=0.50, 95% CI: 0.24-0.77, p&lt;0.001). Global composite scores were not correlated with pre-post changes in well-being. Findings indicate that more time spent practising meditation was related to greater perceived intervention effects. We suggest that future studies closely monitor levels of engagement and map reasons for disengagement."
